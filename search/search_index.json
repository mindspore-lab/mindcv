{"config":{"lang":["en","zh"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#mindcv","title":"MindCV","text":""},{"location":"#introduction","title":"Introduction","text":"<p>MindCV is an open-source toolbox for computer vision research and development based on MindSpore. It collects a series of classic and SoTA vision models, such as ResNet and SwinTransformer, along with their pre-trained weights and training strategies. SoTA methods such as auto augmentation are also provided for performance improvement. With the decoupled module design, it is easy to apply or adapt MindCV to your own CV tasks.</p>"},{"location":"#major-features","title":"Major Features","text":"<ul> <li> <p>Easy-to-Use. MindCV decomposes the vision framework into various configurable components. It is easy to customize your data pipeline, models, and learning pipeline with MindCV:</p> <pre><code>&gt;&gt;&gt; import mindcv\n# create a dataset\n&gt;&gt;&gt; dataset = mindcv.create_dataset('cifar10', download=True)\n# create a model\n&gt;&gt;&gt; network = mindcv.create_model('resnet50', pretrained=True)\n</code></pre> <p>Users can customize and launch their transfer learning or training task in one command line.</p> <pre><code># transfer learning in one command line\npython train.py --model=swin_tiny --pretrained --opt=adamw --lr=0.001 --data_dir=/path/to/data\n</code></pre> </li> <li> <p>State-of-The-Art. MindCV provides various CNN-based and Transformer-based vision models including SwinTransformer. Their pretrained weights and performance reports are provided to help users select and reuse the right model:</p> </li> <li> <p>Flexibility and efficiency. MindCV is built on MindSpore which is an efficient DL framework that can be run on different hardware platforms (GPU/CPU/Ascend). It supports both graph mode for high efficiency and pynative mode for flexibility.</p> </li> </ul>"},{"location":"#model-zoo","title":"Model Zoo","text":"<p>The performance of the models trained with MindCV is summarized in here, where the training recipes and weights are both available.</p> <p>Model introduction and training details can be viewed in each sub-folder under configs.</p>"},{"location":"#installation","title":"Installation","text":"<p>See Installation for details.</p>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#hands-on-tutorial","title":"Hands-on Tutorial","text":"<p>To get started with MindCV, please see the Quick Start, which will give you a quick tour of each key component and the train/validate/predict pipelines.</p> <p>Below are a few code snippets for your taste.</p> <pre><code>&gt;&gt;&gt; import mindcv\n# List and find a pretrained vision model\n&gt;&gt;&gt; mindcv.list_models(\"swin*\", pretrained=True)\n['swin_tiny']\n# Create the model object\n&gt;&gt;&gt; network = mindcv.create_model('swin_tiny', pretrained=True)\n# Validate its accuracy\n&gt;&gt;&gt; !python validate.py --model=swin_tiny --pretrained --dataset=imagenet --val_split=validation\n{'Top_1_Accuracy': 0.80824, 'Top_5_Accuracy': 0.94802, 'loss': 1.7331367141008378}\n</code></pre> Image Classification Demo <p>Right click on the image below and save as <code>dog.jpg</code>.</p> <p><p> </p></p> <p>Classify the downloaded image with a pretrained SoTA model:</p> <pre><code>&gt;&gt;&gt; !python infer.py --model=swin_tiny --image_path='./dog.jpg'\n{'Labrador retriever': 0.5700152, 'golden retriever': 0.034551315, 'kelpie': 0.010108651, 'Chesapeake Bay retriever': 0.008229004, 'Walker hound, Walker foxhound': 0.007791956}\n</code></pre> <p>The top-1 prediction result is labrador retriever, which is the breed of this cut dog.</p>"},{"location":"#training","title":"Training","text":"<p>It is easy to train your model on a standard or customized dataset using <code>train.py</code>, where the training strategy (e.g., augmentation, LR scheduling) can be configured with external arguments or a yaml config file.</p> <ul> <li> <p>Standalone Training</p> <pre><code># standalone training\npython train.py --model=resnet50 --dataset=cifar10 --dataset_download\n</code></pre> <p>Above is an example of training ResNet50 on CIFAR10 dataset on a CPU/GPU/Ascend device</p> </li> <li> <p>Distributed Training</p> <p>For large datasets like ImageNet, it is necessary to do training in distributed mode on multiple devices. This can be achieved with <code>msrun</code> and parallel features supported by MindSpore.</p> <pre><code># distributed training\n# assume you have 4 NPUs\nmsrun --bind_core=True --worker_num 4 python train.py --distribute \\\n    --model=densenet121 --dataset=imagenet --data_dir=/path/to/imagenet\n</code></pre> <p>Notice that if you are using msrun startup with 2 devices, please add <code>--bind_core=True</code> to improve performance. For example:</p> <pre><code>msrun --bind_core=True --worker_num=2--local_worker_num=2 --master_port=8118 \\\n--log_dir=msrun_log --join=True --cluster_time_out=300 \\\npython train.py --distribute --model=densenet121 --dataset=imagenet --data_dir=/path/to/imagenet\n</code></pre> </li> </ul> <p>For more information, please refer to https://www.mindspore.cn/tutorials/experts/en/r2.3.1/parallel/startup_method.html</p> <pre><code>Detailed parameter definitions can be seen in `config.py` and checked by running `python train.py --help'.\n\nTo resume training, please set the `--ckpt_path` and `--ckpt_save_dir` arguments. The optimizer state including the learning rate of the last stopped epoch will also be recovered.\n</code></pre> <ul> <li> <p>Config and Training Strategy</p> <p>You can configure your model and other components either by specifying external parameters or by writing a yaml config file. Here is an example of training using a preset yaml file.</p> <pre><code>msrun --bind_core=True --worker_num 4 python train.py -c configs/squeezenet/squeezenet_1.0_ascend.yaml\n</code></pre> <p>Pre-defined Training Strategies</p> <p>We provide more than 20 training recipes that achieve SoTA results on ImageNet currently. Please look into the <code>configs</code> folder for details. Please feel free to adapt these training strategies to your own model for performance improvement, which can be easily done by modifying the yaml file.</p> </li> <li> <p>Train on ModelArts/OpenI Platform</p> <p>To run training on the ModelArts or OpenI cloud platform:</p> <pre><code>1. Create a new training task on the cloud platform.\n2. Add the parameter `config` and specify the path to the yaml config file on the website UI interface.\n3. Add the parameter `enable_modelarts` and set True on the website UI interface.\n4. Fill in other blanks on the website and launch the training task.\n</code></pre> </li> </ul> <p>Graph Mode and PyNative Mode</p> <p>By default, the training pipeline <code>train.py</code> is run in graph mode on MindSpore, which is optimized for efficiency and parallel computing with a compiled static graph. In contrast, pynative mode is optimized for flexibility and easy debugging. You may alter the parameter <code>--mode</code> to switch to pure pynative mode for debugging purpose.</p> <p>Mixed Mode</p> <p>PyNative mode with mindspore.jit is a mixed mode for comprising flexibility and efficiency in MindSpore. To apply pynative mode with mindspore.jit for training, please run <code>train_with_func.py</code>, e.g.,</p> <pre><code>python train_with_func.py --model=resnet50 --dataset=cifar10 --dataset_download  --epoch_size=10\n</code></pre> <p>Note: this is an experimental function under improvement. It is not stable on MindSpore 1.8.1 or earlier versions.</p>"},{"location":"#validation","title":"Validation","text":"<p>To evaluate the model performance, please run <code>validate.py</code></p> <pre><code># validate a trained checkpoint\npython validate.py --model=resnet50 --dataset=imagenet --data_dir=/path/to/data --ckpt_path=/path/to/model.ckpt\n</code></pre> <p>Validation while Training</p> <p>You can also track the validation accuracy during training by enabling the <code>--val_while_train</code> option.</p> <pre><code>python train.py --model=resnet50 --dataset=cifar10 \\\n    --val_while_train --val_split=test --val_interval=1\n</code></pre> <p>The training loss and validation accuracy for each epoch will be saved in <code>${ckpt_save_dir}/results.log</code>.</p> <p>More examples about training and validation can be seen in examples.</p>"},{"location":"#tutorials","title":"Tutorials","text":"<p>We provide the following jupyter notebook tutorials to help users learn to use MindCV.</p> <ul> <li>Learn about configs</li> <li>Inference with a pretrained model</li> <li>Finetune a pretrained model on custom datasets</li> <li>Customize your model //coming soon</li> <li>Optimizing performance for vision transformer //coming soon</li> </ul>"},{"location":"#supported-algorithms","title":"Supported Algorithms","text":"Supported algorithms  <ul> <li>Augmentation<ul> <li>AutoAugment</li> <li>RandAugment</li> <li>Repeated Augmentation</li> <li>RandErasing (Cutout)</li> <li>CutMix</li> <li>MixUp</li> <li>RandomResizeCrop</li> <li>Color Jitter, Flip, etc</li> </ul> </li> <li>Optimizer<ul> <li>Adam</li> <li>AdamW</li> <li>Lion</li> <li>Adan (experimental)</li> <li>AdaGrad</li> <li>LAMB</li> <li>Momentum</li> <li>RMSProp</li> <li>SGD</li> <li>NAdam</li> </ul> </li> <li>LR Scheduler<ul> <li>Warmup Cosine Decay</li> <li>Step LR</li> <li>Polynomial Decay</li> <li>Exponential Decay</li> </ul> </li> <li>Regularization<ul> <li>Weight Decay</li> <li>Label Smoothing</li> <li>Stochastic Depth (depends on networks)</li> <li>Dropout (depends on networks)</li> </ul> </li> <li>Loss<ul> <li>Cross Entropy (w/ class weight and auxiliary logit support)</li> <li>Binary Cross Entropy  (w/ class weight and auxiliary logit support)</li> <li>Soft Cross Entropy Loss (automatically enabled if mixup or label smoothing is used)</li> <li>Soft Binary Cross Entropy Loss (automatically enabled if mixup or label smoothing is used)</li> </ul> </li> <li>Ensemble<ul> <li>Warmup EMA (Exponential Moving Average)</li> </ul> </li> </ul>"},{"location":"#how-to-contribute","title":"How to Contribute","text":"<p>We appreciate all kinds of contributions including issues and PRs to make MindCV better.</p> <p>Please refer to CONTRIBUTING for the contributing guideline. Please follow the Model Template and Guideline for contributing a model that fits the overall interface :)</p>"},{"location":"#license","title":"License","text":"<p>This project follows the Apache License 2.0 open-source license.</p>"},{"location":"#acknowledgement","title":"Acknowledgement","text":"<p>MindCV is an open-source project jointly developed by the MindSpore team, Xidian University, and Xi'an Jiaotong University. Sincere thanks to all participating researchers and developers for their hard work on this project. We also acknowledge the computing resources provided by OpenI.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you find this project useful in your research, please consider citing:</p> <pre><code>@misc{MindSpore Computer Vision 2022,\n    title={{MindSpore Computer  Vision}:MindSpore Computer Vision Toolbox and Benchmark},\n    author={MindSpore Vision Contributors},\n    howpublished = {\\url{https://github.com/mindspore-lab/mindcv/}},\n    year={2022}\n}\n</code></pre>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#dependency","title":"Dependency","text":"<ul> <li>mindspore &gt;= 1.8.1</li> <li>numpy &gt;= 1.17.0</li> <li>pyyaml &gt;= 5.3</li> <li>tqdm</li> <li>openmpi 4.0.3 (for distributed mode)</li> </ul> <p>To install the python library dependency, just run:</p> <pre><code>pip install -r requirements.txt\n</code></pre> <p>Tip</p> <p>MindSpore can be easily installed by following the official instructions where you can select your hardware platform for the best fit. To run in distributed mode, OpenMPI is required to install.</p> <p>The following instructions assume the desired dependency is fulfilled.</p>"},{"location":"installation/#install-with-pypi","title":"Install with PyPI","text":"<p>MindCV is published as a Python package and can be installed with <code>pip</code>, ideally by using a virtual environment. Open up a terminal and install MindCV with:</p> stablenightly <pre><code>pip install mindcv\n</code></pre> <pre><code># working on it using test.pypi\n</code></pre> <p>This will automatically install compatible versions of dependencies: NumPy, PyYAML and tqdm.</p> <p>Tip</p> <p>If you don't have prior experience with Python, we recommend reading Using Python's pip to Manage Your Projects' Dependencies, which is a really good introduction to the mechanics of Python package management and helps you troubleshoot if you run into errors.</p> <p>Warning</p> <p>The above command will NOT install MindSpore. We highly recommend you install MindSpore following the official instructions.</p>"},{"location":"installation/#install-from-source-bleeding-edge-version","title":"Install from Source (Bleeding Edge Version)","text":""},{"location":"installation/#from-vcs","title":"from VCS","text":"<pre><code>pip install git+https://github.com/mindspore-lab/mindcv.git\n</code></pre>"},{"location":"installation/#from-local-src","title":"from local src","text":"<p>Tip</p> <p>As this project is in active development, if you are a developer or contributor, please prefer this installation!</p> <p>MindCV can be directly used from GitHub by cloning the repository into a local folder which might be useful if you want to use the very latest version:</p> <pre><code>git clone https://github.com/mindspore-lab/mindcv.git\n</code></pre> <p>After cloning from <code>git</code>, it is recommended that you install using \"editable\" mode, which can help resolve potential module import issues:</p> <pre><code>cd mindcv\npip install -e .\n</code></pre>"},{"location":"modelzoo/","title":"Model Zoo","text":"performance tested on Ascend 910(8p) with graph mode model name params(M) cards batch size resolution jit level graph compile ms/step img/s acc@top1 acc@top5 recipe weight bit_resnet50 25.55 8 32 224x224 O2 146s 74.52 3413.33 76.81 93.17 yaml weights cmt_small 26.09 8 128 224x224 O2 1268s 500.64 2048.01 83.24 96.41 yaml weights coat_tiny 5.50 8 32 224x224 O2 543s 254.95 1003.92 79.67 94.88 yaml weights convit_tiny 5.71 8 256 224x224 O2 133s 231.62 8827.59 73.66 91.72 yaml weights convnext_tiny 28.59 8 16 224x224 O2 127s 66.79 1910.45 81.91 95.79 yaml weights convnextv2_tiny 28.64 8 128 224x224 O2 237s 400.20 2560.00 82.43 95.98 yaml weights crossvit_9 8.55 8 256 240x240 O2 206s 550.79 3719.30 73.56 91.79 yaml weights densenet121 8.06 8 32 224x224 O2 191s 43.28 5914.97 75.64 92.84 yaml weights dpn92 37.79 8 32 224x224 O2 293s 78.22 3272.82 79.46 94.49 yaml weights dpn92 37.79 8 32 224x224 O2 293s 78.22 3272.82 79.46 94.49 yaml weights efficientnet_b0 5.33 8 128 224x224 O2 203s 172.78 5926.61 76.89 93.16 yaml weights ghostnet_050 2.60 8 128 224x224 O2 383s 211.13 4850.09 66.03 86.64 yaml weights googlenet 6.99 8 32 224x224 O2 72s 21.40 11962.62 72.68 90.89 yaml weights halonet_50t 22.79 8 64 256x256 O2 261s 421.66 6437.82 79.53 94.79 yaml weights hrnet_w32 41.30 128 8 224x224 O2 1312s 279.10 3668.94 80.64 95.44 yaml weights inception_v3 27.20 8 32 299x299 O2 120s 76.42 3349.91 79.11 94.40 yaml weights inception_v4 42.74 8 32 299x299 O2 177s 76.19 3360.02 80.88 95.34 yaml weights mixnet_s 4.17 8 128 224x224 O2 556s 252.49 4055.61 75.52 92.52 yaml weights mnasnet_075 3.20 8 256 224x224 O2 140s 165.43 12379.86 71.81 90.53 yaml weights mobilenet_v1_025 0.47 8 64 224x224 O2 89s 42.43 12066.93 53.87 77.66 yaml weights mobilenet_v2_075 2.66 8 256 224x224 O2 164s 155.94 13133.26 69.98 89.32 yaml weights mobilenet_v3_small_100 2.55 8 75 224x224 O2 145s 48.14 12463.65 68.10 87.86 yaml weights mobilenet_v3_large_100 5.51 8 75 224x224 O2 271s 47.49 12634.24 75.23 92.31 yaml weights mobilevit_xx_small 1.27 64 8 256x256 O2 301s 53.52 9566.52 68.91 88.91 yaml weights nasnet_a_4x1056 5.33 8 256 224x224 O2 656s 330.89 6189.37 73.65 91.25 yaml weights pit_ti 4.85 8 128 224x224 O2 192s 271.50 3771.64 72.96 91.33 yaml weights poolformer_s12 11.92 8 128 224x224 O2 118s 220.13 4651.80 77.33 93.34 yaml weights pvt_tiny 13.23 8 128 224x224 O2 192s 229.63 4459.35 74.81 92.18 yaml weights pvt_v2_b0 3.67 8 128 224x224 O2 269s 269.38 3801.32 71.50 90.60 yaml weights regnet_x_800mf 7.26 8 64 224x224 O2 99s 42.49 12049.89 76.04 92.97 yaml weights repmlp_t224 38.30 8 128 224x224 O2 289s 578.23 1770.92 76.71 93.30 yaml weights repvgg_a0 9.13 8 32 224x224 O2 50s 20.58 12439.26 72.19 90.75 yaml weights repvgg_a1 14.12 8 32 224x224 O2 29s 20.70 12367.15 74.19 91.89 yaml weights res2net50 25.76 8 32 224x224 O2 119s 39.68 6451.61 79.35 94.64 yaml weights resnest50 27.55 8 128 224x224 O2 83s 244.92 4552.73 80.81 95.16 yaml weights resnet50 25.61 8 32 224x224 O2 43s 31.41 8150.27 76.69 93.50 yaml weights resnetv2_50 25.60 8 32 224x224 O2 52s 32.66 7838.33 76.90 93.37 yaml weights resnext50_32x4d 25.10 8 32 224x224 O2 49s 37.22 6878.02 78.53 94.10 yaml weights rexnet_09 4.13 8 64 224x224 O2 462s 130.10 3935.43 77.06 93.41 yaml weights seresnet18 11.80 8 64 224x224 O2 43s 44.40 11531.53 71.81 90.49 yaml weights shufflenet_v1_g3_05 0.73 8 64 224x224 O2 169s 40.62 12604.63 57.05 79.73 yaml weights shufflenet_v2_x0_5 1.37 8 64 224x224 O2 62s 41.87 12228.33 60.53 82.11 yaml weights skresnet18 11.97 8 64 224x224 O2 60s 45.84 11169.28 73.09 91.20 yaml weights squeezenet1_0 1.25 8 32 224x224 O2 45s 22.36 11449.02 58.67 80.61 yaml weights swin_tiny 33.38 8 256 224x224 O2 226s 454.49 4506.15 80.82 94.80 yaml weights swinv2_tiny_window8 28.78 8 128 256x256 O2 273s 317.19 3228.35 81.42 95.43 yaml weights vgg13 133.04 8 32 224x224 O2 23s 55.20 4637.68 72.87 91.02 yaml weights vgg19 143.66 8 32 224x224 O2 22s 67.42 3797.09 75.21 92.56 yaml weights visformer_tiny 10.33 8 128 224x224 O2 137s 217.92 4698.97 78.28 94.15 yaml weights volo_d1 27 8 128 224x224 O2 275s 270.79 3781.53 82.59 95.99 yaml weights xception 22.91 8 32 299x299 O2 161s 96.78 2645.17 79.01 94.25 yaml weights xcit_tiny_12_p16_224 7.00 8 128 224x224 O2 382s 252.98 4047.75 77.67 93.79 yaml weights performance tested on Ascend Atlas 800T A2 machines with graph mode model name params(M) cards batch size resolution jit level graph compile ms/step img/s acc@top1 acc@top5 recipe weight convit_tiny 5.71 8 256 224x224 O2 153s 226.51 9022.03 73.79 91.70 yaml weights convnext_tiny 28.59 8 16 224x224 O2 137s 48.7 2612.24 81.28 95.61 yaml weights convnextv2_tiny 28.64 8 128 224x224 O2 268s 257.2 3984.44 82.39 95.95 yaml weights crossvit_9 8.55 8 256 240x240 O2 221s 514.36 3984.44 73.38 91.51 yaml weights densenet121 8.06 8 32 224x224 O2 300s 47,34 5446.81 75.67 92.77 yaml weights densenet121 8.06 8 32 224x224 O2 300s 47,34 5446.81 75.67 92.77 yaml weights efficientnet_b0 5.33 8 128 224x224 O2 353s 172.64 5931.42 76.88 93.28 yaml weights googlenet 6.99 8 32 224x224 O2 113s 23.5 10893.62 72.89 90.89 yaml weights googlenet 6.99 8 32 224x224 O2 113s 23.5 10893.62 72.89 90.89 yaml weights inception_v3 27.20 8 32 299x299 O2 172s 70.83 3614.29 79.25 94.47 yaml weights inception_v4 42.74 8 32 299x299 O2 263s 80.97 3161.66 80.98 95.25 yaml weights mixnet_s 4.17 8 128 224x224 O2 706s 228.03 4490.64 75.58 95.54 yaml weights mnasnet_075 3.20 8 256 224x224 O2 144s 175.85 11646.29 71.77 90.52 yaml weights mobilenet_v1_025 0.47 8 64 224x224 O2 195s 47.47 10785.76 54.05 77.74 yaml weights mobilenet_v2_075 2.66 8 256 224x224 O2 233s 174.65 11726.31 69.73 89.35 yaml weights mobilenet_v3_small_100 2.55 8 75 224x224 O2 184s 52.38 11454.75 68.07 87.77 yaml weights mobilenet_v3_large_100 5.51 8 75 224x224 O2 354s 55.89 10735.37 75.59 92.57 yaml weights mobilevit_xx_small 1.27 8 64 256x256 O2 437s 67.24 7614.52 67.11 87.85 yaml weights nasnet_a_4x1056 5.33 8 256 224x224 O2 800s 364.35 5620.97 74.12 91.36 yaml weights pit_ti 4.85 8 128 224x224 O2 212s 266.47 3842.83 73.26 91.57 yaml weights poolformer_s12 11.92 8 128 224x224 O2 177s 211.81 4834.52 77.49 93.55 yaml weights pvt_tiny 13.23 8 128 224x224 O2 212s 237.5 4311.58 74.88 92.12 yaml weights pvt_v2_b0 3.67 8 128 224x224 O2 323s 255.76 4003.75 71.25 90.50 yaml weights regnet_x_800mf 7.26 8 64 224x224 O2 228s 50.74 10090.66 76.11 93.00 yaml weights repmlp_t224 38.30 8 128 224x224 O2 289s 578.23 1770.92 76.71 93.30 yaml weights repvgg_a0 9.13 8 32 224x224 O2 76s 24.12 10613.60 72.29 90.78 yaml weights repvgg_a1 14.12 8 32 224x224 O2 81s 28.29 9096.13 73.68 91.51 yaml weights res2net50 25.76 8 32 224x224 O2 174s 39.6 6464.65 79.33 94.64 yaml weights resnet50 25.61 8 32 224x224 O2 77s 31.9 8025.08 76.76 93.31 yaml weights resnetv2_50 25.60 8 32 224x224 O2 120s 32.19 7781.16 77.03 93.29 yaml weights resnext50_32x4d 25.10 8 32 224x224 O2 156s 44.61 5738.62 78.64 94.18 yaml weights rexnet_09 4.13 8 64 224x224 O2 515s 115.61 3290.28 76.14 92.96 yaml weights seresnet18 11.80 8 64 224x224 O2 90s 51.09 10021.53 72.05 90.59 yaml weights shufflenet_v1_g3_05 0.73 8 64 224x224 O2 191s 47.77 10718.02 57.08 79.89 yaml weights shufflenet_v2_x0_5 1.37 8 64 224x224 O2 100s 47.32 10819.95 60.65 82.26 yaml weights skresnet18 11.97 8 64 224x224 O2 134s 49.83 10274.93 72.85 90.83 yaml weights squeezenet1_0 1.25 8 32 224x224 O2 64s 23.48 10902.90 58.75 80.76 yaml weights swin_tiny 33.38 8 256 224x224 O2 266s 466.6 4389.20 80.90 94.90 yaml weights swinv2_tiny_window8 28.78 8 128 256x256 O2 385s 335.18 3055.07 81.38 95.46 yaml weights vgg13 133.04 8 32 224x224 O2 41s 30.52 8387.94 72.81 91.02 yaml weights vgg19 143.66 8 32 224x224 O2 53s 39.17 6535.61 75.24 92.55 yaml weights visformer_tiny 10.33 8 128 224x224 O2 169s 201.14 5090.98 78.40 94.30 yaml weights xcit_tiny_12_p16_224 7.00 8 128 224x224 O2 330s 229.25 4466.74 77.27 93.56 yaml weights"},{"location":"modelzoo/#notes","title":"Notes","text":"<ul> <li>top-1 and top-5: Accuracy reported on the validation set of ImageNet-1K.</li> </ul>"},{"location":"how_to_guides/feature_extraction/","title":"Multi-Scale Feature Extraction","text":"<p>In this guide, you will learn how to apply multi-scale feature extraction to the models in MindCV. In real deep learning model projects, we often exploit classic CV backbones, such as ResNet, VGG, for the purposes of better performance and fast development. Generally, using only the final output of backbones is not enough. We need outputs from intermediate layers, which act as multi-scale abstractions of the input, to help further boost the performance of our downstream tasks. To this end, we have designed a mechanism for extracting multi-scale features from backbones in MindCV. At the time of composing this guide, MindCV has supported extracting features with this mechanism from ResNet, MobileNetV3, ConvNeXt, ResNeST, EfficientNet, RepVGG, HRNet, and ReXNet. For more details of the feature extraction mechanism, please refer to <code>FeatureExtractWrapper</code>.</p> <p>This guide will help you learn how to add pieces of code to extracting multi-scale features from the rest of backbones. There are mainly two steps to achieve this:</p> <ol> <li>In <code>__init__()</code> of a model, register the intermediate layers whose outputted feature needs to be extracted in <code>self.feature_info</code>.</li> <li>Add a wrapper function for model creation.</li> <li>Pass <code>feature_only=True</code> and <code>out_indices</code> to <code>create_model()</code>.</li> </ol>"},{"location":"how_to_guides/feature_extraction/#layer-registration","title":"Layer Registration","text":"<p>There are mainly three possible scenarios when implementing code for feature extraction in MindCV, i.e., * a model with separate sequential module for each layer, * a model with one sequential module for all layers, and * a model with nonsequential modules.</p>"},{"location":"how_to_guides/feature_extraction/#scenario-1-separate-sequential-module-for-each-layer","title":"Scenario 1: Separate Sequential Module for Each Layer","text":"<p>An example of scenario 1 is shown as follows.</p> <pre><code>class DummyNet1(nn.Cell):\n    def __init__(self, **kwargs):\n        super().__init__()\n\n        # separate sequential module for each layer\n        self.layer1 = Layer()\n        self.layer2 = Layer()\n        self.layer3 = Layer()\n        self.layer4 = Layer()\n\n    def forward_features(self, x):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\n    def construct(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre> <p>To implement feature extraction for this scenario, we add a member variable <code>self.feature_info</code> into <code>__init__()</code> to register the extractable layers, e.g.,</p> <pre><code>class DummyNet1(nn.Cell):\n    def __init__(self, **kwargs):\n        super().__init__()\n        self.feature_info = []  # for layer registration\n\n        self.layer1 = Layer()\n        self.feature_info.append(dict(chs=, reduction=, name=\u201dlayer1\u201d))  # register layer\n        self.layer2 = Layer()\n        self.feature_info.append(dict(chs=, reduction=, name=\u201dlayer2\u201d))  # register layer\n        self.layer3 = Layer()\n        self.feature_info.append(dict(chs=, reduction=, name=\u201dlayer3\u201d))  # register layer\n        self.layer4 = Layer()\n        self.feature_info.append(dict(chs=, reduction=, name=\u201dlayer4\u201d))  # register layer\n</code></pre> <p>As we can see above, <code>self.feature_info</code> is a list of dictionaries, each of which contains three key-value pairs. Specifically, <code>chs</code> denotes the channel number of the produced feature, <code>reduction</code> denotes the total stride at the current layer, and <code>name</code> indicates the name of this layer stored in the model parameters which can be found using <code>get_parameters()</code>.</p> <p>For a real example of this scenario, please refer to ResNet.</p>"},{"location":"how_to_guides/feature_extraction/#scenario-2-one-sequential-module-for-all-layers","title":"Scenario 2: One Sequential Module for All Layers","text":"<p>For some models, the layers are in one sequential module. An example of scenario 2 is shown as follows.</p> <pre><code>class DummyNet2(nn.Cell):\n    def __init__(self, **kwargs):\n        super().__init__()\n        layers = []\n\n        for i in range(4):\n            layers.append(Layer())\n\n        # the layers are in one sequential module\n        self.layers = nn.SequentialCell(layers)\n\n    def forward_features(self, x):\n        x = self.layers(x)\n        return x\n\n    def construct(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre> <p>To implement feature extraction for this scenario, we also need to add <code>self.feature_info</code> into <code>__init__()</code> as in scenario 1, as well as create a member variable <code>self.flatten_sequential = True</code> to indicate that the sequential module in this model needs to be flattened before extracting features, e.g.,</p> <pre><code>class DummyNet2(nn.Cell):\n    def __init__(self, **kwargs):\n        super().__init__()\n        self.feature_info = []  # for layer registration\n        self.flatten_sequential = True  # indication of flattening the sequential module\n\n        layers = []\n\n        for i in range(4):\n            layers.append(Layer())\n            self.feature_info.append(dict(chs=, reduction=, name=f\u201dlayer{i}\u201d))  # register layer\n\n        self.layers = nn.SequentialCell(layers)\n</code></pre> <p>Please be reminded that the order of the module instantiations in <code>__init__()</code> is very important. The order must be kept as same as the order that these modules are called in <code>forward_features()</code> and <code>construct()</code>. Furthermore, only the modules called in <code>forward_features()</code> and <code>construct()</code> should be instantiated as member variables with the type of <code>nn.Cell</code>. Otherwise, the feature extraction mechanism will not work.</p> <p>For a real example of this scenario, please refer to MobileNetV3.</p>"},{"location":"how_to_guides/feature_extraction/#scenario-3-nonsequential-modules","title":"Scenario 3: Nonsequential Modules","text":"<p>Layers in models sometimes are nonsequential modules. An example of scenario 3 is shown as follows.</p> <pre><code>class DummyNet3(nn.Cell):\n    def __init__(self, **kwargs):\n        super().__init__()\n        layer1 = []\n\n        for i in range(3):\n                layer1.append(Layer())\n\n        # layers in self.layer1 are not sequential\n        self.layer1 = nn.CellList(layer1)\n\n        self.stage1 = Stage()\n\n        layer2 = []\n\n        for i in range(3):\n                layer2.append(Layer())\n\n        # layers in self.layer2 are not sequential\n        self.layer2 = nn.CellList(layer2)\n\n        self.stage2 = Stage()\n\n    def forward_features(self, x):\n        x_list = []\n\n        # layers are parallel instead of sequential\n        for i in range(3):\n                x_list.append(self.layer1[i](x))\n\n        x = self.stage1(x_list)\n\n        x_list = []\n\n        # layers are parallel instead of sequential\n        for i in range(3):\n                x_list.append(self.layer2[i](x))\n\n        x = self.stage2(x_list)\n\n        return x\n\n    def construct(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre> <p>To implement feature extraction for this scenario, we need to first create a new model feature class by inheriting the original model class. Then, we add the <code>self.feature_info</code> and a member variable <code>self.is_rewritten = True</code> to indicate that this class is rewritten for feature extraction. Finally, we reimplement <code>forward_features()</code> and <code>construct()</code> with feature extraction logic. Here is an example.</p> <pre><code>class DummyFeatureNet3(DummyNet3):\n    def __init__(self, **kwargs):\n        super(DummyFeatureNet3, self).__init__(**kwargs)\n        self.feature_info = []  # for layer registration\n        self.is_rewritten = True  # indication of rewriting for feature extraction\n\n        self.feature_info.append(dict(chs=, reduction=, name=\u201dstage1\u201d)  # register layer\n        self.feature_info.append(dict(chs=, reduction=, name=\u201dstage2\u201d)  # register layer\n\n    def forward_features(self, x):  # reimplement feature extraction logic\n        out = []\n        x_list = []\n\n        for i in range(3):\n                x_list.append(self.layer1[i](x))\n\n        x = self.stage1(x_list)\n        out.append(x)\n\n        x_list = []\n\n        for i in range(3):\n                x_list.append(self.layer2[i](x))\n\n        x = self.stage2(x_list)\n        out.append(x)\n\n        return out\n\n    def construct(self, x):\n        x = self.forward_features(x)\n        return x\n</code></pre> <p>For a real example of this scenario, please refer to HRNet.</p>"},{"location":"how_to_guides/feature_extraction/#adding-a-wrapper-function-for-model-creation","title":"Adding A Wrapper Function for Model Creation","text":"<p>After adding layer registration, we need to add one more simple wrapper function for model creation, so that the model instance can be passed to <code>build_model_with_cfg()</code> for feature extraction.</p> <p>Usually, the original creation function of a model in MindCV looks like this,</p> <pre><code>@register_model\ndef dummynet18(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; DummyNet:\n    default_cfg = default_cfgs[\"dummynet18\"]\n    model = DummyNet(..., num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <p>As for the models falling into scenarios 1 &amp; 2, in the wrapper function of model creation, simply pass the arguements to <code>build_model_with_cfg()</code>, e.g.,</p> <pre><code>def _create_dummynet(pretrained=False, **kwargs):\n    return build_model_with_cfg(DummyNet, pretrained, **kwargs)\n\n@register_model\ndef dummynet18(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; DummyNet:\n    default_cfg = default_cfgs[\"dummynet18\"]\n    model_args = dict(..., num_classes=num_classes, in_channels=in_channels, **kwargs)\n    return _create_dummynet(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre> <p>As for the models falling into scenario 3, most part of the wrapper function is the same as the ones for scenarios 1 &amp; 2. The difference lies in the part of deciding which model class to be instantiated. This is conditioned on <code>feature_only</code>, e.g.,</p> <pre><code>def _create_dummynet(pretrained=False, **kwargs):\n    if not kwargs.get(\"features_only\", False):\n        return build_model_with_cfg(DummyNet3, pretrained, **kwargs)\n    else:\n        return build_model_with_cfg(DummyFeatureNet3, pretrained, **kwargs)\n\n@register_model\ndef dummynet18(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; DummyNet:\n    default_cfg = default_cfgs[\"dummynet18\"]\n    model_args = dict(..., num_classes=num_classes, in_channels=in_channels, **kwargs)\n    return _create_dummynet(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre> <p>For real examples, please refer to ResNet and HRNet.</p>"},{"location":"how_to_guides/feature_extraction/#passing-arguements-to-create_model","title":"Passing Arguements to <code>create_model()</code>","text":"<p>After the previous two steps are done, we can simply create the backbone that outputs the desired features by passing <code>feature_only=True</code> and <code>out_indices</code> to <code>create_model()</code>, e.g.,</p> <pre><code>from mindcv.models import create_model\n\n\nbackbone = create_model(\n    model_name=\"dummynet18\",\n    features_only=True,  # set features_only to be True\n    out_indices=[0, 1, 2],  # specify the feature_info indices of the desired layers\n)\n</code></pre> <p>In addtion, if we want to load a checkpoint into the backbone for feature extraction and this backbone falls into scenarios 2, we need to also set <code>auto_mapping=True</code>, e.g.,</p> <pre><code>from mindcv.models import create_model\n\n\nbackbone = create_model(\n    model_name=\"dummynet18\",\n    checkpoint_path=\"/path/to/dummynet18.ckpt\",\n    auto_mapping=True,  # set auto_mapping to be True when loading a checkpoint for scenarios 2 models\n    features_only=True,  # set features_only to be True\n    out_indices=[0, 1, 2],  # specify the feature_info indices of the desired layers\n)\n</code></pre> <p>Congradulations! Now you have learnt how to apply multi-scale feature extraction to the models in MindCV.</p>"},{"location":"how_to_guides/finetune_with_a_custom_dataset/","title":"Fine-tune with A Custom Dataset","text":"<p>This document introduces the process for fine-tuning a custom dataset in MindCV and the implementation of fine-tuning techniques such as reading the dataset online, setting the learning rate for specific layers, freezing part of the parameters, etc. The main code is in./example/finetune.py, you can make changes to it based on this tutorial as needed.</p> <p>Next, we will use the FGVC-Aircraft dataset as an example to show how to fine-tune the pre-trained model mobilenet v3-small. Fine-Grained Visual Classification of Aircraft is a commonly used fine-grained image Classification benchmark dataset, which contains 10,000 aircraft images from 100 different types of aircraft (a.k.a variants), that is, 100 images for each aircraft type.</p> <p>First, extract the downloaded dataset to . /data folder, the directory structure of the Aircraft dataset is:</p> <pre><code>aircraft\n\u2514\u2500\u2500 data\n    \u251c\u2500\u2500 images\n    \u2502   \u251c\u2500\u2500 image1.jpg\n    \u2502   \u251c\u2500\u2500 image2.jpg\n    \u2502   \u2514\u2500\u2500 ....\n    \u251c\u2500\u2500 images_variant_test.txt\n    \u251c\u2500\u2500 images_variant_trainval.txt\n    \u2514\u2500\u2500 ....\n</code></pre> <p>The folder \"images\" contains all the 10,000 images, and the airplane types and subset names of each image are recorded in images_variant_*.txt. When this dataset is used for fine-tuning, the training set is usually set by annotation file: images_variant_trainval.txt. Hence, the training set should contain 6667 images and the test set should contain 3333 images after the dataset has been split.</p>"},{"location":"how_to_guides/finetune_with_a_custom_dataset/#data-preprocessing","title":"Data Preprocessing","text":""},{"location":"how_to_guides/finetune_with_a_custom_dataset/#read-custom-dataset","title":"Read Custom Dataset","text":"<p>For custom datasets, you can either organize the dataset file directory locally into a tree structure similar to ImageNet, and then use the function <code>create_dataset</code> to read the dataset (offline way), or if your dataset is medium-scale or above, which is not suitable to use offline way, you can also directly read all the images into a mappable or iterable object, replacing the file splitting and the <code>create_dataset</code> steps (online way).</p>"},{"location":"how_to_guides/finetune_with_a_custom_dataset/#read-dataset-offline","title":"Read Dataset Offline","text":"<p>The function <code>create_dataset</code> uses <code>mindspore.Dataset.ImageFolderDataset</code> function to build a dataset object, all images in the same folder will be assigned a same label, which is the folder name. Therefore, the prerequisite for using this function is that the file directory of the source dataset should follow the following tree structure:</p> <pre><code>DATASET_NAME\n    \u251c\u2500\u2500 split1(e.g. train)/\n    \u2502  \u251c\u2500\u2500 class1/\n    \u2502  \u2502   \u251c\u2500\u2500 000001.jpg\n    \u2502  \u2502   \u251c\u2500\u2500 000002.jpg\n    \u2502  \u2502   \u2514\u2500\u2500 ....\n    \u2502  \u2514\u2500\u2500 class2/\n    \u2502      \u251c\u2500\u2500 000001.jpg\n    \u2502      \u251c\u2500\u2500 000002.jpg\n    \u2502      \u2514\u2500\u2500 ....\n    \u2514\u2500\u2500 split2/\n       \u251c\u2500\u2500 class1/\n       \u2502   \u251c\u2500\u2500 000001.jpg\n       \u2502   \u251c\u2500\u2500 000002.jpg\n       \u2502   \u2514\u2500\u2500 ....\n       \u2514\u2500\u2500 class2/\n           \u251c\u2500\u2500 000001.jpg\n           \u251c\u2500\u2500 000002.jpg\n           \u2514\u2500\u2500 ....\n</code></pre> <p>Next, we'll take the annotation file ./aircraft/data/images_variant_trainval.txt as an example, locally generate the file of train set ./aircraft/data/images/trainval/, which meets the request of a tree-structure directory.</p> <pre><code>import os\nimport shutil\n\n\n# only for Aircraft dataset but not a general one\ndef extract_images(images_path, subset_name, annotation_file_path, copy=True):\n    # read the annotation file to get the label of each image\n    def annotations(annotation_file_path):\n        image_label = {}\n        with open(annotation_file_path, \"r\") as f:\n            lines = f.readlines()\n            for line in lines:\n                label = \" \".join(line.split(\" \")[1:]).replace(\"\\n\", \"\").replace(\"/\", \"_\")\n                if label not in image_label.keys():\n                    image_label[label] = []\n                    image_label[label].append(line.split(\" \")[0])\n                else:\n                    image_label[label].append(line.split(\" \")[0])\n        return image_label\n\n    # make a new folder for subset\n    subset_path = images_path + subset_name\n    os.mkdir(subset_path)\n\n    # extract and copy/move images to the new folder\n    image_label = annotations(annotation_file_path)\n    for label in image_label.keys():\n        label_folder = subset_path + \"/\" + label\n        os.mkdir(label_folder)\n        for image in image_label[label]:\n            image_name = image + \".jpg\"\n            if copy:\n                shutil.copy(images_path + image_name, label_folder + image_name)\n            else:\n                shutil.move(images_path + image_name, label_folder)\n\n\n# take train set of aircraft dataset as an example\nimages_path = \"./aircraft/data/images/\"\nsubset_name = \"trainval\"\nannotation_file_path = \"./aircraft/data/images_variant_trainval.txt\"\nextract_images(images_path, subset_name, annotation_file_path)\n</code></pre> <p>The splitting method of the test set is the same as that of the training set. The file structure of the whole Aircraft dataset should be:</p> <pre><code>aircraft\n\u2514\u2500\u2500 data\n    \u2514\u2500\u2500 images\n        \u251c\u2500\u2500 trainval\n        \u2502   \u251c\u2500\u2500 707-320\n        \u2502   \u2502   \u251c\u2500\u2500 0056978.jpg\n        \u2502   \u2502   \u2514\u2500\u2500 ....\n        \u2502   \u251c\u2500\u2500 727-200\n        \u2502   \u2502   \u251c\u2500\u2500 0048341.jpg\n        \u2502   \u2502   \u2514\u2500\u2500 ....\n        \u2502   \u2514\u2500\u2500 ....\n        \u2514\u2500\u2500 test\n            \u251c\u2500\u2500 707-320\n            \u2502   \u251c\u2500\u2500 0062765.jpg\n            \u2502   \u2514\u2500\u2500 ....\n            \u251c\u2500\u2500 727-200\n            \u2502   \u251c\u2500\u2500 0061581.jpg\n            \u2502   \u2514\u2500\u2500 ....\n            \u2514\u2500\u2500 ....\n</code></pre> <p>./example/finetune.py integrates the whole training pipeline, from pre-processing to the establishment and training of the model: <code>create_dataset</code> -&gt; <code>create_transforms</code> -&gt; <code>create_loader</code> -&gt; <code>create_model</code> -&gt;..., thus, the dataset with the adequate file directory structure can be sent directly to the fine-tuning script to start the subsequent processes including loading dataset and model training by running <code>python ./example/finetune.py --data_dir=./aircraft/data/images/</code> command. For custom datasets, please note that the dataset parameter in the configuration file must be set to an empty string <code>\"\"</code>  in advance.</p>"},{"location":"how_to_guides/finetune_with_a_custom_dataset/#read-dataset-online","title":"Read Dataset Online","text":"<p>Offline data reading takes up extra local disk space to store the newly generated data files. Therefore, when the local storage space is insufficient or the data cannot be backed up to the local environment, the local data files cannot be read using <code>create_dataset</code> directly, you can write a function to read the dataset in an online way.</p> <p>Here's how we generate a random-accessible dataset object that stores the images of the training set and realize a map from indices to data samples:</p> <ul> <li> <p>First, we define a class <code>ImageClsDataset</code> to read the raw data and transform them into a random-accessible dataset:</p> <ul> <li>In the initialization function <code>__init__()</code>, the annotation file path such as ./aircraft/data/images_variant_trainval.txt is taken as input, and used to generate a dictionary <code>self.annotation</code> that stores the one-to-one correspondence between images and tags;</li> <li>Since <code>create_loader</code> will perform a map operation on this iterated object, which does not support string format labels, it is also necessary to generate <code>self.label2id</code> to convert the string format label in <code>self.annotation</code> to integer type;</li> <li>Based on the information stored in <code>self.annotation</code>, we next read each image in the training set as a one-dimensional array  from the folder ./aircraft/data/images/ (the image data must be read as an one-dimensional array due to map operation restrictions in <code>create_loader</code>). The image information and label are stored in <code>self._data</code> and <code>self._label</code> respectively.</li> <li>Next, the mappable object is constructed using the <code>__getitem__</code> function.</li> <li>After writing the ImageClsDataset class, we can pass it the path of the annotation file to instantiate it, and load it as a dataset that can be read by the model through <code>mindspore.dataset.GeneratorDataset</code>. Note that the parameter <code>column_names</code> must be set to be [\"image\", \"label\"] for subsequent reading by other functions. What we've got now is supposed to be the same as what's generated by <code>create_dataset</code>.</li> </ul> </li> </ul> <pre><code>import numpy as np\n\nfrom mindspore.dataset import GeneratorDataset\n\n\nclass ImageClsDataset:\n    def __init__(self, annotation_dir, images_dir):\n        # Read annotations\n        self.annotation = {}\n        with open(annotation_dir, \"r\") as f:\n            lines = f.readlines()\n            for line in lines:\n                image_label = line.replace(\"\\n\", \"\").replace(\"/\", \"_\").split(\" \")\n                image = image_label[0] + \".jpg\"\n                label = \" \".join(image_label[1:])\n                self.annotation[image] = label\n\n        # Transfer string-type label to int-type label\n        self.label2id = {}\n        labels = sorted(list(set(self.annotation.values())))\n        for i in labels:\n            self.label2id[i] = labels.index(i)\n\n        for image, label in self.annotation.items():\n            self.annotation[image] = self.label2id[label]\n\n        # Read image-labels as mappable object\n        label2images = {key: [] for key in self.label2id.values()}\n        for image, label in self.annotation.items():\n            read_image = np.fromfile(images_dir + image, dtype=np.uint8)\n            label2images[label].append(read_image)\n\n        self._data = sum(list(label2images.values()), [])\n        self._label = sum([[i] * len(label2images[i]) for i in label2images.keys()], [])\n\n    # make class ImageClsDataset a mappable object\n    def __getitem__(self, index):\n        return self._data[index], self._label[index]\n\n    def __len__(self):\n        return len(self._data)\n\n\n# take aircraft dataset as an example\nannotation_dir = \"./aircraft/data/images_variant_trainval.txt\"\nimages_dir = \"./aircraft/data/images/\"\ndataset = ImageClsDataset(annotation_dir, images_dir)\ndataset_train = GeneratorDataset(source=dataset, column_names=[\"image\", \"label\"], shuffle=True)\n</code></pre> <p>Compared with the offline way, the online way skipped the step of splitting the data file locally and reading the local file with the <code>create_dataset</code> function. So in the subsequent training, simply replace the part of finetune.py that uses <code>create_dataset</code> with the above code, then you can start training by running finetune.py directly as what you do after reading the dataset offline.</p>"},{"location":"how_to_guides/finetune_with_a_custom_dataset/#augmentation-and-batching","title":"Augmentation and Batching","text":"<p>MindCV uses the <code>create_loader</code> function to perform data augmentation and batching for the dataset read in the previous chapter. Augmentation strategies are defined in advance by the <code>create_transforms</code> function. Batching is set by the parameter <code>batch_size</code> in the <code>create_loader</code> function. All hyperparameters mentioned above can be passed through the model configuration file. Hyper-parameters' specific usage see the API documentation.</p> <p>For small-size custom datasets, it is suggested that data augmentation can be used to the training set to enhance the generalization of the model and prevent overfitting. For the dataset of fine-grained image classification tasks, such as the Aircraft dataset in this tutorial, the classification effect may be not that ideal due to the large variance within the data class, the image size can be set larger by adjusting the hyper-parameter <code>image_resize</code> (such as 448, 512, 600, etc.).</p>"},{"location":"how_to_guides/finetune_with_a_custom_dataset/#fine-tuning","title":"Fine-tuning","text":"<p>Referring to Stanford University CS231n, fine-tuning all the parameters, freezing feature network, and setting learning rates for specific layers are commonly used fine-tuning skills. The first one uses pre-trained weights to initialize the parameters of the target model, and then updates all parameters based on the new dataset, so it's usually time-consuming but will get a high precision. Freezing feature networks are divided into freezing all feature networks(linear probe) and freezing partial feature networks. The former uses the pre-trained model as a feature extractor and only updates the parameters of the full connection layer, which takes a short time but has low accuracy; The latter generally freezes the parameters of shallow layers, which only learn the basic features of images, and only updates the parameters of the deep network and the full connection layer. Setting learning rate for specific layers is similar but more elaborate, it specifies the learning rates used by certain layers during training.</p> <p>For hyper-parameters used in fine-tuning training, you can refer to the configuration file used when pre-training on the ImageNet-1k dataset in ./configs. Note that for fine-tuning, the hyper-parameter <code>pretrained</code> should be set to be <code>True</code> to load the pre-training weight,  <code>num_classes</code>\u00a0should be set to be the number of labels of the custom dataset (e.g. 100 for the Aircraft dataset here), moreover, don't forget to reduce batch_size and epoch_size based on the size of the custom dataset. In addition, since the pre-trained weight already contains a lot of information for identifying images, in order not to destroy this information too much, it is also necessary to reduce the learning rate <code>lr</code> , and it is also recommended to start training and adjust from at most one-tenth of the pre-trained learning rate or 0.0001. These parameters can be modified in the configuration file or added in the shell command as shown below. The training results can be viewed in the file ./ckpt/results.txt.</p> <pre><code>python .examples/finetune/finetune.py --config=./configs/mobilenetv3/mobilnet_v3_small_ascend.yaml --data_dir=./aircraft/data --pretrained=True\n</code></pre> <p>When fine-tuning mobilenet v3-small based on Aircraft dataset, this tutorial mainly made the following changes to the hyper-parameters:</p> Hyper-parameter Pretrain Fine-tune dataset \"imagenet\" \"\" batch_size 75 8 image_resize 224 600 auto_augment - \"randaug-m7-mstd0.5\" num_classes 1000 100 pretrained False True epoch_size 470 50 lr 0.77 0.002"},{"location":"how_to_guides/finetune_with_a_custom_dataset/#fine-tuning-all-the-parameters","title":"Fine-tuning All the Parameters","text":"<p>Since the progress of this type of fine-tuning is the same as training from scratch, simply start the training by running finetune.py and adjust the parameters as training from scratch.</p>"},{"location":"how_to_guides/finetune_with_a_custom_dataset/#freeze-feature-network","title":"Freeze Feature Network","text":""},{"location":"how_to_guides/finetune_with_a_custom_dataset/#linear-probe","title":"Linear Probe","text":"<p>We prevent parameters from updating by setting <code>requires_grad=False</code> for all parameters except those in the full connection layer. In finetune.py, add the following code after <code>create_model</code> :</p> <pre><code>from mindcv.models.registry import _model_pretrained_cfgs\n\n# ...create_model()\n\n# number of parameters to be updated\nnum_params = 2\n\n# read names of parameters in FC layer\nclassifier_names = [_model_pretrained_cfgs[args.model][\"classifier\"] + \".weight\",\n                    _model_pretrained_cfgs[args.model][\"classifier\"] + \".bias\"]\n\n# prevent parameters in network(except the classifier) from updating\nfor param in network.trainable_params():\n    if param.name not in classifier_names:\n        param.requires_grad = False\n</code></pre>"},{"location":"how_to_guides/finetune_with_a_custom_dataset/#freeze-part-of-the-feature-network","title":"Freeze Part of the Feature Network","text":"<p>To balance the speed and precision of fine-tuning, we can also fix some target network parameters and train the parameters in the deep layer only. It is necessary to extract the parameter names in those layers to be frozen and slightly modify the code in the last chapter. By printing the result of <code>create_model</code> -- <code>network</code>, we can see that in MindCV, each layer of the network of mobilenet v3-small is named with <code>features.*</code>. Suppose that we freeze only the first 7 layers of the network, add the following code after <code>create_model</code>:</p> <pre><code># ...create_model()\n\n# read names of network layers\nfreeze_layer=[\"features.\"+str(i) for i in range(7)]\n\n# prevent parameters in the first 7 layers of the network from updating\nfor param in network.trainable_params():\n    for layer in freeze_layer:\n        if layer in param.name:\n            param.requires_grad = False\n</code></pre>"},{"location":"how_to_guides/finetune_with_a_custom_dataset/#set-learning-rate-for-specific-layers","title":"Set Learning Rate for Specific Layers","text":"<p>To further improve the training accuracy of a fine-tuned model, we can set different learning rates for different layers in the network. This is because the shallow part of the network generally recognizes common contours or features, so even if parameters in this part will be updated, the learning rate should be set relatively small; The deep part generally recognizes the detailed personal characteristics of an object, so the learning rate can be set relatively large; Compared with the feature network that needs to retain the pre-training information as much as possible, the classifier needs to be trained from the beginning, hence the learning rate can be appropriately increased. Since this operation is elaborate, we need to enter finetune.py to specify the parameter names of specific layers and the corresponding learning rates.</p> <p>MindCV uses <code>create_optimizer</code> to generate the optimizer and passes the learning rate to the optimizer. To set the tiered learning rate, simply change the <code>params</code> parameter of <code>create_optimizer</code> function in finetune.py from <code>network.trainable_params()</code> to a list containing the names of the specific parameters and the corresponding learning rate, which you can refer to the API documentation of optimizers. The specific structure of the network and the parameter names in each layer can be viewed by printing the result of <code>create_model</code> -- <code>network</code>.</p> <p>Tips: You can also use the same operation to set different weight_decay for parameters.</p>"},{"location":"how_to_guides/finetune_with_a_custom_dataset/#set-learning-rate-for-classifier","title":"Set Learning Rate for Classifier","text":"<p>Taking mobilenet v3-small as an example, the model classifier name starts with \"classifier\", so if we only increase the learning rate of the classifier, we need to specify it at each step of training. <code>lr_scheduler</code> is a learning rate list generated by <code>create_scheduler</code>, which contains the learning rate at each step of training. Suppose we adjust the learning rate of the classifier to 1.2 times that on the feature network. The changes to the finetune.py code are as follows:</p> <pre><code># ...\n\n\n# Note: a)the params-lr dict must contain all the parameters. b)Also, you're recommended to set a dict with a key \"order_params\" to make sure the parameters will be updated in the right order.\nparams_lr_group = [{\"params\": list(filter(lambda x: 'classifier' in x.name, network.trainable_params())),\n                    \"lr\": [i*1.2 for i in lr_scheduler]},\n                   {\"params\": list(filter(lambda x: 'classifier' not in x.name, network.trainable_params())),\n                    \"lr\": lr_scheduler},\n                   {\"order_params\": network.trainable_params()}]\n\noptimizer = create_optimizer(params_lr_group,\n                             opt=args.opt,\n                             lr=lr_scheduler,\n                             ...)\n</code></pre>"},{"location":"how_to_guides/finetune_with_a_custom_dataset/#set-learning-rate-for-any-layers-in-feature-network","title":"Set Learning Rate for Any Layers in Feature Network","text":"<p>Similar to adjusting the learning rate of a classifier alone, setting the learning rate of layers in feature network requires a list specifying the learning rate for each layer. Assuming that we only increase the learning rate of the last three layers of the feature network (with prefix features.13, features.14, features.15), the code for creating the optimizer in finetune.py will be changed as follows:</p> <pre><code># ...\n\n\n# Note: a)the params-lr dict must contain all the parameters. b)Also, you're recommended to set a dict with a key \"order_params\" to make sure the parameters will be updated in the right order.\nparams_lr_group = [{\"params\": list(filter(lambda x: 'features.13' in x.name, network.trainable_params())),\n                    \"lr\": [i * 1.05 for i in lr_scheduler]},\n                   {\"params\": list(filter(lambda x: 'features.14' in x.name, network.trainable_params())),\n                    \"lr\": [i * 1.1 for i in lr_scheduler]},\n                   {\"params\": list(filter(lambda x: 'features.15' in x.name, network.trainable_params())),\n                    \"lr\": [i * 1.15 for i in lr_scheduler]},\n                   {\"params\": list(filter(\n                       lambda x: \".\".join(x.name.split(\".\")[:2]) not in [\"features.13\", \"features.14\", \"features.15\"],\n                       network.trainable_params())),\n                    \"lr\": lr_scheduler},\n                   {\"order_params\": network.trainable_params()}]\n\noptimizer = create_optimizer(params_lr_group,\n                             opt=args.opt,\n                             lr=lr_scheduler,\n                             ...)\n</code></pre>"},{"location":"how_to_guides/finetune_with_a_custom_dataset/#evaluation","title":"Evaluation","text":"<p>After training, use the model weights stored in <code>*_best-ckpt</code> format in the./ckpt folder to evaluate the performance of the network on the test set. Just run validate.py and pass the file path of the model configuration file as well as the model weight to it:</p> <pre><code>python validate.py --config=./configs/mobilenetv3/mobilnet_v3_small_ascend.yaml --data_dir=./aircraft/data --ckpt_path=./ckpt/mobilenet_v3_small_100_best.ckpt\n</code></pre> <p>The following table summarizes the Top-1 accuracy of the fine-tuned mobilenet v3-small on the Aircraft dataset with the same training configuration but different fine-tuning skills:</p> Network Freeze All the Feature Work Freeze Shallow Part of Feature Network Full Fine-tuning Full Fine-tuning with Increasing Learning Rate of Classifier Full Fine-tuning with Increasing Learning Rate of Deep Layers mobilenet v3-small 48.66% 76.83% 88.35% 88.89% 88.68%"},{"location":"how_to_guides/finetune_with_a_custom_dataset/#prediction","title":"Prediction","text":"<p>Refer to this section of the MindCV fine-tuning tutorial: visual model reasoning results, or add the following code in validate.py to generate a text file ./ckpt/pred.txt that stores the true and predicted labels of the test set:</p> <pre><code># ... after model.eval()\n\n# predited label\npred = np.argmax(model.predict(images).asnumpy(), axis=1)\n\n# real label\nimages, labels = next(loader_eval.create_tuple_iterator())\n\n# write pred.txt\nprediction = np.array([pred, labels]).transpose()\nnp.savetxt(\"./ckpt/pred.txt\", prediction, fmt=\"%s\", header=\"pred \\t real\")\n</code></pre>"},{"location":"how_to_guides/finetune_with_a_custom_dataset/#appendix","title":"Appendix","text":"<p>The following table shows the Top-1 accuracy (%) of full-model fine-tuning on the Aircraft dataset on several CNNs. For the classification accuracy that can be achieved on this dataset, see Aircraft Leaderboard and Paper With Code.</p> Network Full Fine-tuning Accuracy with Mindcv Accuracy in Papers mobilenet v3-small 88.35% - mobilenet v3-large 92.22% 83.8% convnext-tiny 93.69% 84.23% resnest50 86.82% -"},{"location":"how_to_guides/write_a_new_model/","title":"Write A New Model","text":"<p>This document provides a reference template for writing the model definition file <code>model.py</code> in the MindSpore, aiming to provide a unified code style.</p> <p>Next, let's take <code>MLP-Mixer</code> as an example.</p>"},{"location":"how_to_guides/write_a_new_model/#file-header","title":"File Header","text":"<p>A brief description of the document. Include the model name and paper title. As follows:</p> <pre><code>\"\"\"\nMindSpore implementation of `${MODEL_NAME}`.\nRefer to ${PAPER_NAME}.\n\"\"\"\n</code></pre>"},{"location":"how_to_guides/write_a_new_model/#module-import","title":"Module Import","text":"<p>There are three types of module imports. Respectively</p> <ul> <li>Python native or third-party libraries. For example, <code>import math</code> and <code>import numpy as np</code>. It should be placed in the first echelon.</li> <li>MindSpore related modules. For example, <code>import mindspore.nn as nn</code> and <code>import mindspore.ops as ops</code>. It should be placed in the second echelon.</li> <li>The module in the MindCV package. For example, <code>from .layers.classifier import ClassifierHead</code>. It should be placed in the third echelon and use relative import.</li> </ul> <p>Examples are as follows:</p> <pre><code>import math\nfrom collections import OrderedDict\n\nimport mindspore.nn as nn\nimport mindspore.ops as ops\nimport mindspore.common.initializer as init\n\nfrom .utils import load_pretrained\nfrom .layers.classifier import ClassifierHead\n</code></pre> <p>Only import necessary modules or packages to avoid importing useless packages.</p>"},{"location":"how_to_guides/write_a_new_model/#__all__","title":"<code>__all__</code>","text":"<p>Python has no native visibility control, its visibility is maintained by a set of \"conventions\" that everyone should consciously abide by <code>__all__</code> is a convention for exposing interfaces to modules and provides a \"white list\" to expose the interface. If <code>__all__</code> is defined, other files use <code>from xxx import *</code> to import this file, only the members listed in <code>__all__</code> will be imported, and other members can be excluded.</p> <p>We agree that the exposed interfaces in the model include the main model class and functions that return models of different specifications, such as:</p> <pre><code>__all__ = [\n    \"MLPMixer\",\n    \"mlp_mixer_s_p32\",\n    \"mlp_mixer_s_p16\",\n    ...\n]\n</code></pre> <p>Where <code>MLPMixer</code> is the main model class, and <code>mlp_mixer_s_p32</code> and <code>mlp_mixer_s_p16</code> are functions that return models of different specifications. Generally speaking, a submodel, that is, a <code>Layer</code> or a <code>Block</code>, should not be shared by other files. If this is the case, you should consider extracting the submodel under <code>${MINDCLS}/models/layers</code> as a common module, such as <code>SEBlock</code>.</p>"},{"location":"how_to_guides/write_a_new_model/#submodel","title":"Submodel","text":"<p>We all know that a depth model is a network composed of multiple layers. Some of these layers can form sub-models of the same topology, which we generally call <code>Layer</code> or <code>Block</code>, such as <code>ResidualBlock</code>. This kind of abstraction is conducive to our understanding of the whole model structure and is also conducive to code writing.</p> <p>We should briefly describe the function of the sub-model through class annotations. In <code>MindSpore</code>, the model class inherits from <code>nn.Cell</code>. Generally speaking, we need to overload the following two functions:</p> <ul> <li>In the <code>__init__</code> function, we should define the neural network layer that needs to be used in the model (the parameters in <code>__init__</code> should be declared with parameter types, that is, type hint).</li> <li>In the <code>construct</code> function, we define the model forward logic.</li> </ul> <p>Examples are as follows:</p> <pre><code>class MixerBlock(nn.Cell):\n    \"\"\"Mixer Layer with token-mixing MLP and channel-mixing MLP\"\"\"\n\n    def __init__(self,\n                 n_patches: int,\n                 n_channels: int,\n                 token_dim: int,\n                 channel_dim: int,\n                 dropout: float = 0.\n                 ) -&gt; None:\n        super().__init__()\n        self.token_mix = nn.SequentialCell(\n            nn.LayerNorm((n_channels,)),\n            TransPose((0, 2, 1)),\n            FeedForward(n_patches, token_dim, dropout),\n            TransPose((0, 2, 1))\n        )\n        self.channel_mix = nn.SequentialCell(\n            nn.LayerNorm((n_channels,)),\n            FeedForward(n_channels, channel_dim, dropout),\n        )\n\n    def construct(self, x):\n        x = x + self.token_mix(x)\n        x = x + self.channel_mix(x)\n        return x\n</code></pre> <p>In the process of compiling the <code>nn.Cell</code> class, there are two noteworthy aspects</p> <ul> <li> <p>CellList &amp; SequentialCell</p> </li> <li> <p>CellList is just a container that contains a list of neural network layers(Cell). The Cells contained by it can be properly registered and will be visible by all Cell methods. We must overwrite the forward calculation, that is, the construct function.</p> </li> <li> <p>SequentialCell is a container that holds a sequential list of layers(Cell). The Cells may have a name(OrderedDict) or not(List). We don't need to implement forward computation, which is done according to the order of the sequential list.</p> </li> <li> <p>construct</p> </li> <li> <p>Assert is not supported. [RuntimeError: ParseStatement] Unsupported statement 'Assert'.</p> </li> <li> <p>Usage of single operator. When calling an operator (such as concat, reshape, mean), use the functional interface mindspore.ops.functional (such as output=ops.concat((x1, x2)) to avoid instantiating the original operator ops.Primary (such as self.Concat()) in init before calling it in construct (output=self.concat((x1, x2)).</p> </li> </ul>"},{"location":"how_to_guides/write_a_new_model/#master-model","title":"Master Model","text":"<p>The main model is the network model definition proposed in the paper, which is composed of multiple sub-models. It is the top-level network suitable for classification, detection, and other tasks. It is basically similar to the submodel in code writing, but there are several differences.</p> <ul> <li>Class annotations. We should give the title and link of the paper here. In addition, since this class is exposed to the outside world, we'd better also add a description of the class initialization parameters. See code below.</li> <li><code>forward_features</code> function. The operational definition of the characteristic network of the model in the function.</li> <li><code>forward_head</code> function. The operation of the classifier of the model is defined in the function.</li> <li><code>construct</code> function. In function call feature network and classifier operation.</li> <li><code>_initialize_weights</code> function. We agree that the random initialization of model parameters is completed by this member function. See code below.</li> </ul> <p>Examples are as follows:</p> <pre><code>class MLPMixer(nn.Cell):\n    r\"\"\"MLP-Mixer model class, based on\n    `\"MLP-Mixer: An all-MLP Architecture for Vision\" &lt;https://arxiv.org/abs/2105.01601&gt;`_\n\n    Args:\n        depth (int) : number of MixerBlocks.\n        patch_size (Union[int, tuple]) : size of a single image patch.\n        n_patches (int) : number of patches.\n        n_channels (int) : channels(dimension) of a single embedded patch.\n        token_dim (int) : hidden dim of token-mixing MLP.\n        channel_dim (int) : hidden dim of channel-mixing MLP.\n        in_channels(int): number the channels of the input. Default: 3.\n        n_classes (int) : number of classification classes. Default: 1000.\n    \"\"\"\n\n    def __init__(self,\n                 depth: int,\n                 patch_size: Union[int, tuple],\n                 n_patches: int,\n                 n_channels: int,\n                 token_dim: int,\n                 channel_dim: int,\n                 in_channels: int = 3,\n                 n_classes: int = 1000,\n                 ) -&gt; None:\n        super().__init__()\n        self.n_patches = n_patches\n        self.n_channels = n_channels\n        # patch with shape of (3, patch_size, patch_size) is embedded to n_channels dim feature.\n        self.to_patch_embedding = nn.SequentialCell(\n            nn.Conv2d(in_chans, n_channels, patch_size, patch_size, pad_mode=\"pad\", padding=0),\n            TransPose(permutation=(0, 2, 1), embedding=True),\n        )\n        self.mixer_blocks = nn.SequentialCell()\n        for _ in range(depth):\n            self.mixer_blocks.append(MixerBlock(n_patches, n_channels, token_dim, channel_dim))\n        self.layer_norm = nn.LayerNorm((n_channels,))\n        self.mlp_head = nn.Dense(n_channels, n_classes)\n        self._initialize_weights()\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.to_patch_embedding(x)\n        x = self.mixer_blocks(x)\n        x = self.layer_norm(x)\n        return ops.mean(x, 1)\n\n    def forward_head(self, x: Tensor)-&gt; Tensor:\n        return self.mlp_head(x)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        return self.forward_head(x)\n\n    def _initialize_weights(self) -&gt; None:\n        for name, m in self.cells_and_names():\n            if isinstance(m, nn.Conv2d):\n                m.weight.set_data(init.initializer(init.Normal(0.01, 0), m.weight.shape))\n                if m.bias is not None:\n                    m.bias.set_data(init.initializer(init.Constant(0), m.bias.shape))\n            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n                m.gamma.set_data(init.initializer(init.Constant(1), m.gamma.shape))\n                if m.beta is not None:\n                    m.beta.set_data(init.initializer(init.Constant(0.0001), m.beta.shape))\n            elif isinstance(m, nn.Dense):\n                m.weight.set_data(init.initializer(init.Normal(0.01, 0), m.weight.shape))\n                if m.bias is not None:\n                    m.bias.set_data(init.initializer(init.Constant(0), m.bias.shape))\n</code></pre>"},{"location":"how_to_guides/write_a_new_model/#specification-function","title":"Specification Function","text":"<p>The model proposed in the paper may have different specifications, such as the size of the <code>channel</code>, the size of the <code>depth</code>, and so on. The specific configuration of these variants should be reflected through the specification function. The specification interface parameters: pretrained, num_classes, in_channels should be named uniformly. At the same time, the pretrain loading operation should be performed in the specification function. Each specification function corresponds to a specification variant that determines the configuration. The configuration transfers the definition of the main model class through the input parameter and returns the instantiated main model class. In addition, you need to register this specification of the model in the package by adding the decorator <code>@register_model</code>.</p> <p>Examples are as follows:</p> <pre><code>@register_model\ndef mlp_mixer_s_p16(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs):\n    nl, pr, ls, hs, ds, dc = 8, 16, 196, 512, 256, 2048\n    _check_resolution_and_length_of_patch(pr, ls)\n    model = MLPMixer(depth=nl, patch_size=pr, n_patches=ls, n_channels=hs, token_dim=ds,\n                    channel_dim=dc, in_chans=in_chans, n_classes=num_classes, **kwargs)\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n    return model\n\n@register_model\ndef mlp_mixer_b_p32(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs):\n    nl, pr, ls, hs, ds, dc = 12, 32, 49, 768, 384, 3072\n    _check_resolution_and_length_of_patch(pr, ls)\n    model = MLPMixer(depth=nl, patch_size=pr, n_patches=ls, n_channels=hs, token_dim=ds,\n                    channel_dim=dc, in_chans=in_chans, n_classes=num_classes, **kwargs)\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n    return model\n</code></pre>"},{"location":"how_to_guides/write_a_new_model/#verify-main-optional","title":"Verify Main (Optional)","text":"<p>The initial writing phase should ensure that the model is operational. The following code blocks can be used for basic verification:</p> <pre><code>if __name__ == '__main__':\n    import numpy as np\n    import mindspore\n    from mindspore import Tensor\n\n    model = mlp_mixer_s_p16()\n    print(model)\n    dummy_input = Tensor(np.random.rand(8, 3, 224, 224), dtype=mindspore.float32)\n    y = model(dummy_input)\n    print(y.shape)\n</code></pre>"},{"location":"how_to_guides/write_a_new_model/#reference-example","title":"Reference Example","text":"<ul> <li>densenet.py</li> <li>shufflenetv1.py</li> <li>shufflenetv2.py</li> <li>mixnet.py</li> <li>mlp_mixer.py</li> </ul>"},{"location":"notes/changelog/","title":"Change Log","text":"<p>Coming soon.</p>"},{"location":"notes/code_of_conduct/","title":"Code of Conduct","text":"<p>Coming soon.</p>"},{"location":"notes/contributing/","title":"Contributing","text":""},{"location":"notes/contributing/#mindcv-contributing-guidelines","title":"MindCV Contributing Guidelines","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p>"},{"location":"notes/contributing/#contributor-license-agreement","title":"Contributor License Agreement","text":"<p>It's required to sign CLA before your first code submission to MindCV community.</p> <p>For individual contributor, please refer to ICLA online document for the detailed information.</p>"},{"location":"notes/contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"notes/contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/mindspore-lab/mindcv/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"notes/contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"notes/contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"notes/contributing/#write-documentation","title":"Write Documentation","text":"<p>MindCV could always use more documentation, whether as part of the official MindCV docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"notes/contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/mindspore-lab/mindcv/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are welcome :)</li> </ul>"},{"location":"notes/contributing/#getting-started","title":"Getting Started","text":"<p>Ready to contribute? Here's how to set up <code>mindcv</code> for local development.</p> <ol> <li>Fork the <code>mindcv</code> repo on GitHub.</li> <li>Clone your fork locally:</li> </ol> <pre><code>git clone git@github.com:your_name_here/mindcv.git\n</code></pre> <p>After that, you should add official repository as the upstream repository:</p> <pre><code>git remote add upstream git@github.com:mindspore-lab/mindcv\n</code></pre> <ol> <li>Install your local copy into a conda environment. Assuming you have conda installed, this is how you set up your fork for local development:</li> </ol> <pre><code>conda create -n mindcv python=3.8\nconda activate mindcv\ncd mindcv\npip install -e .\n</code></pre> <ol> <li>Create a branch for local development:</li> </ol> <pre><code>git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> <ol> <li>When you're done making changes, check that your changes pass the linters and the tests:</li> </ol> <pre><code>pre-commit run --show-diff-on-failure --color=always --all-files\npytest\n</code></pre> <p>If all static linting are passed, you will get output like:</p> <p></p> <p>otherwise, you need to fix the warnings according to the output:</p> <p></p> <p>To get pre-commit and pytest, just pip install them into your conda environment.</p> <ol> <li>Commit your changes and push your branch to GitHub:</li> </ol> <pre><code>git add .\ngit commit -m \"Your detailed description of your changes.\"\ngit push origin name-of-your-bugfix-or-feature\n</code></pre> <ol> <li>Submit a pull request through the GitHub website.</li> </ol>"},{"location":"notes/contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated. Put    your new functionality into a function with a docstring, and add the    feature to the list in README.md.</li> <li>The pull request should work for Python 3.7, 3.8 and 3.9, and for PyPy. Check    https://github.com/mindspore-lab/mindcv/actions    and make sure that the tests pass for all supported Python versions.</li> </ol>"},{"location":"notes/contributing/#tips","title":"Tips","text":"<p>You can install the git hook scripts instead of linting with <code>pre-commit run -a</code> manually.</p> <p>run flowing command to set up the git hook scripts</p> <pre><code>pre-commit install\n</code></pre> <p>now <code>pre-commit</code> will run automatically on <code>git commit</code>!</p>"},{"location":"notes/contributing/#releasing","title":"Releasing","text":"<p>A reminder for the maintainers on how to deploy. Make sure all your changes are committed (including an entry in HISTORY.md). Then run:</p> <pre><code>bump2version patch # possible: major / minor / patch\ngit push\ngit push --tags\n</code></pre> <p>GitHub Action will then deploy to PyPI if tests pass.</p>"},{"location":"notes/faq/","title":"FAQ","text":"<p>Coming soon.</p>"},{"location":"reference/data/","title":"Data","text":""},{"location":"reference/data/#auto-augmentation","title":"Auto Augmentation","text":""},{"location":"reference/data/#mindcv.data.auto_augment.auto_augment_transform","title":"<code>mindcv.data.auto_augment.auto_augment_transform(configs, hparams)</code>","text":"<p>Create a AutoAugment transform Args:     configs: A string that defines the automatic augmentation configuration.         It is composed of multiple parts separated by dashes (\"-\"). The first part defines         the AutoAugment policy ('autoaug', 'autoaugr' or '3a':         'autoaug' for the original AutoAugment policy with PosterizeOriginal,         'autoaugr' for the AutoAugment policy with PosterizeIncreasing operation,          '3a' for the AutoAugment only with 3 augmentations.)         There is no order requirement for the remaining config parts.</p> <pre><code>    - mstd: Float standard deviation of applied magnitude noise.\n\n    Example: 'autoaug-mstd0.5' will be automatically augment using the autoaug strategy\n    and magnitude_std 0.5.\nhparams: Other hparams of the automatic augmentation scheme.\n</code></pre> Source code in <code>mindcv/data/auto_augment.py</code> <pre><code>def auto_augment_transform(configs, hparams):\n    \"\"\"\n    Create a AutoAugment transform\n    Args:\n        configs: A string that defines the automatic augmentation configuration.\n            It is composed of multiple parts separated by dashes (\"-\"). The first part defines\n            the AutoAugment policy ('autoaug', 'autoaugr' or '3a':\n            'autoaug' for the original AutoAugment policy with PosterizeOriginal,\n            'autoaugr' for the AutoAugment policy with PosterizeIncreasing operation,\n             '3a' for the AutoAugment only with 3 augmentations.)\n            There is no order requirement for the remaining config parts.\n\n            - mstd: Float standard deviation of applied magnitude noise.\n\n            Example: 'autoaug-mstd0.5' will be automatically augment using the autoaug strategy\n            and magnitude_std 0.5.\n        hparams: Other hparams of the automatic augmentation scheme.\n    \"\"\"\n    config = configs.split(\"-\")\n    policy_name = config[0]\n    config = config[1:]\n    hparams.setdefault(\"magnitude_std\", 0.5)  # default magnitude_std is set to 0.5\n    for c in config:\n        cs = re.split(r\"(\\d.*)\", c)\n        if len(cs) &lt; 2:\n            continue\n        key, val = cs[:2]\n        if key == \"mstd\":\n            # noise param injected via hparams for now\n            hparams.setdefault(\"magnitude_std\", float(val))\n        else:\n            assert False, \"Unknown AutoAugment config section\"\n    aa_policy = auto_augment_policy(policy_name, hparams=hparams)\n    return AutoAugment(aa_policy)\n</code></pre>"},{"location":"reference/data/#mindcv.data.auto_augment.rand_augment_transform","title":"<code>mindcv.data.auto_augment.rand_augment_transform(configs, hparams)</code>","text":"<p>Create a RandAugment transform Args:     configs: A string that defines the random augmentation configuration.         It is composed of multiple parts separated by dashes (\"-\").         The first part defines the AutoAugment policy ('randaug' policy).         There is no order requirement for the remaining config parts.</p> <pre><code>    - m: Integer magnitude of rand augment. Default: 10\n    - n: Integer num layer (number of transform operations selected for each image). Default: 2\n    - w: Integer probability weight index (the index that affects a group of weights selected by operations).\n    - mstd: Floating standard deviation of applied magnitude noise,\n        or uniform sampling at infinity (or greater than 100).\n    - mmax: Set the upper range limit for magnitude to a value\n        other than the default value of _LEVEL_DENOM (10).\n    - inc: Integer (bool), using the severity increase with magnitude (default: 0).\n\n    Example: 'randaug-w0-n3-mstd0.5' will be random augment\n        using the weights 0, num_layers 3, magnitude_std 0.5.\nhparams: Other hparams (kwargs) for the RandAugmentation scheme.\n</code></pre> Source code in <code>mindcv/data/auto_augment.py</code> <pre><code>def rand_augment_transform(configs, hparams):\n    \"\"\"\n    Create a RandAugment transform\n    Args:\n        configs: A string that defines the random augmentation configuration.\n            It is composed of multiple parts separated by dashes (\"-\").\n            The first part defines the AutoAugment policy ('randaug' policy).\n            There is no order requirement for the remaining config parts.\n\n            - m: Integer magnitude of rand augment. Default: 10\n            - n: Integer num layer (number of transform operations selected for each image). Default: 2\n            - w: Integer probability weight index (the index that affects a group of weights selected by operations).\n            - mstd: Floating standard deviation of applied magnitude noise,\n                or uniform sampling at infinity (or greater than 100).\n            - mmax: Set the upper range limit for magnitude to a value\n                other than the default value of _LEVEL_DENOM (10).\n            - inc: Integer (bool), using the severity increase with magnitude (default: 0).\n\n            Example: 'randaug-w0-n3-mstd0.5' will be random augment\n                using the weights 0, num_layers 3, magnitude_std 0.5.\n        hparams: Other hparams (kwargs) for the RandAugmentation scheme.\n    \"\"\"\n    magnitude = _LEVEL_DENOM  # default to _LEVEL_DENOM for magnitude (currently 10)\n    num_layers = 2  # default to 2 ops per image\n    hparams.setdefault(\"magnitude_std\", 0.5)  # default magnitude_std is set to 0.5\n    weight_idx = None  # default to no probability weights for op choice\n    transforms = _RAND_TRANSFORMS\n    config = configs.split(\"-\")\n    assert config[0] == \"randaug\"\n    config = config[1:]\n    for c in config:\n        cs = re.split(r\"(\\d.*)\", c)\n        if len(cs) &lt; 2:\n            continue\n        key, val = cs[:2]\n        if key == \"mstd\":\n            # noise param / randomization of magnitude values\n            mstd = float(val)\n            if mstd &gt; 100:\n                # use uniform sampling in 0 to magnitude if mstd is &gt; 100\n                mstd = float(\"inf\")\n            hparams.setdefault(\"magnitude_std\", mstd)\n        elif key == \"mmax\":\n            # clip magnitude between [0, mmax] instead of default [0, _LEVEL_DENOM]\n            hparams.setdefault(\"magnitude_max\", int(val))\n        elif key == \"inc\":\n            if bool(val):\n                transforms = _RAND_INCREASING_TRANSFORMS\n        elif key == \"m\":\n            magnitude = int(val)\n        elif key == \"n\":\n            num_layers = int(val)\n        elif key == \"w\":\n            weight_idx = int(val)\n        else:\n            assert False, \"Unknown RandAugment config section\"\n    ra_ops = rand_augment_ops(magnitude=magnitude, hparams=hparams, transforms=transforms)\n    choice_weights = None if weight_idx is None else _select_rand_weights(weight_idx)\n    return RandAugment(ra_ops, num_layers, choice_weights=choice_weights)\n</code></pre>"},{"location":"reference/data/#mindcv.data.auto_augment.trivial_augment_wide_transform","title":"<code>mindcv.data.auto_augment.trivial_augment_wide_transform(configs, hparams)</code>","text":"<p>Create a TrivialAugmentWide transform Args:     configs: A string that defines the TrivialAugmentWide configuration.         It is composed of multiple parts separated by dashes (\"-\").         The first part defines the AutoAugment name, it should be 'trivialaugwide'.         the second part(not necessary) the maximum value of magnitude.</p> <pre><code>    - m: final magnitude of a operation will uniform sampling from [0, m] . Default: 31\n\n    Example: 'trivialaugwide-m20' will be TrivialAugmentWide\n    with mgnitude uniform sampling from [0, 20],\nhparams: Other hparams (kwargs) for the TrivialAugment scheme.\n</code></pre> <p>Returns:     A Mindspore compatible Transform</p> Source code in <code>mindcv/data/auto_augment.py</code> <pre><code>def trivial_augment_wide_transform(configs, hparams):\n    \"\"\"\n    Create a TrivialAugmentWide transform\n    Args:\n        configs: A string that defines the TrivialAugmentWide configuration.\n            It is composed of multiple parts separated by dashes (\"-\").\n            The first part defines the AutoAugment name, it should be 'trivialaugwide'.\n            the second part(not necessary) the maximum value of magnitude.\n\n            - m: final magnitude of a operation will uniform sampling from [0, m] . Default: 31\n\n            Example: 'trivialaugwide-m20' will be TrivialAugmentWide\n            with mgnitude uniform sampling from [0, 20],\n        hparams: Other hparams (kwargs) for the TrivialAugment scheme.\n    Returns:\n        A Mindspore compatible Transform\n    \"\"\"\n    magnitude = 31\n    transforms = _TRIVIALAUGMENT_WIDE_TRANSFORMS\n    config = configs.split(\"-\")\n    assert config[0] == \"trivialaugwide\"\n    config = config[1:]\n    for c in config:\n        cs = re.split(r\"(\\d.*)\", c)\n        if len(cs) &lt; 2:\n            continue\n        key, val = cs[:2]\n        if key == \"m\":\n            magnitude = int(val)\n        else:\n            assert False, \"Unknown TrivialAugmentWide config section\"\n    if not hparams:\n        hparams = dict()\n    hparams[\"magnitude_max\"] = magnitude\n    hparams[\"magnitude_std\"] = float(\"inf\")  # default to uniform sampling\n    hparams[\"trivialaugwide\"] = True\n    ta_ops = trivial_augment_wide_ops(magnitude=magnitude, hparams=hparams, transforms=transforms)\n    return TrivialAugmentWide(ta_ops)\n</code></pre>"},{"location":"reference/data/#mindcv.data.auto_augment.augment_and_mix_transform","title":"<code>mindcv.data.auto_augment.augment_and_mix_transform(configs, hparams=None)</code>","text":"<p>Create AugMix PyTorch transform</p> PARAMETER DESCRIPTION <code>configs</code> <p>String defining configuration of AugMix augmentation. Consists of multiple sections separated by dashes ('-'). The first section defines the specific name of augment, it should be 'augmix'. The remaining sections, not order sepecific determine     'm' - integer magnitude (severity) of augmentation mix (default: 3)     'w' - integer width of augmentation chain (default: 3)     'd' - integer depth of augmentation chain (-1 is random [1, 3], default: -1)     'a' - integer or float, the args of beta deviation of beta for generate the weight, default 1.. Ex 'augmix-m5-w4-d2' results in AugMix with severity 5, chain width 4, chain depth 2</p> <p> TYPE: <code>str</code> </p> <code>hparams</code> <p>Other hparams (kwargs) for the Augmentation transforms</p> <p> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <p>A Mindspore compatible Transform</p> Source code in <code>mindcv/data/auto_augment.py</code> <pre><code>def augment_and_mix_transform(configs, hparams=None):\n    \"\"\"Create AugMix PyTorch transform\n\n    Args:\n        configs (str): String defining configuration of AugMix augmentation. Consists of multiple sections separated\n            by dashes ('-'). The first section defines the specific name of augment, it should be 'augmix'.\n            The remaining sections, not order sepecific determine\n                'm' - integer magnitude (severity) of augmentation mix (default: 3)\n                'w' - integer width of augmentation chain (default: 3)\n                'd' - integer depth of augmentation chain (-1 is random [1, 3], default: -1)\n                'a' - integer or float, the args of beta deviation of beta for generate the weight, default 1..\n            Ex 'augmix-m5-w4-d2' results in AugMix with severity 5, chain width 4, chain depth 2\n\n        hparams: Other hparams (kwargs) for the Augmentation transforms\n\n    Returns:\n         A Mindspore compatible Transform\n    \"\"\"\n    magnitude = 3\n    width = 3\n    depth = -1\n    alpha = 1.0\n    config = configs.split(\"-\")\n    assert config[0] == \"augmix\"\n    config = config[1:]\n    for c in config:\n        cs = re.split(r\"(\\d.*)\", c)\n        if len(cs) &lt; 2:\n            continue\n        key, val = cs[:2]\n        if key == \"m\":\n            magnitude = int(val)\n        elif key == \"w\":\n            width = int(val)\n        elif key == \"d\":\n            depth = int(val)\n        elif key == \"a\":\n            alpha = float(val)\n        else:\n            assert False, \"Unknown AugMix config section\"\n    if not hparams:\n        hparams = dict()\n    hparams[\"magnitude_std\"] = float(\"inf\")  # default to uniform sampling (if not set via mstd arg)\n    ops = augmix_ops(magnitude=magnitude, hparams=hparams)\n    return AugMixAugment(ops, alpha=alpha, width=width, depth=depth)\n</code></pre>"},{"location":"reference/data/#dataset-factory","title":"Dataset Factory","text":""},{"location":"reference/data/#mindcv.data.dataset_factory.create_dataset","title":"<code>mindcv.data.dataset_factory.create_dataset(name='', root=None, split='train', shuffle=True, num_samples=None, num_shards=None, shard_id=None, num_parallel_workers=None, download=False, num_aug_repeats=0, **kwargs)</code>","text":"<p>Creates dataset by name.</p> PARAMETER DESCRIPTION <code>name</code> <p>dataset name like MNIST, CIFAR10, ImageNeT, ''. '' means a customized dataset. Default: ''.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>root</code> <p>dataset root dir. Default: None.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>split</code> <p>data split: '' or split name string (train/val/test), if it is '', no split is used. Otherwise, it is a subfolder of root dir, e.g., train, val, test. Default: 'train'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'train'</code> </p> <code>shuffle</code> <p>whether to shuffle the dataset. Default: True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>num_samples</code> <p>Number of elements to sample (default=None, which means sample all elements).</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>num_shards</code> <p>Number of shards that the dataset will be divided into (default=None). When this argument is specified, <code>num_samples</code> reflects the maximum sample number of per shard.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>shard_id</code> <p>The shard ID within <code>num_shards</code> (default=None). This argument can only be specified when <code>num_shards</code> is also specified.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>num_parallel_workers</code> <p>Number of workers to read the data (default=None, set in the config).</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>download</code> <p>whether to download the dataset. Default: False</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>num_aug_repeats</code> <p>Number of dataset repetition for repeated augmentation. If 0 or 1, repeated augmentation is disabled. Otherwise, repeated augmentation is enabled and the common choice is 3. (Default: 0)</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> Note <p>For custom datasets and imagenet, the dataset dir should follow the structure like: .dataset_name/ \u251c\u2500\u2500 split1/ \u2502  \u251c\u2500\u2500 class1/ \u2502  \u2502   \u251c\u2500\u2500 000001.jpg \u2502  \u2502   \u251c\u2500\u2500 000002.jpg \u2502  \u2502   \u2514\u2500\u2500 .... \u2502  \u2514\u2500\u2500 class2/ \u2502      \u251c\u2500\u2500 000001.jpg \u2502      \u251c\u2500\u2500 000002.jpg \u2502      \u2514\u2500\u2500 .... \u2514\u2500\u2500 split2/    \u251c\u2500\u2500 class1/    \u2502   \u251c\u2500\u2500 000001.jpg    \u2502   \u251c\u2500\u2500 000002.jpg    \u2502   \u2514\u2500\u2500 ....    \u2514\u2500\u2500 class2/        \u251c\u2500\u2500 000001.jpg        \u251c\u2500\u2500 000002.jpg        \u2514\u2500\u2500 ....</p> RETURNS DESCRIPTION <p>Dataset object</p> Source code in <code>mindcv/data/dataset_factory.py</code> <pre><code>def create_dataset(\n    name: str = \"\",\n    root: Optional[str] = None,\n    split: str = \"train\",\n    shuffle: bool = True,\n    num_samples: Optional[int] = None,\n    num_shards: Optional[int] = None,\n    shard_id: Optional[int] = None,\n    num_parallel_workers: Optional[int] = None,\n    download: bool = False,\n    num_aug_repeats: int = 0,\n    **kwargs,\n):\n    r\"\"\"Creates dataset by name.\n\n    Args:\n        name: dataset name like MNIST, CIFAR10, ImageNeT, ''. '' means a customized dataset. Default: ''.\n        root: dataset root dir. Default: None.\n        split: data split: '' or split name string (train/val/test), if it is '', no split is used.\n            Otherwise, it is a subfolder of root dir, e.g., train, val, test. Default: 'train'.\n        shuffle: whether to shuffle the dataset. Default: True.\n        num_samples: Number of elements to sample (default=None, which means sample all elements).\n        num_shards: Number of shards that the dataset will be divided into (default=None).\n            When this argument is specified, `num_samples` reflects the maximum sample number of per shard.\n        shard_id: The shard ID within `num_shards` (default=None).\n            This argument can only be specified when `num_shards` is also specified.\n        num_parallel_workers: Number of workers to read the data (default=None, set in the config).\n        download: whether to download the dataset. Default: False\n        num_aug_repeats: Number of dataset repetition for repeated augmentation.\n            If 0 or 1, repeated augmentation is disabled.\n            Otherwise, repeated augmentation is enabled and the common choice is 3. (Default: 0)\n\n    Note:\n        For custom datasets and imagenet, the dataset dir should follow the structure like:\n        .dataset_name/\n        \u251c\u2500\u2500 split1/\n        \u2502  \u251c\u2500\u2500 class1/\n        \u2502  \u2502   \u251c\u2500\u2500 000001.jpg\n        \u2502  \u2502   \u251c\u2500\u2500 000002.jpg\n        \u2502  \u2502   \u2514\u2500\u2500 ....\n        \u2502  \u2514\u2500\u2500 class2/\n        \u2502      \u251c\u2500\u2500 000001.jpg\n        \u2502      \u251c\u2500\u2500 000002.jpg\n        \u2502      \u2514\u2500\u2500 ....\n        \u2514\u2500\u2500 split2/\n           \u251c\u2500\u2500 class1/\n           \u2502   \u251c\u2500\u2500 000001.jpg\n           \u2502   \u251c\u2500\u2500 000002.jpg\n           \u2502   \u2514\u2500\u2500 ....\n           \u2514\u2500\u2500 class2/\n               \u251c\u2500\u2500 000001.jpg\n               \u251c\u2500\u2500 000002.jpg\n               \u2514\u2500\u2500 ....\n\n    Returns:\n        Dataset object\n    \"\"\"\n    name = name.lower()\n    if root is None:\n        root = os.path.join(get_dataset_download_root(), name)\n\n    assert (num_samples is None) or (num_aug_repeats == 0), \"num_samples and num_aug_repeats can NOT be set together.\"\n\n    # subset sampling\n    if num_samples is not None and num_samples &gt; 0:\n        # TODO: rewrite ordered distributed sampler (subset sampling in distributed mode is not tested)\n        if num_shards is not None and num_shards &gt; 1:  # distributed\n            _logger.info(f\"number of shards: {num_shards}, number of samples: {num_samples}\")\n            sampler = DistributedSampler(num_shards, shard_id, shuffle=shuffle, num_samples=num_samples)\n        else:  # standalone\n            if shuffle:\n                sampler = ds.RandomSampler(replacement=False, num_samples=num_samples)\n            else:\n                sampler = ds.SequentialSampler(num_samples=num_samples)\n        mindspore_kwargs = dict(\n            shuffle=None,\n            sampler=sampler,\n            num_parallel_workers=num_parallel_workers,\n            **kwargs,\n        )\n    else:\n        sampler = None\n        mindspore_kwargs = dict(\n            shuffle=shuffle,\n            sampler=sampler,\n            num_shards=num_shards,\n            shard_id=shard_id,\n            num_parallel_workers=num_parallel_workers,\n            **kwargs,\n        )\n\n    # sampler for repeated augmentation\n    if num_aug_repeats &gt; 0:\n        dataset_size = get_dataset_size(name, root, split)\n        _logger.info(\n            f\"Repeated augmentation is enabled, num_aug_repeats: {num_aug_repeats}, \"\n            f\"original dataset size: {dataset_size}.\"\n        )\n        # since drop_remainder is usually True, we don't need to do rounding in sampling\n        sampler = RepeatAugSampler(\n            dataset_size,\n            num_shards=num_shards,\n            rank_id=shard_id,\n            num_repeats=num_aug_repeats,\n            selected_round=0,\n            shuffle=shuffle,\n        )\n        mindspore_kwargs = dict(shuffle=None, sampler=sampler, num_shards=None, shard_id=None, **kwargs)\n\n    # create dataset\n    if name in _MINDSPORE_BASIC_DATASET:\n        dataset_class = _MINDSPORE_BASIC_DATASET[name][0]\n        dataset_download = _MINDSPORE_BASIC_DATASET[name][1]\n        dataset_new_path = None\n        if download:\n            if shard_id is not None:\n                root = os.path.join(root, f\"dataset_{str(shard_id)}\")\n            dataset_download = dataset_download(root)\n            dataset_download.download()\n            dataset_new_path = dataset_download.path\n\n        dataset = dataset_class(\n            dataset_dir=dataset_new_path if dataset_new_path else root,\n            usage=split,\n            **mindspore_kwargs,\n        )\n        # address ms dataset num_classes empty issue\n        if name == \"mnist\":\n            dataset.num_classes = lambda: 10\n        elif name == \"cifar10\":\n            dataset.num_classes = lambda: 10\n        elif name == \"cifar100\":\n            dataset.num_classes = lambda: 100\n\n    else:\n        if name == \"imagenet\" and download:\n            raise ValueError(\n                \"Imagenet dataset download is not supported. \"\n                \"Please download imagenet from https://www.image-net.org/download.php, \"\n                \"and parse the path of dateset directory via args.data_dir.\"\n            )\n\n        if os.path.isdir(root):\n            root = os.path.join(root, split)\n        dataset = ImageFolderDataset(dataset_dir=root, **mindspore_kwargs)\n        \"\"\" Another implementation which a bit slower than ImageFolderDataset\n            imagenet_dataset = ImageNetDataset(dataset_dir=root)\n            sampler = RepeatAugSampler(len(imagenet_dataset), num_shards=num_shards, rank_id=shard_id,\n                                       num_repeats=repeated_aug, selected_round=1, shuffle=shuffle)\n            dataset = ds.GeneratorDataset(imagenet_dataset, column_names=imagenet_dataset.column_names, sampler=sampler)\n        \"\"\"\n    return dataset\n</code></pre>"},{"location":"reference/data/#sampler","title":"Sampler","text":""},{"location":"reference/data/#mindcv.data.distributed_sampler.RepeatAugSampler","title":"<code>mindcv.data.distributed_sampler.RepeatAugSampler</code>","text":"<p>Sampler that restricts data loading to a subset of the dataset for distributed, with repeated augmentation. It ensures that different each augmented version of a sample will be visible to a different process.</p> <p>This sampler was adapted from https://github.com/facebookresearch/deit/blob/0c4b8f60/samplers.py</p> PARAMETER DESCRIPTION <code>dataset_size</code> <p>dataset size.</p> <p> </p> <code>num_shards</code> <p>num devices.</p> <p> DEFAULT: <code>None</code> </p> <code>rank_id</code> <p>device id.</p> <p> DEFAULT: <code>None</code> </p> <code>shuffle(bool)</code> <p>True for using shuffle, False for not using.</p> <p> </p> <code>num_repeats(int)</code> <p>num of repeated instances in repeated augmentation, Default:3.</p> <p> </p> <code>selected_round(int)</code> <p>round the total num of samples by this factor, Defailt:256.</p> <p> </p> Source code in <code>mindcv/data/distributed_sampler.py</code> <pre><code>class RepeatAugSampler:\n    \"\"\"Sampler that restricts data loading to a subset of the dataset for distributed,\n    with repeated augmentation.\n    It ensures that different each augmented version of a sample will be visible to a\n    different process.\n\n    This sampler was adapted from https://github.com/facebookresearch/deit/blob/0c4b8f60/samplers.py\n\n    Args:\n        dataset_size: dataset size.\n        num_shards: num devices.\n        rank_id: device id.\n        shuffle(bool): True for using shuffle, False for not using.\n        num_repeats(int): num of repeated instances in repeated augmentation, Default:3.\n        selected_round(int): round the total num of samples by this factor, Defailt:256.\n    \"\"\"\n\n    def __init__(\n        self,\n        dataset_size,\n        num_shards=None,\n        rank_id=None,\n        shuffle=True,\n        num_repeats=3,\n        selected_round=256,\n    ):\n        if num_shards is None:\n            _logger.warning(\"num_shards is set to 1 in RepeatAugSampler since it is not passed in\")\n            num_shards = 1\n        if rank_id is None:\n            rank_id = 0\n\n        # assert isinstance(num_repeats, int), f'num_repeats should be Type integer, but got {type(num_repeats)}'\n\n        self.dataset_size = dataset_size\n        self.num_shards = num_shards\n        self.rank_id = rank_id\n        self.shuffle = shuffle\n        self.num_repeats = int(num_repeats)\n        self.epoch = 0\n        self.num_samples = int(math.ceil(self.dataset_size * num_repeats / self.num_shards))\n        self.total_size = self.num_samples * self.num_shards\n        # Determine the number of samples to select per epoch for each rank.\n        if selected_round:\n            self.num_selected_samples = int(\n                math.floor(self.dataset_size // selected_round * selected_round / num_shards)\n            )\n        else:\n            self.num_selected_samples = int(math.ceil(self.dataset_size / num_shards))\n\n    def __iter__(self):\n        # deterministically shuffle based on epoch\n        # print('__iter__  generating new shuffled indices: ', self.epoch)\n        if self.shuffle:\n            indices = np.random.RandomState(seed=self.epoch).permutation(self.dataset_size)\n            indices = indices.tolist()\n            self.epoch += 1\n            # print(indices[:30])\n        else:\n            indices = list(range(self.dataset_size))\n        # produce repeats e.g. [0, 0, 0, 1, 1, 1, 2, 2, 2....]\n        indices = [ele for ele in indices for i in range(self.num_repeats)]\n\n        # add extra samples to make it evenly divisible\n        padding_size = self.total_size - len(indices)\n        if padding_size &gt; 0:\n            indices += indices[:padding_size]\n        assert len(indices) == self.total_size\n\n        # subsample per rank\n        indices = indices[self.rank_id : self.total_size : self.num_shards]\n        assert len(indices) == self.num_samples\n\n        # return up to num selected samples\n        return iter(indices[: self.num_selected_samples])\n\n    def __len__(self):\n        return self.num_selected_samples\n\n    def set_epoch(self, epoch):\n        self.epoch = epoch\n</code></pre>"},{"location":"reference/data/#dataloader","title":"DataLoader","text":""},{"location":"reference/data/#mindcv.data.loader.create_loader","title":"<code>mindcv.data.loader.create_loader(dataset, batch_size, drop_remainder=False, is_training=False, mixup=0.0, cutmix=0.0, cutmix_prob=0.0, num_classes=1000, transform=None, target_transform=None, num_parallel_workers=None, python_multiprocessing=False, separate=False)</code>","text":"<p>Creates dataloader.</p> <p>Applies operations such as transform and batch to the <code>ms.dataset.Dataset</code> object created by the <code>create_dataset</code> function to get the dataloader.</p> PARAMETER DESCRIPTION <code>dataset</code> <p>dataset object created by <code>create_dataset</code>.</p> <p> TYPE: <code>Dataset</code> </p> <code>batch_size</code> <p>The number of rows each batch is created with. An int or callable object which takes exactly 1 parameter, BatchInfo.</p> <p> TYPE: <code>int or function</code> </p> <code>drop_remainder</code> <p>Determines whether to drop the last block whose data row number is less than batch size (default=False). If True, and if there are less than batch_size rows available to make the last batch, then those rows will be dropped and not propagated to the child node.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>is_training</code> <p>whether it is in train mode. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>mixup</code> <p>mixup alpha, mixup will be enabled if &gt; 0. (default=0.0).</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>cutmix</code> <p>cutmix alpha, cutmix will be enabled if &gt; 0. (default=0.0). This operation is experimental.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>cutmix_prob</code> <p>prob of doing cutmix for an image (default=0.0)</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>num_classes</code> <p>the number of classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>transform</code> <p>the list of transformations that wil be applied on the image, which is obtained by <code>create_transform</code>. If None, the default imagenet transformation for evaluation will be applied. Default: None.</p> <p> TYPE: <code>list or None</code> DEFAULT: <code>None</code> </p> <code>target_transform</code> <p>the list of transformations that will be applied on the label. If None, the label will be converted to the type of ms.int32. Default: None.</p> <p> TYPE: <code>list or None</code> DEFAULT: <code>None</code> </p> <code>num_parallel_workers</code> <p>Number of workers(threads) to process the dataset in parallel (default=None).</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>python_multiprocessing</code> <p>Parallelize Python operations with multiple worker processes. This option could be beneficial if the Python operation is computational heavy (default=False).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>separate(bool,</code> <p>separate the image clean and the image been transformed. If separate==True, that means the dataset returned has 3 parts: * the first part called image \"clean\", which means the image without auto_augment (e.g., auto-aug) * the second and third parts called image transformed, hence, with the auto_augment transform. Refer to \".transforms_factory.create_transforms\" for more information.</p> <p> TYPE: <code>optional</code> </p> Note <ol> <li>cutmix is now experimental (which means performance gain is not guarantee)     and can not be used together with mixup due to the label int type conflict.</li> <li><code>is_training</code>, <code>mixup</code>, <code>num_classes</code> is used for MixUp, which is a kind of transform operation.   However, we are not able to merge it into <code>transform</code>, due to the limitations of the <code>mindspore.dataset</code> API.</li> </ol> RETURNS DESCRIPTION <p>BatchDataset, dataset batched.</p> Source code in <code>mindcv/data/loader.py</code> <pre><code>def create_loader(\n    dataset,\n    batch_size,\n    drop_remainder=False,\n    is_training=False,\n    mixup=0.0,\n    cutmix=0.0,\n    cutmix_prob=0.0,\n    num_classes=1000,\n    transform=None,\n    target_transform=None,\n    num_parallel_workers=None,\n    python_multiprocessing=False,\n    separate=False,\n):\n    r\"\"\"Creates dataloader.\n\n    Applies operations such as transform and batch to the `ms.dataset.Dataset` object\n    created by the `create_dataset` function to get the dataloader.\n\n    Args:\n        dataset (ms.dataset.Dataset): dataset object created by `create_dataset`.\n        batch_size (int or function): The number of rows each batch is created with. An\n            int or callable object which takes exactly 1 parameter, BatchInfo.\n        drop_remainder (bool, optional): Determines whether to drop the last block\n            whose data row number is less than batch size (default=False). If True, and if there are less\n            than batch_size rows available to make the last batch, then those rows will\n            be dropped and not propagated to the child node.\n        is_training (bool): whether it is in train mode. Default: False.\n        mixup (float): mixup alpha, mixup will be enabled if &gt; 0. (default=0.0).\n        cutmix (float): cutmix alpha, cutmix will be enabled if &gt; 0. (default=0.0). This operation is experimental.\n        cutmix_prob (float): prob of doing cutmix for an image (default=0.0)\n        num_classes (int): the number of classes. Default: 1000.\n        transform (list or None): the list of transformations that wil be applied on the image,\n            which is obtained by `create_transform`. If None, the default imagenet transformation\n            for evaluation will be applied. Default: None.\n        target_transform (list or None): the list of transformations that will be applied on the label.\n            If None, the label will be converted to the type of ms.int32. Default: None.\n        num_parallel_workers (int, optional): Number of workers(threads) to process the dataset in parallel\n            (default=None).\n        python_multiprocessing (bool, optional): Parallelize Python operations with multiple worker processes. This\n            option could be beneficial if the Python operation is computational heavy (default=False).\n        separate(bool, optional): separate the image clean and the image been transformed.\n            If separate==True, that means the dataset returned has 3 parts:\n            * the first part called image \"clean\", which means the image without auto_augment (e.g., auto-aug)\n            * the second and third parts called image transformed, hence, with the auto_augment transform.\n            Refer to \".transforms_factory.create_transforms\" for more information.\n\n    Note:\n        1. cutmix is now experimental (which means performance gain is not guarantee)\n            and can not be used together with mixup due to the label int type conflict.\n        2. `is_training`, `mixup`, `num_classes` is used for MixUp, which is a kind of transform operation.\n          However, we are not able to merge it into `transform`, due to the limitations of the `mindspore.dataset` API.\n\n\n    Returns:\n        BatchDataset, dataset batched.\n    \"\"\"\n\n    if target_transform is None:\n        target_transform = transforms.TypeCast(ms.int32)\n    target_input_columns = \"label\" if \"label\" in dataset.get_col_names() else \"fine_label\"\n    dataset = dataset.map(\n        operations=target_transform,\n        input_columns=target_input_columns,\n        num_parallel_workers=num_parallel_workers,\n        python_multiprocessing=python_multiprocessing,\n    )\n\n    if transform is None:\n        warnings.warn(\n            \"Using None as the default value of transform will set it back to \"\n            \"traditional image transform, which is not recommended. \"\n            \"You should explicitly call `create_transforms` and pass it to `create_loader`.\"\n        )\n        transform = create_transforms(\"imagenet\", is_training=False)\n\n    # only apply augment splits to train dataset\n    if separate and is_training:\n        assert isinstance(transform, tuple) and len(transform) == 3\n\n        # Note: mindspore-2.0 delete the parameter column_order\n        sig = inspect.signature(dataset.map)\n        pass_column_order = False if \"kwargs\" in sig.parameters else True\n\n        # map all the transform\n        dataset = map_transform_splits(\n            dataset, transform, num_parallel_workers, python_multiprocessing, pass_column_order\n        )\n        # after batch, datasets has 4 columns\n        dataset = dataset.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n        # concat the 3 columns of image\n        dataset = dataset.map(\n            operations=concat_per_batch_map,\n            input_columns=[\"image_clean\", \"image_aug1\", \"image_aug2\", \"label\"],\n            output_columns=[\"image\", \"label\"],\n            column_order=[\"image\", \"label\"] if pass_column_order else None,\n            num_parallel_workers=num_parallel_workers,\n            python_multiprocessing=python_multiprocessing,\n        )\n\n    else:\n        dataset = dataset.map(\n            operations=transform,\n            input_columns=\"image\",\n            num_parallel_workers=num_parallel_workers,\n            python_multiprocessing=python_multiprocessing,\n        )\n\n        dataset = dataset.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n\n    if is_training:\n        if (mixup + cutmix &gt; 0.0) and batch_size &gt; 1:\n            # TODO: use mindspore vision cutmix and mixup after the confliction fixed in later release\n            # set label_smoothing 0 here since label smoothing is computed in loss module\n            mixup_fn = Mixup(\n                mixup_alpha=mixup,\n                cutmix_alpha=cutmix,\n                cutmix_minmax=None,\n                prob=cutmix_prob,\n                switch_prob=0.5,\n                label_smoothing=0.0,\n                num_classes=num_classes,\n            )\n            # images in a batch are mixed. labels are converted soft onehot labels.\n            dataset = dataset.map(\n                operations=mixup_fn,\n                input_columns=[\"image\", target_input_columns],\n                num_parallel_workers=num_parallel_workers,\n            )\n\n    return dataset\n</code></pre>"},{"location":"reference/data/#mixup","title":"MixUp","text":""},{"location":"reference/data/#mindcv.data.mixup.Mixup","title":"<code>mindcv.data.mixup.Mixup</code>","text":"<p>Mixup/Cutmix that applies different params to each element or whole batch</p> PARAMETER DESCRIPTION <code>mixup_alpha</code> <p>mixup alpha value, mixup is active if &gt; 0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>cutmix_alpha</code> <p>cutmix alpha value, cutmix is active if &gt; 0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>cutmix_minmax</code> <p>cutmix min/max image ratio, cutmix is active and uses this vs alpha if not None.</p> <p> TYPE: <code>List[float]</code> DEFAULT: <code>None</code> </p> <code>prob</code> <p>probability of applying mixup or cutmix per batch or element</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>switch_prob</code> <p>probability of switching to cutmix instead of mixup when both are active</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p> <code>mode</code> <p>how to apply mixup/cutmix params (per 'batch', 'pair' (pair of elements), 'elem' (element)</p> <p> TYPE: <code>str</code> DEFAULT: <code>'batch'</code> </p> <code>correct_lam</code> <p>apply lambda correction when cutmix bbox clipped by image borders</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>label_smoothing</code> <p>apply label smoothing to the mixed target tensor</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> <code>num_classes</code> <p>number of classes for target</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> Source code in <code>mindcv/data/mixup.py</code> <pre><code>class Mixup:\n    \"\"\"Mixup/Cutmix that applies different params to each element or whole batch\n\n    Args:\n        mixup_alpha (float): mixup alpha value, mixup is active if &gt; 0.\n        cutmix_alpha (float): cutmix alpha value, cutmix is active if &gt; 0.\n        cutmix_minmax (List[float]): cutmix min/max image ratio, cutmix is active and uses this vs alpha if not None.\n        prob (float): probability of applying mixup or cutmix per batch or element\n        switch_prob (float): probability of switching to cutmix instead of mixup when both are active\n        mode (str): how to apply mixup/cutmix params (per 'batch', 'pair' (pair of elements), 'elem' (element)\n        correct_lam (bool): apply lambda correction when cutmix bbox clipped by image borders\n        label_smoothing (float): apply label smoothing to the mixed target tensor\n        num_classes (int): number of classes for target\n    \"\"\"\n\n    def __init__(\n        self,\n        mixup_alpha=1.0,\n        cutmix_alpha=0.0,\n        cutmix_minmax=None,\n        prob=1.0,\n        switch_prob=0.5,\n        mode=\"batch\",\n        correct_lam=True,\n        label_smoothing=0.1,\n        num_classes=1000,\n    ):\n        self.mixup_alpha = mixup_alpha\n        self.cutmix_alpha = cutmix_alpha\n        self.cutmix_minmax = cutmix_minmax\n        if self.cutmix_minmax is not None:\n            assert len(self.cutmix_minmax) == 2\n            # force cutmix alpha == 1.0 when minmax active to keep logic simple &amp; safe\n            self.cutmix_alpha = 1.0\n        self.mix_prob = prob\n        self.switch_prob = switch_prob\n        self.label_smoothing = label_smoothing\n        self.num_classes = num_classes\n        self.mode = mode\n        self.correct_lam = correct_lam  # correct lambda based on clipped area for cutmix\n        self.mixup_enabled = True  # set false to disable mixing (intended tp be set by train loop)\n\n    def _params_per_elem(self, batch_size):\n        \"\"\"_params_per_elem\"\"\"\n        lam = np.ones(batch_size, dtype=np.float32)\n        use_cutmix = np.zeros(batch_size, dtype=np.bool_)\n        if self.mixup_enabled:\n            if self.mixup_alpha &gt; 0.0 and self.cutmix_alpha &gt; 0.0:\n                use_cutmix = np.random.rand(batch_size) &lt; self.switch_prob\n                lam_mix = np.where(\n                    use_cutmix,\n                    np.random.beta(self.cutmix_alpha, self.cutmix_alpha, size=batch_size),\n                    np.random.beta(self.mixup_alpha, self.mixup_alpha, size=batch_size),\n                )\n            elif self.mixup_alpha &gt; 0.0:\n                lam_mix = np.random.beta(self.mixup_alpha, self.mixup_alpha, size=batch_size)\n            elif self.cutmix_alpha &gt; 0.0:\n                use_cutmix = np.ones(batch_size, dtype=np.bool_)\n                lam_mix = np.random.beta(self.cutmix_alpha, self.cutmix_alpha, size=batch_size)\n            else:\n                assert False, \"One of mixup_alpha &gt; 0., cutmix_alpha &gt; 0., cutmix_minmax not None should be true.\"\n            lam = np.where(np.random.rand(batch_size) &lt; self.mix_prob, lam_mix.astype(np.float32), lam)\n        return lam, use_cutmix\n\n    def _params_per_batch(self):\n        \"\"\"_params_per_batch\"\"\"\n        lam = 1.0\n        use_cutmix = False\n        if self.mixup_enabled and np.random.rand() &lt; self.mix_prob:\n            if self.mixup_alpha &gt; 0.0 and self.cutmix_alpha &gt; 0.0:\n                use_cutmix = np.random.rand() &lt; self.switch_prob\n                lam_mix = (\n                    np.random.beta(self.cutmix_alpha, self.cutmix_alpha)\n                    if use_cutmix\n                    else np.random.beta(self.mixup_alpha, self.mixup_alpha)\n                )\n            elif self.mixup_alpha &gt; 0.0:\n                lam_mix = np.random.beta(self.mixup_alpha, self.mixup_alpha)\n            elif self.cutmix_alpha &gt; 0.0:\n                use_cutmix = True\n                lam_mix = np.random.beta(self.cutmix_alpha, self.cutmix_alpha)\n            else:\n                assert False, \"One of mixup_alpha &gt; 0., cutmix_alpha &gt; 0., cutmix_minmax not None should be true.\"\n            lam = float(lam_mix)\n        return lam, use_cutmix\n\n    def _mix_elem(self, x):\n        \"\"\"_mix_elem\"\"\"\n        batch_size = len(x)\n        lam_batch, use_cutmix = self._params_per_elem(batch_size)\n        x_orig = x.clone()  # need to keep an unmodified original for mixing source\n        for i in range(batch_size):\n            j = batch_size - i - 1\n            lam = lam_batch[i]\n            if lam != 1.0:\n                if use_cutmix[i]:\n                    (yl, yh, xl, xh), lam = cutmix_bbox_and_lam(\n                        x[i].shape, lam, ratio_minmax=self.cutmix_minmax, correct_lam=self.correct_lam\n                    )\n                    x[i][:, yl:yh, xl:xh] = x_orig[j][:, yl:yh, xl:xh]\n                    lam_batch[i] = lam\n                else:\n                    x[i] = x[i] * lam + x_orig[j] * (1 - lam)\n        return P.ExpandDims()(Tensor(lam_batch, dtype=mstype.float32), 1)\n\n    def _mix_pair(self, x):\n        \"\"\"_mix_pair\"\"\"\n        batch_size = len(x)\n        lam_batch, use_cutmix = self._params_per_elem(batch_size // 2)\n        x_orig = x.clone()  # need to keep an unmodified original for mixing source\n        for i in range(batch_size // 2):\n            j = batch_size - i - 1\n            lam = lam_batch[i]\n            if lam != 1.0:\n                if use_cutmix[i]:\n                    (yl, yh, xl, xh), lam = cutmix_bbox_and_lam(\n                        x[i].shape, lam, ratio_minmax=self.cutmix_minmax, correct_lam=self.correct_lam\n                    )\n                    x[i][:, yl:yh, xl:xh] = x_orig[j][:, yl:yh, xl:xh]\n                    x[j][:, yl:yh, xl:xh] = x_orig[i][:, yl:yh, xl:xh]\n                    lam_batch[i] = lam\n                else:\n                    x[i] = x[i] * lam + x_orig[j] * (1 - lam)\n                    x[j] = x[j] * lam + x_orig[i] * (1 - lam)\n        lam_batch = np.concatenate((lam_batch, lam_batch[::-1]))\n        return P.ExpandDims()(Tensor(lam_batch, dtype=mstype.float32), 1)\n\n    def _mix_batch(self, x):\n        \"\"\"_mix_batch\"\"\"\n        lam, use_cutmix = self._params_per_batch()\n        if lam == 1.0:\n            return 1.0\n        if use_cutmix:\n            (yl, yh, xl, xh), lam = cutmix_bbox_and_lam(\n                x.shape, lam, ratio_minmax=self.cutmix_minmax, correct_lam=self.correct_lam\n            )\n            x[:, :, yl:yh, xl:xh] = np.flip(x, axis=0)[:, :, yl:yh, xl:xh]\n        else:\n            x_flipped = np.flip(x, axis=0) * (1.0 - lam)\n            x *= lam\n            x += x_flipped\n        return lam\n\n    def __call__(self, x, target):\n        \"\"\"Mixup apply\"\"\"\n        # the same to image, label\n        assert len(x) % 2 == 0, \"Batch size should be even when using this\"\n        if self.mode == \"elem\":\n            lam = self._mix_elem(x)\n        elif self.mode == \"pair\":\n            lam = self._mix_pair(x)\n        else:\n            lam = self._mix_batch(x)\n        target = mixup_target(target, self.num_classes, lam, self.label_smoothing)\n        return x.astype(np.float32), target.astype(np.float32)\n</code></pre>"},{"location":"reference/data/#transform-factory","title":"Transform Factory","text":""},{"location":"reference/data/#mindcv.data.transforms_factory.create_transforms","title":"<code>mindcv.data.transforms_factory.create_transforms(dataset_name='', image_resize=224, is_training=False, auto_augment=None, separate=False, **kwargs)</code>","text":"<p>Creates a list of transform operation on image data.</p> PARAMETER DESCRIPTION <code>dataset_name</code> <p>if '', customized dataset. Currently, apply the same transform pipeline as ImageNet. if standard dataset name is given including imagenet, cifar10, mnist, preset transforms will be returned. Default: ''.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>image_resize</code> <p>the image size after resize for adapting to network. Default: 224.</p> <p> TYPE: <code>int</code> DEFAULT: <code>224</code> </p> <code>is_training</code> <p>if True, augmentation will be applied if support. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>separate</code> <p>separate the image clean and the image been transformed. If separate==True, the transformers are returned as a tuple of 3 separate transforms for use in a mixing dataset that  passes: * all data through the primary transform, called \"clean\" data * a portion of the data through the secondary transform (e.g., auto-aug) * normalized and converts the branches above with the third, transform</p> <p> DEFAULT: <code>False</code> </p> <code>**kwargs</code> <p>additional args parsed to <code>transforms_imagenet_train</code> and <code>transforms_imagenet_eval</code></p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <p>A list of transformation operations</p> Source code in <code>mindcv/data/transforms_factory.py</code> <pre><code>def create_transforms(\n    dataset_name=\"\",\n    image_resize=224,\n    is_training=False,\n    auto_augment=None,\n    separate=False,\n    **kwargs,\n):\n    r\"\"\"Creates a list of transform operation on image data.\n\n    Args:\n        dataset_name (str): if '', customized dataset. Currently, apply the same transform pipeline as ImageNet.\n            if standard dataset name is given including imagenet, cifar10, mnist, preset transforms will be returned.\n            Default: ''.\n        image_resize (int): the image size after resize for adapting to network. Default: 224.\n        is_training (bool): if True, augmentation will be applied if support. Default: False.\n        auto_augment(str)\uff1aaugmentation strategies, such as \"augmix\", \"autoaug\" etc.\n        separate: separate the image clean and the image been transformed. If separate==True, the transformers are\n            returned as a tuple of 3 separate transforms for use in a mixing dataset that  passes:\n            * all data through the primary transform, called \"clean\" data\n            * a portion of the data through the secondary transform (e.g., auto-aug)\n            * normalized and converts the branches above with the third, transform\n        **kwargs: additional args parsed to `transforms_imagenet_train` and `transforms_imagenet_eval`\n\n    Returns:\n        A list of transformation operations\n    \"\"\"\n\n    dataset_name = dataset_name.lower()\n\n    if dataset_name in (\"imagenet\", \"\"):\n        trans_args = dict(image_resize=image_resize, **kwargs)\n        if is_training:\n            return transforms_imagenet_train(auto_augment=auto_augment, separate=separate, **trans_args)\n\n        return transforms_imagenet_eval(**trans_args)\n    elif dataset_name in (\"cifar10\", \"cifar100\"):\n        trans_list = transforms_cifar(resize=image_resize, is_training=is_training)\n        return trans_list\n    elif dataset_name == \"mnist\":\n        trans_list = transforms_mnist(resize=image_resize)\n        return trans_list\n    else:\n        raise NotImplementedError(\n            f\"Only supports creating transforms for ['imagenet'] datasets, but got {dataset_name}.\"\n        )\n</code></pre>"},{"location":"reference/loss/","title":"Loss","text":""},{"location":"reference/loss/#loss-factory","title":"Loss Factory","text":""},{"location":"reference/loss/#mindcv.loss.loss_factory.create_loss","title":"<code>mindcv.loss.loss_factory.create_loss(name='CE', weight=None, reduction='mean', label_smoothing=0.0, aux_factor=0.0)</code>","text":"<p>Creates loss function</p> PARAMETER DESCRIPTION <code>name</code> <p>loss name : 'CE' for cross_entropy. 'BCE': binary cross entropy. Default: 'CE'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'CE'</code> </p> <code>weight</code> <p>Class weight. A rescaling weight given to the loss of each batch element. If given, has to be a Tensor of size 'nbatch'. Data type must be float16 or float32.</p> <p> TYPE: <code>Tensor</code> DEFAULT: <code>None</code> </p> <code>reduction</code> <p>Apply specific reduction method to the output: 'mean' or 'sum'. By default, the sum of the output will be divided by the number of elements in the output. 'sum': the output will be summed. Default:'mean'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'mean'</code> </p> <code>label_smoothing</code> <p>Label smoothing factor, a regularization tool used to prevent the model from overfitting when calculating Loss. The value range is [0.0, 1.0]. Default: 0.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>aux_factor</code> <p>Auxiliary loss factor. Set aux_factor &gt; 0.0 if the model has auxiliary logit outputs (i.e., deep supervision), like inception_v3. Default: 0.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> Inputs <ul> <li>logits (Tensor or Tuple of Tensor): Input logits. Shape [N, C], where N means the number of samples,     C means number of classes. Tuple of two input logits are supported in order (main_logits, aux_logits)     for auxiliary loss used in networks like inception_v3. Data type must be float16 or float32.</li> <li>labels (Tensor): Ground truth labels. Shape: [N] or [N, C].     (1) If in shape [N], sparse labels representing the class indices. Must be int type.     (2) shape [N, C], dense labels representing the ground truth class probability values,     or the one-hot labels. Must be float type. If the loss type is BCE, the shape of labels must be [N, C].</li> </ul> RETURNS DESCRIPTION <p>Loss function to compute the loss between the input logits and labels.</p> Source code in <code>mindcv/loss/loss_factory.py</code> <pre><code>def create_loss(\n    name: str = \"CE\",\n    weight: Optional[Tensor] = None,\n    reduction: str = \"mean\",\n    label_smoothing: float = 0.0,\n    aux_factor: float = 0.0,\n):\n    r\"\"\"Creates loss function\n\n    Args:\n        name (str):  loss name : 'CE' for cross_entropy. 'BCE': binary cross entropy. Default: 'CE'.\n        weight (Tensor): Class weight. A rescaling weight given to the loss of each batch element.\n            If given, has to be a Tensor of size 'nbatch'. Data type must be float16 or float32.\n        reduction: Apply specific reduction method to the output: 'mean' or 'sum'.\n            By default, the sum of the output will be divided by the number of elements in the output.\n            'sum': the output will be summed. Default:'mean'.\n        label_smoothing: Label smoothing factor, a regularization tool used to prevent the model\n            from overfitting when calculating Loss. The value range is [0.0, 1.0]. Default: 0.0.\n        aux_factor (float): Auxiliary loss factor. Set aux_factor &gt; 0.0 if the model has auxiliary logit outputs\n            (i.e., deep supervision), like inception_v3. Default: 0.0.\n\n    Inputs:\n        - logits (Tensor or Tuple of Tensor): Input logits. Shape [N, C], where N means the number of samples,\n            C means number of classes. Tuple of two input logits are supported in order (main_logits, aux_logits)\n            for auxiliary loss used in networks like inception_v3. Data type must be float16 or float32.\n        - labels (Tensor): Ground truth labels. Shape: [N] or [N, C].\n            (1) If in shape [N], sparse labels representing the class indices. Must be int type.\n            (2) shape [N, C], dense labels representing the ground truth class probability values,\n            or the one-hot labels. Must be float type. If the loss type is BCE, the shape of labels must be [N, C].\n\n    Returns:\n       Loss function to compute the loss between the input logits and labels.\n    \"\"\"\n    name = name.lower()\n\n    if name == \"ce\":\n        loss = CrossEntropySmooth(smoothing=label_smoothing, aux_factor=aux_factor, reduction=reduction, weight=weight)\n    elif name == \"bce\":\n        loss = BinaryCrossEntropySmooth(\n            smoothing=label_smoothing, aux_factor=aux_factor, reduction=reduction, weight=weight, pos_weight=None\n        )\n    elif name == \"asl_single_label\":\n        loss = AsymmetricLossSingleLabel(smoothing=label_smoothing)\n    elif name == \"asl_multi_label\":\n        loss = AsymmetricLossMultilabel()\n    elif name == \"jsd\":\n        loss = JSDCrossEntropy(smoothing=label_smoothing, aux_factor=aux_factor, reduction=reduction, weight=weight)\n    else:\n        raise NotImplementedError\n\n    return loss\n</code></pre>"},{"location":"reference/loss/#cross-entropy","title":"Cross Entropy","text":""},{"location":"reference/loss/#mindcv.loss.cross_entropy_smooth.CrossEntropySmooth","title":"<code>mindcv.loss.cross_entropy_smooth.CrossEntropySmooth</code>","text":"<p>               Bases: <code>LossBase</code></p> <p>Cross entropy loss with label smoothing. Apply softmax activation function to input <code>logits</code>, and uses the given logits to compute cross entropy between the logits and the label.</p> PARAMETER DESCRIPTION <code>smoothing</code> <p>Label smoothing factor, a regularization tool used to prevent the model from overfitting when calculating Loss. The value range is [0.0, 1.0]. Default: 0.0.</p> <p> DEFAULT: <code>0.0</code> </p> <code>aux_factor</code> <p>Auxiliary loss factor. Set aux_factor &gt; 0.0 if the model has auxiliary logit outputs (i.e., deep supervision), like inception_v3.  Default: 0.0.</p> <p> DEFAULT: <code>0.0</code> </p> <code>reduction</code> <p>Apply specific reduction method to the output: 'mean' or 'sum'. Default: 'mean'.</p> <p> DEFAULT: <code>'mean'</code> </p> <code>weight</code> <p>Class weight. Shape [C]. A rescaling weight applied to the loss of each batch element. Data type must be float16 or float32.</p> <p> TYPE: <code>Tensor</code> DEFAULT: <code>None</code> </p> Inputs <p>logits (Tensor or Tuple of Tensor): Input logits. Shape [N, C], where N is # samples, C is # classes.     Tuple composed of multiple logits are supported in order (main_logits, aux_logits)     for auxiliary loss used in networks like inception_v3. labels (Tensor): Ground truth label. Shape: [N] or [N, C].     (1) Shape (N), sparse labels representing the class indices. Must be int type.     (2) Shape [N, C], dense labels representing the ground truth class probability values,     or the one-hot labels. Must be float type.</p> Source code in <code>mindcv/loss/cross_entropy_smooth.py</code> <pre><code>class CrossEntropySmooth(nn.LossBase):\n    \"\"\"\n    Cross entropy loss with label smoothing.\n    Apply softmax activation function to input `logits`, and uses the given logits to compute cross entropy\n    between the logits and the label.\n\n    Args:\n        smoothing: Label smoothing factor, a regularization tool used to prevent the model\n            from overfitting when calculating Loss. The value range is [0.0, 1.0]. Default: 0.0.\n        aux_factor: Auxiliary loss factor. Set aux_factor &gt; 0.0 if the model has auxiliary logit outputs\n            (i.e., deep supervision), like inception_v3.  Default: 0.0.\n        reduction: Apply specific reduction method to the output: 'mean' or 'sum'. Default: 'mean'.\n        weight (Tensor): Class weight. Shape [C]. A rescaling weight applied to the loss of each batch element.\n            Data type must be float16 or float32.\n\n    Inputs:\n        logits (Tensor or Tuple of Tensor): Input logits. Shape [N, C], where N is # samples, C is # classes.\n            Tuple composed of multiple logits are supported in order (main_logits, aux_logits)\n            for auxiliary loss used in networks like inception_v3.\n        labels (Tensor): Ground truth label. Shape: [N] or [N, C].\n            (1) Shape (N), sparse labels representing the class indices. Must be int type.\n            (2) Shape [N, C], dense labels representing the ground truth class probability values,\n            or the one-hot labels. Must be float type.\n    \"\"\"\n\n    def __init__(self, smoothing=0.0, aux_factor=0.0, reduction=\"mean\", weight=None):\n        super().__init__()\n        self.smoothing = smoothing\n        self.aux_factor = aux_factor\n        self.reduction = reduction\n        self.weight = weight\n\n    def construct(self, logits, labels):\n        loss_aux = 0\n\n        if isinstance(logits, tuple):\n            main_logits = logits[0]\n            for aux in logits[1:]:\n                if self.aux_factor &gt; 0:\n                    loss_aux += F.cross_entropy(\n                        aux, labels, weight=self.weight, reduction=self.reduction, label_smoothing=self.smoothing\n                    )\n        else:\n            main_logits = logits\n\n        loss_logits = F.cross_entropy(\n            main_logits, labels, weight=self.weight, reduction=self.reduction, label_smoothing=self.smoothing\n        )\n        loss = loss_logits + self.aux_factor * loss_aux\n        return loss\n</code></pre>"},{"location":"reference/loss/#binary-cross-entropy","title":"Binary Cross Entropy","text":""},{"location":"reference/loss/#mindcv.loss.binary_cross_entropy_smooth.BinaryCrossEntropySmooth","title":"<code>mindcv.loss.binary_cross_entropy_smooth.BinaryCrossEntropySmooth</code>","text":"<p>               Bases: <code>LossBase</code></p> <p>Binary cross entropy loss with label smoothing. Apply sigmoid activation function to input <code>logits</code>, and uses the given logits to compute binary cross entropy between the logits and the label.</p> PARAMETER DESCRIPTION <code>smoothing</code> <p>Label smoothing factor, a regularization tool used to prevent the model from overfitting when calculating Loss. The value range is [0.0, 1.0]. Default: 0.0.</p> <p> DEFAULT: <code>0.0</code> </p> <code>aux_factor</code> <p>Auxiliary loss factor. Set aux_factor &gt; 0.0 if the model has auxiliary logit outputs (i.e., deep supervision), like inception_v3.  Default: 0.0.</p> <p> DEFAULT: <code>0.0</code> </p> <code>reduction</code> <p>Apply specific reduction method to the output: 'mean' or 'sum'. Default: 'mean'.</p> <p> DEFAULT: <code>'mean'</code> </p> <code>weight</code> <p>Class weight. A rescaling weight applied to the loss of each batch element. Shape [C]. It can be broadcast to a tensor with shape of <code>logits</code>. Data type must be float16 or float32.</p> <p> TYPE: <code>Tensor</code> DEFAULT: <code>None</code> </p> <code>pos_weight</code> <p>Positive weight for each class. A weight of positive examples. Shape [C]. Must be a vector with length equal to the number of classes. It can be broadcast to a tensor with shape of <code>logits</code>. Data type must be float16 or float32.</p> <p> TYPE: <code>Tensor</code> DEFAULT: <code>None</code> </p> Inputs <p>logits (Tensor or Tuple of Tensor): (1) Input logits. Shape [N, C], where N is # samples, C is # classes.     Or (2) Tuple of two input logits (main_logits and aux_logits) for auxiliary loss. labels (Tensor): Ground truth label, (1) shape [N, C], has the same shape as <code>logits</code> or (2) shape [N].     can be a class probability matrix or one-hot labels. Data type must be float16 or float32.</p> Source code in <code>mindcv/loss/binary_cross_entropy_smooth.py</code> <pre><code>class BinaryCrossEntropySmooth(nn.LossBase):\n    \"\"\"\n    Binary cross entropy loss with label smoothing.\n    Apply sigmoid activation function to input `logits`, and uses the given logits to compute binary cross entropy\n    between the logits and the label.\n\n    Args:\n        smoothing: Label smoothing factor, a regularization tool used to prevent the model\n            from overfitting when calculating Loss. The value range is [0.0, 1.0]. Default: 0.0.\n        aux_factor: Auxiliary loss factor. Set aux_factor &gt; 0.0 if the model has auxiliary logit outputs\n            (i.e., deep supervision), like inception_v3.  Default: 0.0.\n        reduction: Apply specific reduction method to the output: 'mean' or 'sum'. Default: 'mean'.\n        weight (Tensor): Class weight. A rescaling weight applied to the loss of each batch element. Shape [C].\n            It can be broadcast to a tensor with shape of `logits`. Data type must be float16 or float32.\n        pos_weight (Tensor): Positive weight for each class. A weight of positive examples. Shape [C].\n            Must be a vector with length equal to the number of classes.\n            It can be broadcast to a tensor with shape of `logits`. Data type must be float16 or float32.\n\n    Inputs:\n        logits (Tensor or Tuple of Tensor): (1) Input logits. Shape [N, C], where N is # samples, C is # classes.\n            Or (2) Tuple of two input logits (main_logits and aux_logits) for auxiliary loss.\n        labels (Tensor): Ground truth label, (1) shape [N, C], has the same shape as `logits` or (2) shape [N].\n            can be a class probability matrix or one-hot labels. Data type must be float16 or float32.\n    \"\"\"\n\n    def __init__(self, smoothing=0.0, aux_factor=0.0, reduction=\"mean\", weight=None, pos_weight=None):\n        super().__init__()\n        self.smoothing = smoothing\n        self.aux_factor = aux_factor\n        self.reduction = reduction\n        self.weight = weight\n        self.pos_weight = pos_weight\n        self.ones = P.OnesLike()\n        self.one_hot = P.OneHot()\n\n    def construct(self, logits, labels):\n        loss_aux = 0\n        aux_logits = None\n\n        if isinstance(logits, tuple):\n            main_logits = logits[0]\n        else:\n            main_logits = logits\n\n        if main_logits.size != labels.size:\n            # We must explicitly convert the label to one-hot,\n            # for binary_cross_entropy_with_logits restricting input and label have the same shape.\n            class_dim = 0 if main_logits.ndim == 1 else 1\n            n_classes = main_logits.shape[class_dim]\n            labels = self.one_hot(labels, n_classes, Tensor(1.0), Tensor(0.0))\n\n        ones_input = self.ones(main_logits)\n        if self.weight is not None:\n            weight = self.weight\n        else:\n            weight = ones_input\n        if self.pos_weight is not None:\n            pos_weight = self.pos_weight\n        else:\n            pos_weight = ones_input\n\n        if self.smoothing &gt; 0.0:\n            class_dim = 0 if main_logits.ndim == 1 else -1\n            n_classes = main_logits.shape[class_dim]\n            labels = labels * (1 - self.smoothing) + self.smoothing / n_classes\n\n        if self.aux_factor &gt; 0 and aux_logits is not None:\n            for aux_logits in logits[1:]:\n                loss_aux += F.binary_cross_entropy_with_logits(\n                    aux_logits, labels, weight=weight, pos_weight=pos_weight, reduction=self.reduction\n                )\n        # else:\n        #    warnings.warn(\"There are logit tuple input, but the auxiliary loss factor is 0.\")\n\n        loss_logits = F.binary_cross_entropy_with_logits(\n            main_logits, labels, weight=weight, pos_weight=pos_weight, reduction=self.reduction\n        )\n\n        loss = loss_logits + self.aux_factor * loss_aux\n\n        return loss\n</code></pre>"},{"location":"reference/models.layers/","title":"Common Layers in Model","text":""},{"location":"reference/models.layers/#activation","title":"Activation","text":""},{"location":"reference/models.layers/#mindcv.models.layers.activation.Swish","title":"<code>mindcv.models.layers.activation.Swish</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Swish activation function: x * sigmoid(x).</p> Return <p>Tensor</p> Example <p>x = Tensor(((20, 16), (50, 50)), mindspore.float32) Swish()(x)</p> Source code in <code>mindcv/models/layers/activation.py</code> <pre><code>class Swish(nn.Cell):\n    \"\"\"\n    Swish activation function: x * sigmoid(x).\n\n    Args:\n        None\n\n    Return:\n        Tensor\n\n    Example:\n        &gt;&gt;&gt; x = Tensor(((20, 16), (50, 50)), mindspore.float32)\n        &gt;&gt;&gt; Swish()(x)\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.result = None\n        self.sigmoid = nn.Sigmoid()\n\n    def construct(self, x):\n        result = x * self.sigmoid(x)\n        return result\n</code></pre>"},{"location":"reference/models.layers/#droppath","title":"DropPath","text":""},{"location":"reference/models.layers/#mindcv.models.layers.drop_path.DropPath","title":"<code>mindcv.models.layers.drop_path.DropPath</code>","text":"<p>               Bases: <code>Cell</code></p> <p>DropPath (Stochastic Depth) regularization layers</p> Source code in <code>mindcv/models/layers/drop_path.py</code> <pre><code>class DropPath(nn.Cell):\n    \"\"\"DropPath (Stochastic Depth) regularization layers\"\"\"\n\n    def __init__(\n        self,\n        drop_prob: float = 0.0,\n        scale_by_keep: bool = True,\n    ) -&gt; None:\n        super().__init__()\n        self.keep_prob = 1.0 - drop_prob\n        self.scale_by_keep = scale_by_keep\n        self.dropout = Dropout(p=drop_prob)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        if self.keep_prob == 1.0 or not self.training:\n            return x\n        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n        random_tensor = self.dropout(ones(shape))\n        if not self.scale_by_keep:\n            random_tensor = ops.mul(random_tensor, self.keep_prob)\n        return x * random_tensor\n</code></pre>"},{"location":"reference/models.layers/#identity","title":"Identity","text":""},{"location":"reference/models.layers/#mindcv.models.layers.identity.Identity","title":"<code>mindcv.models.layers.identity.Identity</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Identity</p> Source code in <code>mindcv/models/layers/identity.py</code> <pre><code>class Identity(nn.Cell):\n    \"\"\"Identity\"\"\"\n\n    def construct(self, x):\n        return x\n</code></pre>"},{"location":"reference/models.layers/#mlp","title":"MLP","text":""},{"location":"reference/models.layers/#mindcv.models.layers.mlp.Mlp","title":"<code>mindcv.models.layers.mlp.Mlp</code>","text":"<p>               Bases: <code>Cell</code></p> Source code in <code>mindcv/models/layers/mlp.py</code> <pre><code>class Mlp(nn.Cell):\n    def __init__(\n        self,\n        in_features: int,\n        hidden_features: Optional[int] = None,\n        out_features: Optional[int] = None,\n        act_layer: Optional[nn.Cell] = nn.GELU,\n        drop: float = 0.0,\n    ) -&gt; None:\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Dense(in_channels=in_features, out_channels=hidden_features, has_bias=True)\n        self.act = act_layer()\n        self.fc2 = nn.Dense(in_channels=hidden_features, out_channels=out_features, has_bias=True)\n        self.drop = Dropout(p=drop)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n</code></pre>"},{"location":"reference/models.layers/#patch-embedding","title":"Patch Embedding","text":""},{"location":"reference/models.layers/#mindcv.models.layers.patch_embed.PatchEmbed","title":"<code>mindcv.models.layers.patch_embed.PatchEmbed</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Image to Patch Embedding</p> PARAMETER DESCRIPTION <code>image_size</code> <p>Image size.  Default: 224.</p> <p> TYPE: <code>int</code> DEFAULT: <code>224</code> </p> <code>patch_size</code> <p>Patch token size. Default: 4.</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> <code>in_chans</code> <p>Number of input image channels. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>embed_dim</code> <p>Number of linear projection output channels. Default: 96.</p> <p> TYPE: <code>int</code> DEFAULT: <code>96</code> </p> <code>norm_layer</code> <p>Normalization layer. Default: None</p> <p> TYPE: <code>Cell</code> DEFAULT: <code>None</code> </p> Source code in <code>mindcv/models/layers/patch_embed.py</code> <pre><code>class PatchEmbed(nn.Cell):\n    \"\"\"Image to Patch Embedding\n\n    Args:\n        image_size (int): Image size.  Default: 224.\n        patch_size (int): Patch token size. Default: 4.\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        norm_layer (nn.Cell, optional): Normalization layer. Default: None\n    \"\"\"\n    output_fmt: Format\n\n    def __init__(\n        self,\n        image_size: Optional[int] = 224,\n        patch_size: int = 4,\n        in_chans: int = 3,\n        embed_dim: int = 96,\n        norm_layer: Optional[nn.Cell] = None,\n        flatten: bool = True,\n        output_fmt: Optional[str] = None,\n        bias: bool = True,\n        strict_img_size: bool = True,\n        dynamic_img_pad: bool = False,\n    ) -&gt; None:\n        super().__init__()\n        self.patch_size = to_2tuple(patch_size)\n        if image_size is not None:\n            self.image_size = to_2tuple(image_size)\n            self.patches_resolution = tuple([s // p for s, p in zip(self.image_size, self.patch_size)])\n            self.num_patches = self.patches_resolution[0] * self.patches_resolution[1]\n        else:\n            self.image_size = None\n            self.patches_resolution = None\n            self.num_patches = None\n\n        if output_fmt is not None:\n            self.flatten = False\n            self.output_fmt = Format(output_fmt)\n        else:\n            self.flatten = flatten\n            self.output_fmt = Format.NCHW\n\n        self.strict_img_size = strict_img_size\n        self.dynamic_img_pad = dynamic_img_pad\n        self.embed_dim = embed_dim\n\n        self.proj = nn.Conv2d(in_channels=in_chans, out_channels=embed_dim, kernel_size=patch_size, stride=patch_size,\n                              pad_mode='pad', has_bias=bias, weight_init=\"TruncatedNormal\")\n\n        if norm_layer is not None:\n            if isinstance(embed_dim, int):\n                embed_dim = (embed_dim,)\n            self.norm = norm_layer(embed_dim, epsilon=1e-5)\n        else:\n            self.norm = None\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        \"\"\"docstring\"\"\"\n        B, C, H, W = x.shape\n        if self.image_size is not None:\n            if self.strict_img_size:\n                if (H, W) != (self.image_size[0], self.image_size[1]):\n                    raise ValueError(f\"Input height and width ({H},{W}) doesn't match model ({self.image_size[0]},\"\n                                     f\"{self.image_size[1]}).\")\n            elif not self.dynamic_img_pad:\n                if H % self.patch_size[0] != 0:\n                    raise ValueError(f\"Input height ({H}) should be divisible by patch size ({self.patch_size[0]}).\")\n                if W % self.patch_size[1] != 0:\n                    raise ValueError(f\"Input width ({W}) should be divisible by patch size ({self.patch_size[1]}).\")\n        if self.dynamic_img_pad:\n            pad_h = (self.patch_size[0] - H % self.patch_size[0]) % self.patch_size[0]\n            pad_w = (self.patch_size[1] - W % self.patch_size[1]) % self.patch_size[1]\n            x = ops.pad(x, (0, pad_w, 0, pad_h))\n\n        # FIXME look at relaxing size constraints\n        x = self.proj(x)\n        if self.flatten:\n            x = ops.Reshape()(x, (B, self.embed_dim, -1))  # B Ph*Pw C\n            x = ops.Transpose()(x, (0, 2, 1))\n        elif self.output_fmt != \"NCHW\":\n            x = nchw_to(x, self.output_fmt)\n        if self.norm is not None:\n            x = self.norm(x)\n        return x\n</code></pre>"},{"location":"reference/models.layers/#mindcv.models.layers.patch_embed.PatchEmbed.construct","title":"<code>mindcv.models.layers.patch_embed.PatchEmbed.construct(x)</code>","text":"<p>docstring</p> Source code in <code>mindcv/models/layers/patch_embed.py</code> <pre><code>def construct(self, x: Tensor) -&gt; Tensor:\n    \"\"\"docstring\"\"\"\n    B, C, H, W = x.shape\n    if self.image_size is not None:\n        if self.strict_img_size:\n            if (H, W) != (self.image_size[0], self.image_size[1]):\n                raise ValueError(f\"Input height and width ({H},{W}) doesn't match model ({self.image_size[0]},\"\n                                 f\"{self.image_size[1]}).\")\n        elif not self.dynamic_img_pad:\n            if H % self.patch_size[0] != 0:\n                raise ValueError(f\"Input height ({H}) should be divisible by patch size ({self.patch_size[0]}).\")\n            if W % self.patch_size[1] != 0:\n                raise ValueError(f\"Input width ({W}) should be divisible by patch size ({self.patch_size[1]}).\")\n    if self.dynamic_img_pad:\n        pad_h = (self.patch_size[0] - H % self.patch_size[0]) % self.patch_size[0]\n        pad_w = (self.patch_size[1] - W % self.patch_size[1]) % self.patch_size[1]\n        x = ops.pad(x, (0, pad_w, 0, pad_h))\n\n    # FIXME look at relaxing size constraints\n    x = self.proj(x)\n    if self.flatten:\n        x = ops.Reshape()(x, (B, self.embed_dim, -1))  # B Ph*Pw C\n        x = ops.Transpose()(x, (0, 2, 1))\n    elif self.output_fmt != \"NCHW\":\n        x = nchw_to(x, self.output_fmt)\n    if self.norm is not None:\n        x = self.norm(x)\n    return x\n</code></pre>"},{"location":"reference/models.layers/#pooling","title":"Pooling","text":""},{"location":"reference/models.layers/#mindcv.models.layers.pooling.GlobalAvgPooling","title":"<code>mindcv.models.layers.pooling.GlobalAvgPooling</code>","text":"<p>               Bases: <code>Cell</code></p> <p>GlobalAvgPooling, same as torch.nn.AdaptiveAvgPool2d when output shape is 1</p> Source code in <code>mindcv/models/layers/pooling.py</code> <pre><code>class GlobalAvgPooling(nn.Cell):\n    \"\"\"\n    GlobalAvgPooling, same as torch.nn.AdaptiveAvgPool2d when output shape is 1\n    \"\"\"\n\n    def __init__(self, keep_dims: bool = False) -&gt; None:\n        super().__init__()\n        self.keep_dims = keep_dims\n\n    def construct(self, x):\n        x = ops.mean(x, axis=(2, 3), keep_dims=self.keep_dims)\n        return x\n</code></pre>"},{"location":"reference/models.layers/#selective-kernel","title":"Selective Kernel","text":""},{"location":"reference/models.layers/#mindcv.models.layers.selective_kernel.SelectiveKernelAttn","title":"<code>mindcv.models.layers.selective_kernel.SelectiveKernelAttn</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Selective Kernel Attention Module Selective Kernel attention mechanism factored out into its own module.</p> Source code in <code>mindcv/models/layers/selective_kernel.py</code> <pre><code>class SelectiveKernelAttn(nn.Cell):\n    \"\"\"Selective Kernel Attention Module\n    Selective Kernel attention mechanism factored out into its own module.\n    \"\"\"\n\n    def __init__(\n        self,\n        channels: int,\n        num_paths: int = 2,\n        attn_channels: int = 32,\n        activation: Optional[nn.Cell] = nn.ReLU,\n        norm: Optional[nn.Cell] = nn.BatchNorm2d,\n    ):\n        super().__init__()\n        self.num_paths = num_paths\n        self.mean = GlobalAvgPooling(keep_dims=True)\n        self.fc_reduce = nn.Conv2d(channels, attn_channels, kernel_size=1, has_bias=False)\n        self.bn = norm(attn_channels)\n        self.act = activation()\n        self.fc_select = nn.Conv2d(attn_channels, channels * num_paths, kernel_size=1)\n        self.softmax = nn.Softmax(axis=1)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.mean((x.sum(1)))\n        x = self.fc_reduce(x)\n        x = self.bn(x)\n        x = self.act(x)\n        x = self.fc_select(x)\n        b, c, h, w = x.shape\n        x = x.reshape((b, self.num_paths, c // self.num_paths, h, w))\n        x = self.softmax(x)\n        return x\n</code></pre>"},{"location":"reference/models.layers/#mindcv.models.layers.selective_kernel.SelectiveKernel","title":"<code>mindcv.models.layers.selective_kernel.SelectiveKernel</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Selective Kernel Convolution Module As described in Selective Kernel Networks (https://arxiv.org/abs/1903.06586) with some modifications. Largest change is the input split, which divides the input channels across each convolution path, this can be viewed as a grouping of sorts, but the output channel counts expand to the module level value. This keeps the parameter count from ballooning when the convolutions themselves don't have groups, but still provides a noteworthy increase in performance over similar param count models without this attention layer. -Ross W Args:     in_channels (int):  module input (feature) channel count     out_channels (int):  module output (feature) channel count     kernel_size (int, list): kernel size for each convolution branch     stride (int): stride for convolutions     dilation (int): dilation for module as a whole, impacts dilation of each branch     groups (int): number of groups for each branch     rd_ratio (int, float): reduction factor for attention features     rd_channels(int): reduction channels can be specified directly by arg (if rd_channels is set)     rd_divisor(int): divisor can be specified to keep channels     keep_3x3 (bool): keep all branch convolution kernels as 3x3, changing larger kernels for dilations     split_input (bool): split input channels evenly across each convolution branch, keeps param count lower,         can be viewed as grouping by path, output expands to module out_channels count     activation (nn.Module): activation layer to use     norm (nn.Module): batchnorm/norm layer to use</p> Source code in <code>mindcv/models/layers/selective_kernel.py</code> <pre><code>class SelectiveKernel(nn.Cell):\n    \"\"\"Selective Kernel Convolution Module\n    As described in Selective Kernel Networks (https://arxiv.org/abs/1903.06586) with some modifications.\n    Largest change is the input split, which divides the input channels across each convolution path, this can\n    be viewed as a grouping of sorts, but the output channel counts expand to the module level value. This keeps\n    the parameter count from ballooning when the convolutions themselves don't have groups, but still provides\n    a noteworthy increase in performance over similar param count models without this attention layer. -Ross W\n    Args:\n        in_channels (int):  module input (feature) channel count\n        out_channels (int):  module output (feature) channel count\n        kernel_size (int, list): kernel size for each convolution branch\n        stride (int): stride for convolutions\n        dilation (int): dilation for module as a whole, impacts dilation of each branch\n        groups (int): number of groups for each branch\n        rd_ratio (int, float): reduction factor for attention features\n        rd_channels(int): reduction channels can be specified directly by arg (if rd_channels is set)\n        rd_divisor(int): divisor can be specified to keep channels\n        keep_3x3 (bool): keep all branch convolution kernels as 3x3, changing larger kernels for dilations\n        split_input (bool): split input channels evenly across each convolution branch, keeps param count lower,\n            can be viewed as grouping by path, output expands to module out_channels count\n        activation (nn.Module): activation layer to use\n        norm (nn.Module): batchnorm/norm layer to use\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: Optional[int] = None,\n        kernel_size: Optional[Union[int, List]] = None,\n        stride: int = 1,\n        dilation: int = 1,\n        groups: int = 1,\n        rd_ratio: float = 1.0 / 16,\n        rd_channels: Optional[int] = None,\n        rd_divisor: int = 8,\n        keep_3x3: bool = True,\n        split_input: bool = True,\n        activation: Optional[nn.Cell] = nn.ReLU,\n        norm: Optional[nn.Cell] = nn.BatchNorm2d,\n    ):\n        super().__init__()\n        out_channels = out_channels or in_channels\n        kernel_size = kernel_size or [3, 5]  # default to one 3x3 and one 5x5 branch. 5x5 -&gt; 3x3 + dilation\n        _kernel_valid(kernel_size)\n        if not isinstance(kernel_size, list):\n            kernel_size = [kernel_size] * 2\n        if keep_3x3:\n            dilation = [dilation * (k - 1) // 2 for k in kernel_size]\n            kernel_size = [3] * len(kernel_size)\n        else:\n            dilation = [dilation] * len(kernel_size)\n        self.num_paths = len(kernel_size)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.split_input = split_input\n        if self.split_input:\n            assert in_channels % self.num_paths == 0\n            in_channels = in_channels // self.num_paths\n        groups = min(out_channels, groups)\n        self.split = Split(split_size_or_sections=self.in_channels // self.num_paths, output_num=self.num_paths, axis=1)\n\n        self.paths = nn.CellList([\n            Conv2dNormActivation(in_channels, out_channels, kernel_size=k, stride=stride, groups=groups,\n                                 dilation=d, activation=activation, norm=norm)\n            for k, d in zip(kernel_size, dilation)\n        ])\n\n        attn_channels = rd_channels or make_divisible(out_channels * rd_ratio, divisor=rd_divisor)\n        self.attn = SelectiveKernelAttn(out_channels, self.num_paths, attn_channels)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x_paths = []\n        if self.split_input:\n            x_split = self.split(x)\n            for i, op in enumerate(self.paths):\n                x_paths.append(op(x_split[i]))\n        else:\n            for op in self.paths:\n                x_paths.append(op(x))\n\n        x = ops.stack(x_paths, axis=1)\n        x_attn = self.attn(x)\n        x = x * x_attn\n        x = x.sum(1)\n        return x\n</code></pre>"},{"location":"reference/models.layers/#squeeze-and-excite","title":"Squeeze and Excite","text":""},{"location":"reference/models.layers/#mindcv.models.layers.squeeze_excite.SqueezeExcite","title":"<code>mindcv.models.layers.squeeze_excite.SqueezeExcite</code>","text":"<p>               Bases: <code>Cell</code></p> <p>SqueezeExcite Module as defined in original SE-Nets with a few additions. Additions include:     * divisor can be specified to keep channels % div == 0 (default: 8)     * reduction channels can be specified directly by arg (if rd_channels is set)     * reduction channels can be specified by float rd_ratio (default: 1/16)     * customizable activation, normalization, and gate layer</p> Source code in <code>mindcv/models/layers/squeeze_excite.py</code> <pre><code>class SqueezeExcite(nn.Cell):\n    \"\"\"SqueezeExcite Module as defined in original SE-Nets with a few additions.\n    Additions include:\n        * divisor can be specified to keep channels % div == 0 (default: 8)\n        * reduction channels can be specified directly by arg (if rd_channels is set)\n        * reduction channels can be specified by float rd_ratio (default: 1/16)\n        * customizable activation, normalization, and gate layer\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        rd_ratio: float = 1.0 / 16,\n        rd_channels: Optional[int] = None,\n        rd_divisor: int = 8,\n        norm: Optional[nn.Cell] = None,\n        act_layer: nn.Cell = nn.ReLU,\n        gate_layer: nn.Cell = nn.Sigmoid,\n    ) -&gt; None:\n        super().__init__()\n        self.norm = norm\n        self.act = act_layer()\n        self.gate = gate_layer()\n        if not rd_channels:\n            rd_channels = make_divisible(in_channels * rd_ratio, rd_divisor)\n\n        self.conv_reduce = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=rd_channels,\n            kernel_size=1,\n            has_bias=True,\n        )\n        if self.norm:\n            self.bn = nn.BatchNorm2d(rd_channels)\n        self.conv_expand = nn.Conv2d(\n            in_channels=rd_channels,\n            out_channels=in_channels,\n            kernel_size=1,\n            has_bias=True,\n        )\n        self.pool = GlobalAvgPooling(keep_dims=True)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x_se = self.pool(x)\n        x_se = self.conv_reduce(x_se)\n        if self.norm:\n            x_se = self.bn(x_se)\n        x_se = self.act(x_se)\n        x_se = self.conv_expand(x_se)\n        x_se = self.gate(x_se)\n        x = x * x_se\n        return x\n</code></pre>"},{"location":"reference/models.layers/#mindcv.models.layers.squeeze_excite.SqueezeExciteV2","title":"<code>mindcv.models.layers.squeeze_excite.SqueezeExciteV2</code>","text":"<p>               Bases: <code>Cell</code></p> <p>SqueezeExcite Module as defined in original SE-Nets with a few additions. V1 uses 1x1conv to replace fc layers, and V2 uses nn.Dense to implement directly.</p> Source code in <code>mindcv/models/layers/squeeze_excite.py</code> <pre><code>class SqueezeExciteV2(nn.Cell):\n    \"\"\"SqueezeExcite Module as defined in original SE-Nets with a few additions.\n    V1 uses 1x1conv to replace fc layers, and V2 uses nn.Dense to implement directly.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        rd_ratio: float = 1.0 / 16,\n        rd_channels: Optional[int] = None,\n        rd_divisor: int = 8,\n        norm: Optional[nn.Cell] = None,\n        act_layer: nn.Cell = nn.ReLU,\n        gate_layer: nn.Cell = nn.Sigmoid,\n    ) -&gt; None:\n        super().__init__()\n        self.norm = norm\n        self.act = act_layer()\n        self.gate = gate_layer()\n        if not rd_channels:\n            rd_channels = make_divisible(in_channels * rd_ratio, rd_divisor)\n\n        self.conv_reduce = nn.Dense(\n            in_channels=in_channels,\n            out_channels=rd_channels,\n            has_bias=True,\n        )\n        if self.norm:\n            self.bn = nn.BatchNorm2d(rd_channels)\n        self.conv_expand = nn.Dense(\n            in_channels=rd_channels,\n            out_channels=in_channels,\n            has_bias=True,\n        )\n        self.pool = GlobalAvgPooling(keep_dims=False)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x_se = self.pool(x)\n        x_se = self.conv_reduce(x_se)\n        if self.norm:\n            x_se = self.bn(x_se)\n        x_se = self.act(x_se)\n        x_se = self.conv_expand(x_se)\n        x_se = self.gate(x_se)\n        x_se = ops.expand_dims(x_se, -1)\n        x_se = ops.expand_dims(x_se, -1)\n        x = x * x_se\n        return x\n</code></pre>"},{"location":"reference/models/","title":"Models","text":""},{"location":"reference/models/#create-model","title":"Create Model","text":""},{"location":"reference/models/#mindcv.models.model_factory.create_model","title":"<code>mindcv.models.model_factory.create_model(model_name, num_classes=1000, pretrained=False, in_channels=3, checkpoint_path='', ema=False, auto_mapping=False, **kwargs)</code>","text":"<p>Creates model by name.</p> PARAMETER DESCRIPTION <code>model_name</code> <p>The name of model.</p> <p> TYPE: <code>str</code> </p> <code>num_classes</code> <p>The number of classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>pretrained</code> <p>Whether to load the pretrained model. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>in_channels</code> <p>The input channels. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>checkpoint_path</code> <p>The path of checkpoint files. Default: \"\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>ema</code> <p>Whether use ema method. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>auto_mapping</code> <p>Whether to automatically map the names of checkpoint weights to the names of model weights when there are differences in names. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>**kwargs</code> <p>additional args, e.g., \"features_only\", \"out_indices\".</p> <p> DEFAULT: <code>{}</code> </p> Source code in <code>mindcv/models/model_factory.py</code> <pre><code>def create_model(\n    model_name: str,\n    num_classes: int = 1000,\n    pretrained: bool = False,\n    in_channels: int = 3,\n    checkpoint_path: str = \"\",\n    ema: bool = False,\n    auto_mapping: bool = False,\n    **kwargs,\n):\n    r\"\"\"Creates model by name.\n\n    Args:\n        model_name (str):  The name of model.\n        num_classes (int): The number of classes. Default: 1000.\n        pretrained (bool): Whether to load the pretrained model. Default: False.\n        in_channels (int): The input channels. Default: 3.\n        checkpoint_path (str): The path of checkpoint files. Default: \"\".\n        ema (bool): Whether use ema method. Default: False.\n        auto_mapping (bool): Whether to automatically map the names of checkpoint weights\n            to the names of model weights when there are differences in names. Default: False.\n        **kwargs: additional args, e.g., \"features_only\", \"out_indices\".\n    \"\"\"\n\n    if checkpoint_path != \"\" and pretrained:\n        raise ValueError(\"checkpoint_path is mutually exclusive with pretrained\")\n\n    model_args = dict(num_classes=num_classes, pretrained=pretrained, in_channels=in_channels)\n    kwargs = {k: v for k, v in kwargs.items() if v is not None}\n\n    if not is_model(model_name):\n        raise RuntimeError(f\"Unknown model {model_name}\")\n\n    create_fn = model_entrypoint(model_name)\n    model = create_fn(**model_args, **kwargs)\n\n    if checkpoint_path:\n        load_model_checkpoint(model, checkpoint_path, ema, auto_mapping)\n\n    return model\n</code></pre>"},{"location":"reference/models/#bit","title":"bit","text":""},{"location":"reference/models/#cait","title":"cait","text":""},{"location":"reference/models/#cmt","title":"cmt","text":""},{"location":"reference/models/#coat","title":"coat","text":""},{"location":"reference/models/#convit","title":"convit","text":""},{"location":"reference/models/#convnext","title":"convnext","text":""},{"location":"reference/models/#crossvit","title":"crossvit","text":""},{"location":"reference/models/#densenet","title":"densenet","text":""},{"location":"reference/models/#dpn","title":"dpn","text":""},{"location":"reference/models/#edgenext","title":"edgenext","text":""},{"location":"reference/models/#efficientnet","title":"efficientnet","text":""},{"location":"reference/models/#features","title":"features","text":""},{"location":"reference/models/#ghostnet","title":"ghostnet","text":""},{"location":"reference/models/#halonet","title":"halonet","text":""},{"location":"reference/models/#hrnet","title":"hrnet","text":""},{"location":"reference/models/#inceptionv3","title":"inceptionv3","text":""},{"location":"reference/models/#inceptionv4","title":"inceptionv4","text":""},{"location":"reference/models/#mae","title":"mae","text":""},{"location":"reference/models/#mixnet","title":"mixnet","text":""},{"location":"reference/models/#mlpmixer","title":"mlpmixer","text":""},{"location":"reference/models/#mnasnet","title":"mnasnet","text":""},{"location":"reference/models/#mobilenetv1","title":"mobilenetv1","text":""},{"location":"reference/models/#mobilenetv2","title":"mobilenetv2","text":""},{"location":"reference/models/#mobilenetv3","title":"mobilenetv3","text":""},{"location":"reference/models/#mobilevit","title":"mobilevit","text":""},{"location":"reference/models/#nasnet","title":"nasnet","text":""},{"location":"reference/models/#pit","title":"pit","text":""},{"location":"reference/models/#poolformer","title":"poolformer","text":""},{"location":"reference/models/#pvt","title":"pvt","text":""},{"location":"reference/models/#pvtv2","title":"pvtv2","text":""},{"location":"reference/models/#regnet","title":"regnet","text":""},{"location":"reference/models/#repmlp","title":"repmlp","text":""},{"location":"reference/models/#repvgg","title":"repvgg","text":""},{"location":"reference/models/#res2net","title":"res2net","text":""},{"location":"reference/models/#resnest","title":"resnest","text":""},{"location":"reference/models/#resnet","title":"resnet","text":""},{"location":"reference/models/#resnetv2","title":"resnetv2","text":""},{"location":"reference/models/#rexnet","title":"rexnet","text":""},{"location":"reference/models/#senet","title":"senet","text":""},{"location":"reference/models/#shufflenetv1","title":"shufflenetv1","text":""},{"location":"reference/models/#shufflenetv2","title":"shufflenetv2","text":""},{"location":"reference/models/#sknet","title":"sknet","text":""},{"location":"reference/models/#squeezenet","title":"squeezenet","text":""},{"location":"reference/models/#swintransformer","title":"swintransformer","text":""},{"location":"reference/models/#swintransformerv2","title":"swintransformerv2","text":""},{"location":"reference/models/#vgg","title":"vgg","text":""},{"location":"reference/models/#visformer","title":"visformer","text":""},{"location":"reference/models/#vit","title":"vit","text":""},{"location":"reference/models/#volo","title":"volo","text":""},{"location":"reference/models/#xcit","title":"xcit","text":""},{"location":"reference/optim/","title":"Optimizer","text":""},{"location":"reference/optim/#optimizer-factory","title":"Optimizer Factory","text":""},{"location":"reference/optim/#mindcv.optim.optim_factory.create_optimizer","title":"<code>mindcv.optim.optim_factory.create_optimizer(model_or_params, opt='adam', lr=0.001, weight_decay=0, momentum=0.9, nesterov=False, weight_decay_filter='disable', layer_decay=None, loss_scale=1.0, schedule_decay=0.004, checkpoint_path='', eps=1e-10, **kwargs)</code>","text":"<p>Creates optimizer by name.</p> PARAMETER DESCRIPTION <code>model_or_params</code> <p>network or network parameters. Union[list[Parameter],list[dict], nn.Cell], which must be the list of parameters or list of dicts or nn.Cell. When the list element is a dictionary, the key of the dictionary can be \"params\", \"lr\", \"weight_decay\",\"grad_centralization\" and \"order_params\".</p> <p> </p> <code>opt</code> <p>wrapped optimizer. You could choose like 'sgd', 'nesterov', 'momentum', 'adam', 'adamw', 'lion', 'rmsprop', 'adagrad', 'lamb'. 'adam' is the default choose for convolution-based networks. 'adamw' is recommended for ViT-based networks. Default: 'adam'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'adam'</code> </p> <code>lr</code> <p>learning rate: float or lr scheduler. Fixed and dynamic learning rate are supported. Default: 1e-3.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>0.001</code> </p> <code>weight_decay</code> <p>weight decay factor. It should be noted that weight decay can be a constant value or a Cell. It is a Cell only when dynamic weight decay is applied. Dynamic weight decay is similar to dynamic learning rate, users need to customize a weight decay schedule only with global step as input, and during training, the optimizer calls the instance of WeightDecaySchedule to get the weight decay value of current step. Default: 0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0</code> </p> <code>momentum</code> <p>momentum if the optimizer supports. Default: 0.9.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.9</code> </p> <code>nesterov</code> <p>Whether to use Nesterov Accelerated Gradient (NAG) algorithm to update the gradients. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>weight_decay_filter</code> <p>filters to filter parameters from weight_decay. - \"disable\": No parameters to filter. - \"auto\": We do not apply weight decay filtering to any parameters. However, MindSpore currently         automatically filters the parameters of Norm layer from weight decay. - \"norm_and_bias\": Filter the paramters of Norm layer and Bias from weight decay.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'disable'</code> </p> <code>layer_decay</code> <p>for apply layer-wise learning rate decay.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>loss_scale</code> <p>A floating point value for the loss scale, which must be larger than 0.0. Default: 1.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> RETURNS DESCRIPTION <p>Optimizer object</p> Source code in <code>mindcv/optim/optim_factory.py</code> <pre><code>def create_optimizer(\n    model_or_params,\n    opt: str = \"adam\",\n    lr: Optional[float] = 1e-3,\n    weight_decay: float = 0,\n    momentum: float = 0.9,\n    nesterov: bool = False,\n    weight_decay_filter: str = \"disable\",\n    layer_decay: Optional[float] = None,\n    loss_scale: float = 1.0,\n    schedule_decay: float = 4e-3,\n    checkpoint_path: str = \"\",\n    eps: float = 1e-10,\n    **kwargs,\n):\n    r\"\"\"Creates optimizer by name.\n\n    Args:\n        model_or_params: network or network parameters. Union[list[Parameter],list[dict], nn.Cell], which must be\n            the list of parameters or list of dicts or nn.Cell. When the list element is a dictionary, the key of\n            the dictionary can be \"params\", \"lr\", \"weight_decay\",\"grad_centralization\" and \"order_params\".\n        opt: wrapped optimizer. You could choose like 'sgd', 'nesterov', 'momentum', 'adam', 'adamw', 'lion',\n            'rmsprop', 'adagrad', 'lamb'. 'adam' is the default choose for convolution-based networks.\n            'adamw' is recommended for ViT-based networks. Default: 'adam'.\n        lr: learning rate: float or lr scheduler. Fixed and dynamic learning rate are supported. Default: 1e-3.\n        weight_decay: weight decay factor. It should be noted that weight decay can be a constant value or a Cell.\n            It is a Cell only when dynamic weight decay is applied. Dynamic weight decay is similar to\n            dynamic learning rate, users need to customize a weight decay schedule only with global step as input,\n            and during training, the optimizer calls the instance of WeightDecaySchedule to get the weight decay value\n            of current step. Default: 0.\n        momentum: momentum if the optimizer supports. Default: 0.9.\n        nesterov: Whether to use Nesterov Accelerated Gradient (NAG) algorithm to update the gradients. Default: False.\n        weight_decay_filter: filters to filter parameters from weight_decay.\n            - \"disable\": No parameters to filter.\n            - \"auto\": We do not apply weight decay filtering to any parameters. However, MindSpore currently\n                    automatically filters the parameters of Norm layer from weight decay.\n            - \"norm_and_bias\": Filter the paramters of Norm layer and Bias from weight decay.\n        layer_decay: for apply layer-wise learning rate decay.\n        loss_scale: A floating point value for the loss scale, which must be larger than 0.0. Default: 1.0.\n\n    Returns:\n        Optimizer object\n    \"\"\"\n\n    no_weight_decay = {}\n    if isinstance(model_or_params, nn.Cell):\n        # a model was passed in, extract parameters and add weight decays to appropriate layers\n        if hasattr(model_or_params, \"no_weight_decay\"):\n            no_weight_decay = model_or_params.no_weight_decay()\n        params = model_or_params.trainable_params()\n\n    else:\n        params = model_or_params\n\n    if weight_decay_filter == \"auto\":\n        _logger.warning(\n            \"You are using AUTO weight decay filter, which means the weight decay filter isn't explicitly pass in \"\n            \"when creating an mindspore.nn.Optimizer instance. \"\n            \"NOTE: mindspore.nn.Optimizer will filter Norm parmas from weight decay. \"\n        )\n    elif layer_decay is not None and isinstance(model_or_params, nn.Cell):\n        params = param_groups_layer_decay(\n            model_or_params,\n            lr=lr,\n            weight_decay=weight_decay,\n            layer_decay=layer_decay,\n            no_weight_decay_list=no_weight_decay,\n        )\n        weight_decay = 0.0\n    elif weight_decay_filter == \"disable\" or \"norm_and_bias\":\n        params = init_group_params(params, weight_decay, weight_decay_filter, no_weight_decay)\n        weight_decay = 0.0\n    else:\n        raise ValueError(\n            f\"weight decay filter only support ['disable', 'auto', 'norm_and_bias'], but got{weight_decay_filter}.\"\n        )\n\n    opt = opt.lower()\n    opt_args = dict(**kwargs)\n    # if lr is not None:\n    #    opt_args.setdefault('lr', lr)\n\n    # non-adaptive: SGD, momentum, and nesterov\n    if opt == \"sgd\":\n        # note: nn.Momentum may perform better if momentum &gt; 0.\n        optimizer = nn.SGD(\n            params=params,\n            learning_rate=lr,\n            momentum=momentum,\n            weight_decay=weight_decay,\n            nesterov=nesterov,\n            loss_scale=loss_scale,\n            **opt_args,\n        )\n    elif opt in [\"momentum\", \"nesterov\"]:\n        optimizer = nn.Momentum(\n            params=params,\n            learning_rate=lr,\n            momentum=momentum,\n            weight_decay=weight_decay,\n            use_nesterov=nesterov,\n            loss_scale=loss_scale,\n        )\n    # adaptive\n    elif opt == \"adam\":\n        optimizer = nn.Adam(\n            params=params,\n            learning_rate=lr,\n            weight_decay=weight_decay,\n            loss_scale=loss_scale,\n            use_nesterov=nesterov,\n            **opt_args,\n        )\n    elif opt == \"adamw\":\n        optimizer = AdamW(\n            params=params,\n            learning_rate=lr,\n            weight_decay=weight_decay,\n            loss_scale=loss_scale,\n            **opt_args,\n        )\n    elif opt == \"lion\":\n        optimizer = Lion(\n            params=params,\n            learning_rate=lr,\n            weight_decay=weight_decay,\n            loss_scale=loss_scale,\n            **opt_args,\n        )\n    elif opt == \"nadam\":\n        optimizer = NAdam(\n            params=params,\n            learning_rate=lr,\n            weight_decay=weight_decay,\n            loss_scale=loss_scale,\n            schedule_decay=schedule_decay,\n            **opt_args,\n        )\n    elif opt == \"adan\":\n        optimizer = Adan(\n            params=params,\n            learning_rate=lr,\n            weight_decay=weight_decay,\n            loss_scale=loss_scale,\n            **opt_args,\n        )\n    elif opt == \"rmsprop\":\n        optimizer = nn.RMSProp(\n            params=params,\n            learning_rate=lr,\n            momentum=momentum,\n            weight_decay=weight_decay,\n            loss_scale=loss_scale,\n            epsilon=eps,\n            **opt_args,\n        )\n    elif opt == \"adagrad\":\n        optimizer = nn.Adagrad(\n            params=params,\n            learning_rate=lr,\n            weight_decay=weight_decay,\n            loss_scale=loss_scale,\n            **opt_args,\n        )\n    elif opt == \"lamb\":\n        assert loss_scale == 1.0, \"Loss scaler is not supported by Lamb optimizer\"\n        optimizer = nn.Lamb(\n            params=params,\n            learning_rate=lr,\n            weight_decay=weight_decay,\n            **opt_args,\n        )\n    else:\n        raise ValueError(f\"Invalid optimizer: {opt}\")\n\n    if os.path.exists(checkpoint_path):\n        param_dict = load_checkpoint(checkpoint_path)\n        load_param_into_net(optimizer, param_dict)\n\n    return optimizer\n</code></pre>"},{"location":"reference/optim/#adamw","title":"AdamW","text":""},{"location":"reference/optim/#mindcv.optim.adamw.AdamW","title":"<code>mindcv.optim.adamw.AdamW</code>","text":"<p>               Bases: <code>Optimizer</code></p> <p>Implements the gradient clipping by norm for a AdamWeightDecay optimizer.</p> Source code in <code>mindcv/optim/adamw.py</code> <pre><code>class AdamW(Optimizer):\n    \"\"\"\n    Implements the gradient clipping by norm for a AdamWeightDecay optimizer.\n    \"\"\"\n\n    @opt_init_args_register\n    def __init__(\n        self,\n        params,\n        learning_rate=1e-3,\n        beta1=0.9,\n        beta2=0.999,\n        eps=1e-8,\n        weight_decay=0.0,\n        loss_scale=1.0,\n        clip=False,\n    ):\n        super().__init__(learning_rate, params, weight_decay)\n        _check_param_value(beta1, beta2, eps, self.cls_name)\n        self.beta1 = Tensor(np.array([beta1]).astype(np.float32))\n        self.beta2 = Tensor(np.array([beta2]).astype(np.float32))\n        self.eps = Tensor(np.array([eps]).astype(np.float32))\n        self.moments1 = self.parameters.clone(prefix=\"adam_m\", init=\"zeros\")\n        self.moments2 = self.parameters.clone(prefix=\"adam_v\", init=\"zeros\")\n        self.hyper_map = ops.HyperMap()\n        self.beta1_power = Parameter(initializer(1, [1], ms.float32), name=\"beta1_power\")\n        self.beta2_power = Parameter(initializer(1, [1], ms.float32), name=\"beta2_power\")\n\n        self.reciprocal_scale = Tensor(1.0 / loss_scale, ms.float32)\n        self.clip = clip\n\n    def get_lr(self):\n        \"\"\"\n        The optimizer calls this interface to get the learning rate for the current step. User-defined optimizers based\n        on :class:`mindspore.nn.Optimizer` can also call this interface before updating the parameters.\n\n        Returns:\n            float, the learning rate of current step.\n        \"\"\"\n        lr = self.learning_rate\n        if self.dynamic_lr:\n            if self.is_group_lr:\n                lr = ()\n                for learning_rate in self.learning_rate:\n                    current_dynamic_lr = learning_rate(self.global_step).reshape(())\n                    lr += (current_dynamic_lr,)\n            else:\n                lr = self.learning_rate(self.global_step).reshape(())\n        if self._is_dynamic_lr_or_weight_decay():\n            self.assignadd(self.global_step, self.global_step_increase_tensor)\n        return lr\n\n    @jit\n    def construct(self, gradients):\n        lr = self.get_lr()\n        gradients = scale_grad(gradients, self.reciprocal_scale)\n        if self.clip:\n            gradients = ops.clip_by_global_norm(gradients, 5.0, None)\n\n        beta1_power = self.beta1_power * self.beta1\n        self.beta1_power = beta1_power\n        beta2_power = self.beta2_power * self.beta2\n        self.beta2_power = beta2_power\n\n        if self.is_group:\n            if self.is_group_lr:\n                optim_result = self.hyper_map(\n                    ops.partial(_adam_opt, beta1_power, beta2_power, self.beta1, self.beta2, self.eps),\n                    lr,\n                    self.weight_decay,\n                    self.parameters,\n                    self.moments1,\n                    self.moments2,\n                    gradients,\n                    self.decay_flags,\n                    self.optim_filter,\n                )\n            else:\n                optim_result = self.hyper_map(\n                    ops.partial(_adam_opt, beta1_power, beta2_power, self.beta1, self.beta2, self.eps, lr),\n                    self.weight_decay,\n                    self.parameters,\n                    self.moments1,\n                    self.moments2,\n                    gradients,\n                    self.decay_flags,\n                    self.optim_filter,\n                )\n        else:\n            optim_result = self.hyper_map(\n                ops.partial(\n                    _adam_opt, beta1_power, beta2_power, self.beta1, self.beta2, self.eps, lr, self.weight_decay\n                ),\n                self.parameters,\n                self.moments1,\n                self.moments2,\n                gradients,\n                self.decay_flags,\n                self.optim_filter,\n            )\n        if self.use_parallel:\n            self.broadcast_params(optim_result)\n        return optim_result\n</code></pre>"},{"location":"reference/optim/#mindcv.optim.adamw.AdamW.get_lr","title":"<code>mindcv.optim.adamw.AdamW.get_lr()</code>","text":"<p>The optimizer calls this interface to get the learning rate for the current step. User-defined optimizers based on :class:<code>mindspore.nn.Optimizer</code> can also call this interface before updating the parameters.</p> RETURNS DESCRIPTION <p>float, the learning rate of current step.</p> Source code in <code>mindcv/optim/adamw.py</code> <pre><code>def get_lr(self):\n    \"\"\"\n    The optimizer calls this interface to get the learning rate for the current step. User-defined optimizers based\n    on :class:`mindspore.nn.Optimizer` can also call this interface before updating the parameters.\n\n    Returns:\n        float, the learning rate of current step.\n    \"\"\"\n    lr = self.learning_rate\n    if self.dynamic_lr:\n        if self.is_group_lr:\n            lr = ()\n            for learning_rate in self.learning_rate:\n                current_dynamic_lr = learning_rate(self.global_step).reshape(())\n                lr += (current_dynamic_lr,)\n        else:\n            lr = self.learning_rate(self.global_step).reshape(())\n    if self._is_dynamic_lr_or_weight_decay():\n        self.assignadd(self.global_step, self.global_step_increase_tensor)\n    return lr\n</code></pre>"},{"location":"reference/optim/#adan","title":"Adan","text":""},{"location":"reference/optim/#mindcv.optim.adan.Adan","title":"<code>mindcv.optim.adan.Adan</code>","text":"<p>               Bases: <code>Optimizer</code></p> <p>The Adan (ADAptive Nesterov momentum algorithm) Optimizer from https://arxiv.org/abs/2208.06677</p> <p>Note: it is an experimental version.</p> Source code in <code>mindcv/optim/adan.py</code> <pre><code>class Adan(Optimizer):\n    \"\"\"\n    The Adan (ADAptive Nesterov momentum algorithm) Optimizer from https://arxiv.org/abs/2208.06677\n\n    Note: it is an experimental version.\n    \"\"\"\n\n    @opt_init_args_register\n    def __init__(\n        self,\n        params,\n        learning_rate=1e-3,\n        beta1=0.98,\n        beta2=0.92,\n        beta3=0.99,\n        eps=1e-8,\n        use_locking=False,\n        weight_decay=0.0,\n        loss_scale=1.0,\n    ):\n        super().__init__(\n            learning_rate, params, weight_decay=weight_decay, loss_scale=loss_scale\n        )  # Optimized inherit weight decay is bloaked. weight decay is computed in this py.\n\n        _check_param_value(beta1, beta2, eps, self.cls_name)\n        assert isinstance(use_locking, bool), f\"For {self.cls_name}, use_looking should be bool\"\n\n        self.beta1 = Tensor(beta1, mstype.float32)\n        self.beta2 = Tensor(beta2, mstype.float32)\n        self.beta3 = Tensor(beta3, mstype.float32)\n\n        self.eps = Tensor(eps, mstype.float32)\n        self.use_locking = use_locking\n        self.moment1 = self._parameters.clone(prefix=\"moment1\", init=\"zeros\")  # m\n        self.moment2 = self._parameters.clone(prefix=\"moment2\", init=\"zeros\")  # v\n        self.moment3 = self._parameters.clone(prefix=\"moment3\", init=\"zeros\")  # n\n        self.prev_gradient = self._parameters.clone(prefix=\"prev_gradient\", init=\"zeros\")\n\n        self.weight_decay = Tensor(weight_decay, mstype.float32)\n\n    def get_lr(self):\n        \"\"\"\n        The optimizer calls this interface to get the learning rate for the current step. User-defined optimizers based\n        on :class:`mindspore.nn.Optimizer` can also call this interface before updating the parameters.\n\n        Returns:\n            float, the learning rate of current step.\n        \"\"\"\n        lr = self.learning_rate\n        if self.dynamic_lr:\n            if self.is_group_lr:\n                lr = ()\n                for learning_rate in self.learning_rate:\n                    current_dynamic_lr = learning_rate(self.global_step).reshape(())\n                    lr += (current_dynamic_lr,)\n            else:\n                lr = self.learning_rate(self.global_step).reshape(())\n        if self._is_dynamic_lr_or_weight_decay():\n            self.assignadd(self.global_step, self.global_step_increase_tensor)\n        return lr\n\n    @jit\n    def construct(self, gradients):\n        params = self._parameters\n        moment1 = self.moment1\n        moment2 = self.moment2\n        moment3 = self.moment3\n\n        gradients = self.flatten_gradients(gradients)\n        gradients = self.gradients_centralization(gradients)\n        gradients = self.scale_grad(gradients)\n        gradients = self._grad_sparse_indices_deduplicate(gradients)\n        lr = self.get_lr()\n\n        # TODO: currently not support dist\n        success = self.map_(\n            ops.partial(_adan_opt, self.beta1, self.beta2, self.beta3, self.eps, lr, self.weight_decay),\n            params,\n            moment1,\n            moment2,\n            moment3,\n            gradients,\n            self.prev_gradient,\n        )\n\n        return success\n\n    @Optimizer.target.setter\n    def target(self, value):\n        \"\"\"\n        If the input value is set to \"CPU\", the parameters will be updated on the host using the Fused\n        optimizer operation.\n        \"\"\"\n        self._set_base_target(value)\n</code></pre>"},{"location":"reference/optim/#mindcv.optim.adan.Adan.get_lr","title":"<code>mindcv.optim.adan.Adan.get_lr()</code>","text":"<p>The optimizer calls this interface to get the learning rate for the current step. User-defined optimizers based on :class:<code>mindspore.nn.Optimizer</code> can also call this interface before updating the parameters.</p> RETURNS DESCRIPTION <p>float, the learning rate of current step.</p> Source code in <code>mindcv/optim/adan.py</code> <pre><code>def get_lr(self):\n    \"\"\"\n    The optimizer calls this interface to get the learning rate for the current step. User-defined optimizers based\n    on :class:`mindspore.nn.Optimizer` can also call this interface before updating the parameters.\n\n    Returns:\n        float, the learning rate of current step.\n    \"\"\"\n    lr = self.learning_rate\n    if self.dynamic_lr:\n        if self.is_group_lr:\n            lr = ()\n            for learning_rate in self.learning_rate:\n                current_dynamic_lr = learning_rate(self.global_step).reshape(())\n                lr += (current_dynamic_lr,)\n        else:\n            lr = self.learning_rate(self.global_step).reshape(())\n    if self._is_dynamic_lr_or_weight_decay():\n        self.assignadd(self.global_step, self.global_step_increase_tensor)\n    return lr\n</code></pre>"},{"location":"reference/optim/#mindcv.optim.adan.Adan.target","title":"<code>mindcv.optim.adan.Adan.target(value)</code>","text":"<p>If the input value is set to \"CPU\", the parameters will be updated on the host using the Fused optimizer operation.</p> Source code in <code>mindcv/optim/adan.py</code> <pre><code>@Optimizer.target.setter\ndef target(self, value):\n    \"\"\"\n    If the input value is set to \"CPU\", the parameters will be updated on the host using the Fused\n    optimizer operation.\n    \"\"\"\n    self._set_base_target(value)\n</code></pre>"},{"location":"reference/optim/#lion","title":"Lion","text":""},{"location":"reference/optim/#mindcv.optim.lion.Lion","title":"<code>mindcv.optim.lion.Lion</code>","text":"<p>               Bases: <code>Optimizer</code></p> <p>Implementation of Lion optimizer from paper 'https://arxiv.org/abs/2302.06675'. Additionally, this implementation is with gradient clipping.</p> <p>Notes: lr is usually 3-10x smaller than adamw. weight decay is usually 3-10x larger than adamw.</p> Source code in <code>mindcv/optim/lion.py</code> <pre><code>class Lion(Optimizer):\n    \"\"\"\n    Implementation of Lion optimizer from paper 'https://arxiv.org/abs/2302.06675'.\n    Additionally, this implementation is with gradient clipping.\n\n    Notes:\n    lr is usually 3-10x smaller than adamw.\n    weight decay is usually 3-10x larger than adamw.\n    \"\"\"\n\n    @opt_init_args_register\n    def __init__(\n        self,\n        params,\n        learning_rate=2e-4,\n        beta1=0.9,\n        beta2=0.99,\n        weight_decay=0.0,\n        loss_scale=1.0,\n        clip=False,\n    ):\n        super().__init__(learning_rate, params, weight_decay)\n        _check_param_value(beta1, beta2, self.cls_name)\n        self.beta1 = Tensor(np.array([beta1]).astype(np.float32))\n        self.beta2 = Tensor(np.array([beta2]).astype(np.float32))\n        self.moments1 = self.parameters.clone(prefix=\"lion_m\", init=\"zeros\")\n        self.hyper_map = ops.HyperMap()\n        self.beta1_power = Parameter(initializer(1, [1], ms.float32), name=\"beta1_power\")\n        self.beta2_power = Parameter(initializer(1, [1], ms.float32), name=\"beta2_power\")\n\n        self.reciprocal_scale = Tensor(1.0 / loss_scale, ms.float32)\n        self.clip = clip\n\n    def get_lr(self):\n        \"\"\"\n        The optimizer calls this interface to get the learning rate for the current step. User-defined optimizers based\n        on :class:`mindspore.nn.Optimizer` can also call this interface before updating the parameters.\n\n        Returns:\n            float, the learning rate of current step.\n        \"\"\"\n        lr = self.learning_rate\n        if self.dynamic_lr:\n            if self.is_group_lr:\n                lr = ()\n                for learning_rate in self.learning_rate:\n                    current_dynamic_lr = learning_rate(self.global_step).reshape(())\n                    lr += (current_dynamic_lr,)\n            else:\n                lr = self.learning_rate(self.global_step).reshape(())\n        if self._is_dynamic_lr_or_weight_decay():\n            self.assignadd(self.global_step, self.global_step_increase_tensor)\n        return lr\n\n    @jit\n    def construct(self, gradients):\n        lr = self.get_lr()\n        gradients = scale_grad(gradients, self.reciprocal_scale)\n        if self.clip:\n            gradients = ops.clip_by_global_norm(gradients, 5.0, None)\n\n        beta1_power = self.beta1_power * self.beta1\n        self.beta1_power = beta1_power\n        beta2_power = self.beta2_power * self.beta2\n        self.beta2_power = beta2_power\n\n        if self.is_group:\n            if self.is_group_lr:\n                optim_result = self.hyper_map(\n                    ops.partial(_lion_opt, beta1_power, beta2_power, self.beta1, self.beta2),\n                    lr,\n                    self.weight_decay,\n                    self.parameters,\n                    self.moments1,\n                    gradients,\n                    self.decay_flags,\n                    self.optim_filter,\n                )\n            else:\n                optim_result = self.hyper_map(\n                    ops.partial(_lion_opt, beta1_power, beta2_power, self.beta1, self.beta2, lr),\n                    self.weight_decay,\n                    self.parameters,\n                    self.moments1,\n                    gradients,\n                    self.decay_flags,\n                    self.optim_filter,\n                )\n        else:\n            optim_result = self.hyper_map(\n                ops.partial(_lion_opt, beta1_power, beta2_power, self.beta1, self.beta2, lr, self.weight_decay),\n                self.parameters,\n                self.moments1,\n                gradients,\n                self.decay_flags,\n                self.optim_filter,\n            )\n        if self.use_parallel:\n            self.broadcast_params(optim_result)\n        return optim_result\n</code></pre>"},{"location":"reference/optim/#mindcv.optim.lion.Lion.get_lr","title":"<code>mindcv.optim.lion.Lion.get_lr()</code>","text":"<p>The optimizer calls this interface to get the learning rate for the current step. User-defined optimizers based on :class:<code>mindspore.nn.Optimizer</code> can also call this interface before updating the parameters.</p> RETURNS DESCRIPTION <p>float, the learning rate of current step.</p> Source code in <code>mindcv/optim/lion.py</code> <pre><code>def get_lr(self):\n    \"\"\"\n    The optimizer calls this interface to get the learning rate for the current step. User-defined optimizers based\n    on :class:`mindspore.nn.Optimizer` can also call this interface before updating the parameters.\n\n    Returns:\n        float, the learning rate of current step.\n    \"\"\"\n    lr = self.learning_rate\n    if self.dynamic_lr:\n        if self.is_group_lr:\n            lr = ()\n            for learning_rate in self.learning_rate:\n                current_dynamic_lr = learning_rate(self.global_step).reshape(())\n                lr += (current_dynamic_lr,)\n        else:\n            lr = self.learning_rate(self.global_step).reshape(())\n    if self._is_dynamic_lr_or_weight_decay():\n        self.assignadd(self.global_step, self.global_step_increase_tensor)\n    return lr\n</code></pre>"},{"location":"reference/optim/#nadam","title":"NAdam","text":""},{"location":"reference/optim/#mindcv.optim.nadam.NAdam","title":"<code>mindcv.optim.nadam.NAdam</code>","text":"<p>               Bases: <code>Optimizer</code></p> <p>Implements NAdam algorithm (a variant of Adam based on Nesterov momentum).</p> Source code in <code>mindcv/optim/nadam.py</code> <pre><code>class NAdam(Optimizer):\n    \"\"\"\n    Implements NAdam algorithm (a variant of Adam based on Nesterov momentum).\n    \"\"\"\n\n    @opt_init_args_register\n    def __init__(\n        self,\n        params,\n        learning_rate=2e-3,\n        beta1=0.9,\n        beta2=0.999,\n        eps=1e-8,\n        weight_decay=0.0,\n        loss_scale=1.0,\n        schedule_decay=4e-3,\n    ):\n        super().__init__(learning_rate, params, weight_decay, loss_scale)\n        _check_param_value(beta1, beta2, eps, self.cls_name)\n        self.beta1 = Tensor(np.array([beta1]).astype(np.float32))\n        self.beta2 = Tensor(np.array([beta2]).astype(np.float32))\n        self.eps = Tensor(np.array([eps]).astype(np.float32))\n        self.moments1 = self.parameters.clone(prefix=\"nadam_m\", init=\"zeros\")\n        self.moments2 = self.parameters.clone(prefix=\"nadam_v\", init=\"zeros\")\n        self.schedule_decay = Tensor(np.array([schedule_decay]).astype(np.float32))\n        self.mu_schedule = Parameter(initializer(1, [1], ms.float32), name=\"mu_schedule\")\n        self.beta2_power = Parameter(initializer(1, [1], ms.float32), name=\"beta2_power\")\n\n    def get_lr(self):\n        \"\"\"\n        The optimizer calls this interface to get the learning rate for the current step. User-defined optimizers based\n        on :class:`mindspore.nn.Optimizer` can also call this interface before updating the parameters.\n\n        Returns:\n            float, the learning rate of current step.\n        \"\"\"\n        lr = self.learning_rate\n        if self.dynamic_lr:\n            if self.is_group_lr:\n                lr = ()\n                for learning_rate in self.learning_rate:\n                    current_dynamic_lr = learning_rate(self.global_step).reshape(())\n                    lr += (current_dynamic_lr,)\n            else:\n                lr = self.learning_rate(self.global_step).reshape(())\n        if self._is_dynamic_lr_or_weight_decay():\n            self.assignadd(self.global_step, self.global_step_increase_tensor)\n        return lr\n\n    @jit\n    def construct(self, gradients):\n        lr = self.get_lr()\n        params = self.parameters\n        step = self.global_step + _scaler_one\n        gradients = self.decay_weight(gradients)\n        mu = self.beta1 * (\n            _scaler_one - Tensor(0.5, ms.float32) * ops.pow(Tensor(0.96, ms.float32), step * self.schedule_decay)\n        )\n        mu_next = self.beta1 * (\n            _scaler_one\n            - Tensor(0.5, ms.float32) * ops.pow(Tensor(0.96, ms.float32), (step + _scaler_one) * self.schedule_decay)\n        )\n        mu_schedule = self.mu_schedule * mu\n        mu_schedule_next = self.mu_schedule * mu * mu_next\n        self.mu_schedule = mu_schedule\n        beta2_power = self.beta2_power * self.beta2\n        self.beta2_power = beta2_power\n\n        num_params = len(params)\n        for i in range(num_params):\n            ops.assign(self.moments1[i], self.beta1 * self.moments1[i] + (_scaler_one - self.beta1) * gradients[i])\n            ops.assign(\n                self.moments2[i], self.beta2 * self.moments2[i] + (_scaler_one - self.beta2) * ops.square(gradients[i])\n            )\n\n            regulate_m = mu_next * self.moments1[i] / (_scaler_one - mu_schedule_next) + (_scaler_one - mu) * gradients[\n                i\n            ] / (_scaler_one - mu_schedule)\n            regulate_v = self.moments2[i] / (_scaler_one - beta2_power)\n\n            update = params[i] - lr * regulate_m / (self.eps + ops.sqrt(regulate_v))\n            ops.assign(params[i], update)\n\n        return params\n</code></pre>"},{"location":"reference/optim/#mindcv.optim.nadam.NAdam.get_lr","title":"<code>mindcv.optim.nadam.NAdam.get_lr()</code>","text":"<p>The optimizer calls this interface to get the learning rate for the current step. User-defined optimizers based on :class:<code>mindspore.nn.Optimizer</code> can also call this interface before updating the parameters.</p> RETURNS DESCRIPTION <p>float, the learning rate of current step.</p> Source code in <code>mindcv/optim/nadam.py</code> <pre><code>def get_lr(self):\n    \"\"\"\n    The optimizer calls this interface to get the learning rate for the current step. User-defined optimizers based\n    on :class:`mindspore.nn.Optimizer` can also call this interface before updating the parameters.\n\n    Returns:\n        float, the learning rate of current step.\n    \"\"\"\n    lr = self.learning_rate\n    if self.dynamic_lr:\n        if self.is_group_lr:\n            lr = ()\n            for learning_rate in self.learning_rate:\n                current_dynamic_lr = learning_rate(self.global_step).reshape(())\n                lr += (current_dynamic_lr,)\n        else:\n            lr = self.learning_rate(self.global_step).reshape(())\n    if self._is_dynamic_lr_or_weight_decay():\n        self.assignadd(self.global_step, self.global_step_increase_tensor)\n    return lr\n</code></pre>"},{"location":"reference/scheduler/","title":"Learning Rate Scheduler","text":""},{"location":"reference/scheduler/#scheduler-factory","title":"Scheduler Factory","text":""},{"location":"reference/scheduler/#mindcv.scheduler.scheduler_factory.create_scheduler","title":"<code>mindcv.scheduler.scheduler_factory.create_scheduler(steps_per_epoch, scheduler='constant', lr=0.01, min_lr=1e-06, warmup_epochs=3, warmup_factor=0.0, decay_epochs=10, decay_rate=0.9, milestones=None, num_epochs=200, num_cycles=1, cycle_decay=1.0, lr_epoch_stair=False)</code>","text":"<p>Creates learning rate scheduler by name.</p> PARAMETER DESCRIPTION <code>steps_per_epoch</code> <p>number of steps per epoch.</p> <p> TYPE: <code>int</code> </p> <code>scheduler</code> <p>scheduler name like 'constant', 'cosine_decay', 'step_decay', 'exponential_decay', 'polynomial_decay', 'multi_step_decay'. Default: 'constant'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'constant'</code> </p> <code>lr</code> <p>learning rate value. Default: 0.01.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.01</code> </p> <code>min_lr</code> <p>lower lr bound for 'cosine_decay' schedulers. Default: 1e-6.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1e-06</code> </p> <code>warmup_epochs</code> <p>epochs to warmup LR, if scheduler supports. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>warmup_factor</code> <p>the warmup phase of scheduler is a linearly increasing lr, the beginning factor is <code>warmup_factor</code>, i.e., the lr of the first step/epoch is lr*warmup_factor, and the ending lr in the warmup phase is lr. Default: 0.0</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>decay_epochs</code> <p>for 'cosine_decay' schedulers, decay LR to min_lr in <code>decay_epochs</code>. For 'step_decay' scheduler, decay LR by a factor of <code>decay_rate</code> every <code>decay_epochs</code>. Default: 10.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>decay_rate</code> <p>LR decay rate. Default: 0.9.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.9</code> </p> <code>milestones</code> <p>list of epoch milestones for 'multi_step_decay' scheduler. Must be increasing. Default: None</p> <p> TYPE: <code>list</code> DEFAULT: <code>None</code> </p> <code>num_epochs</code> <p>Number of total epochs. Default: 200.</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> <code>num_cycles</code> <p>Number of cycles for cosine decay and cyclic. Default: 1.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>cycle_decay</code> <p>Decay rate of lr max in each cosine cycle. Default: 1.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>lr_epoch_stair</code> <p>If True, LR will be updated in the beginning of each new epoch and the LR will be consistent for each batch in one epoch. Otherwise, learning rate will be updated dynamically in each step. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>mindcv/scheduler/scheduler_factory.py</code> <pre><code>def create_scheduler(\n    steps_per_epoch: int,\n    scheduler: str = \"constant\",\n    lr: float = 0.01,\n    min_lr: float = 1e-6,\n    warmup_epochs: int = 3,\n    warmup_factor: float = 0.0,\n    decay_epochs: int = 10,\n    decay_rate: float = 0.9,\n    milestones: list = None,\n    num_epochs: int = 200,\n    num_cycles: int = 1,\n    cycle_decay: float = 1.0,\n    lr_epoch_stair: bool = False,\n):\n    r\"\"\"Creates learning rate scheduler by name.\n\n    Args:\n        steps_per_epoch: number of steps per epoch.\n        scheduler: scheduler name like 'constant', 'cosine_decay', 'step_decay',\n            'exponential_decay', 'polynomial_decay', 'multi_step_decay'. Default: 'constant'.\n        lr: learning rate value. Default: 0.01.\n        min_lr: lower lr bound for 'cosine_decay' schedulers. Default: 1e-6.\n        warmup_epochs: epochs to warmup LR, if scheduler supports. Default: 3.\n        warmup_factor: the warmup phase of scheduler is a linearly increasing lr,\n            the beginning factor is `warmup_factor`, i.e., the lr of the first step/epoch is lr*warmup_factor,\n            and the ending lr in the warmup phase is lr. Default: 0.0\n        decay_epochs: for 'cosine_decay' schedulers, decay LR to min_lr in `decay_epochs`.\n            For 'step_decay' scheduler, decay LR by a factor of `decay_rate` every `decay_epochs`. Default: 10.\n        decay_rate: LR decay rate. Default: 0.9.\n        milestones: list of epoch milestones for 'multi_step_decay' scheduler. Must be increasing. Default: None\n        num_epochs: Number of total epochs. Default: 200.\n        num_cycles: Number of cycles for cosine decay and cyclic. Default: 1.\n        cycle_decay: Decay rate of lr max in each cosine cycle. Default: 1.0.\n        lr_epoch_stair: If True, LR will be updated in the beginning of each new epoch\n            and the LR will be consistent for each batch in one epoch.\n            Otherwise, learning rate will be updated dynamically in each step. Default: False.\n    Returns:\n        Cell object for computing LR with input of current global steps\n    \"\"\"\n    # check params\n    if milestones is None:\n        milestones = []\n\n    if warmup_epochs + decay_epochs &gt; num_epochs:\n        _logger.warning(\"warmup_epochs + decay_epochs &gt; num_epochs. Please check and reduce decay_epochs!\")\n\n    # lr warmup phase\n    warmup_lr_scheduler = []\n    if warmup_epochs &gt; 0:\n        if warmup_factor == 0 and lr_epoch_stair:\n            _logger.warning(\n                \"The warmup factor is set to 0, lr of 0-th epoch is always zero! \" \"Recommend value is 0.01.\"\n            )\n        warmup_func = linear_lr if lr_epoch_stair else linear_refined_lr\n        warmup_lr_scheduler = warmup_func(\n            start_factor=warmup_factor,\n            end_factor=1.0,\n            total_iters=warmup_epochs,\n            lr=lr,\n            steps_per_epoch=steps_per_epoch,\n            epochs=warmup_epochs,\n        )\n\n    # lr decay phase\n    main_epochs = num_epochs - warmup_epochs\n    if scheduler in [\"cosine_decay\", \"warmup_cosine_decay\"]:\n        cosine_func = cosine_decay_lr if lr_epoch_stair else cosine_decay_refined_lr\n        main_lr_scheduler = cosine_func(\n            decay_epochs=decay_epochs,\n            eta_min=min_lr,\n            eta_max=lr,\n            steps_per_epoch=steps_per_epoch,\n            epochs=main_epochs,\n            num_cycles=num_cycles,\n            cycle_decay=cycle_decay,\n        )\n    elif scheduler == \"one_cycle\":\n        if lr_epoch_stair or warmup_epochs &gt; 0:\n            raise ValueError(\n                \"OneCycle scheduler doesn't support learning rate varies with epoch and warmup_epochs &gt; 0.\"\n            )\n        div_factor = 25.0\n        initial_lr = lr / div_factor\n        final_div_factor = initial_lr / min_lr\n        main_lr_scheduler = one_cycle_lr(\n            max_lr=lr,\n            final_div_factor=final_div_factor,\n            steps_per_epoch=steps_per_epoch,\n            epochs=main_epochs,\n        )\n    elif scheduler == \"cyclic\":\n        if lr_epoch_stair or warmup_epochs &gt; 0:\n            raise ValueError(\"Cyclic scheduler doesn't support learning rate varies with epoch and warmup_epochs &gt; 0.\")\n        num_steps = steps_per_epoch * main_epochs\n        step_size_up = int(num_steps / num_cycles / 2)\n        main_lr_scheduler = cyclic_lr(\n            base_lr=min_lr,\n            max_lr=lr,\n            step_size_up=step_size_up,\n            steps_per_epoch=steps_per_epoch,\n            epochs=main_epochs,\n        )\n    elif scheduler == \"exponential_decay\":\n        exponential_func = exponential_lr if lr_epoch_stair else exponential_refined_lr\n        main_lr_scheduler = exponential_func(\n            gamma=decay_rate, lr=lr, steps_per_epoch=steps_per_epoch, epochs=main_epochs\n        )\n    elif scheduler == \"polynomial_decay\":\n        polynomial_func = polynomial_lr if lr_epoch_stair else polynomial_refined_lr\n        main_lr_scheduler = polynomial_func(\n            total_iters=main_epochs, power=decay_rate, lr=lr, steps_per_epoch=steps_per_epoch, epochs=main_epochs\n        )\n    elif scheduler == \"step_decay\":\n        main_lr_scheduler = step_lr(\n            step_size=decay_epochs, gamma=decay_rate, lr=lr, steps_per_epoch=steps_per_epoch, epochs=main_epochs\n        )\n    elif scheduler == \"multi_step_decay\":\n        main_lr_scheduler = multi_step_lr(\n            milestones=milestones, gamma=decay_rate, lr=lr, steps_per_epoch=steps_per_epoch, epochs=main_epochs\n        )\n    elif scheduler == \"constant\":\n        main_lr_scheduler = [lr for _ in range(steps_per_epoch * main_epochs)]\n    else:\n        raise ValueError(f\"Invalid scheduler: {scheduler}\")\n\n    # combine\n    lr_scheduler = warmup_lr_scheduler + main_lr_scheduler\n\n    return lr_scheduler\n</code></pre>"},{"location":"reference/scheduler/#mindcv.scheduler.dynamic_lr","title":"<code>mindcv.scheduler.dynamic_lr</code>","text":"<p>Meta learning rate scheduler.</p> <p>This module implements exactly the same learning rate scheduler as native PyTorch, see <code>\"torch.optim.lr_scheduler\" &lt;https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate&gt;</code>_. At present, only <code>constant_lr</code>, <code>linear_lr</code>, <code>polynomial_lr</code>, <code>exponential_lr</code>, <code>step_lr</code>, <code>multi_step_lr</code>, <code>cosine_annealing_lr</code>, <code>cosine_annealing_warm_restarts_lr</code>, <code>one_cycle_lr</code>, <code>cyclic_lr</code> are implemented. The number, name and usage of the Positional Arguments are exactly the same as those of native PyTorch.</p> <p>However, due to the constraint of having to explicitly return the learning rate at each step, we have to introduce additional Keyword Arguments. There are only three Keyword Arguments introduced, namely <code>lr</code>, <code>steps_per_epoch</code> and <code>epochs</code>, explained as follows: <code>lr</code>: the basic learning rate when creating optim in torch. <code>steps_per_epoch</code>: the number of steps(iterations) of each epoch. <code>epochs</code>: the number of epoch. It and <code>steps_per_epoch</code> determine the length of the returned lrs.</p> <p>In all schedulers, <code>one_cycle_lr</code> and <code>cyclic_lr</code> only need two Keyword Arguments except <code>lr</code>, since when creating optim in torch, <code>lr</code> argument will have no effect if using the two schedulers above.</p> <p>Since most scheduler in PyTorch are coarse-grained, that is the learning rate is constant within a single epoch. For non-stepwise scheduler, we introduce several fine-grained variation, that is the learning rate is also changed within a single epoch. The function name of these variants have the <code>refined</code> keyword. The implemented fine-grained variation are list as follows: <code>linear_refined_lr</code>, <code>polynomial_refined_lr</code>, etc.</p>"},{"location":"reference/scheduler/#mindcv.scheduler.dynamic_lr.cosine_decay_lr","title":"<code>mindcv.scheduler.dynamic_lr.cosine_decay_lr(decay_epochs, eta_min, *, eta_max, steps_per_epoch, epochs, num_cycles=1, cycle_decay=1.0)</code>","text":"<p>update every epoch</p> Source code in <code>mindcv/scheduler/dynamic_lr.py</code> <pre><code>def cosine_decay_lr(decay_epochs, eta_min, *, eta_max, steps_per_epoch, epochs, num_cycles=1, cycle_decay=1.0):\n    \"\"\"update every epoch\"\"\"\n    tot_steps = steps_per_epoch * epochs\n    lrs = []\n\n    for c in range(num_cycles):\n        lr_max = eta_max * (cycle_decay**c)\n        delta = 0.5 * (lr_max - eta_min)\n        for i in range(steps_per_epoch * decay_epochs):\n            t_cur = math.floor(i / steps_per_epoch)\n            t_cur = min(t_cur, decay_epochs)\n            lr_cur = eta_min + delta * (1.0 + math.cos(math.pi * t_cur / decay_epochs))\n            if len(lrs) &lt; tot_steps:\n                lrs.append(lr_cur)\n            else:\n                break\n\n    if epochs &gt; num_cycles * decay_epochs:\n        for i in range((epochs - (num_cycles * decay_epochs)) * steps_per_epoch):\n            lrs.append(eta_min)\n\n    return lrs\n</code></pre>"},{"location":"reference/scheduler/#mindcv.scheduler.dynamic_lr.cosine_decay_refined_lr","title":"<code>mindcv.scheduler.dynamic_lr.cosine_decay_refined_lr(decay_epochs, eta_min, *, eta_max, steps_per_epoch, epochs, num_cycles=1, cycle_decay=1.0)</code>","text":"<p>update every step</p> Source code in <code>mindcv/scheduler/dynamic_lr.py</code> <pre><code>def cosine_decay_refined_lr(decay_epochs, eta_min, *, eta_max, steps_per_epoch, epochs, num_cycles=1, cycle_decay=1.0):\n    \"\"\"update every step\"\"\"\n    tot_steps = steps_per_epoch * epochs\n    lrs = []\n\n    for c in range(num_cycles):\n        lr_max = eta_max * (cycle_decay**c)\n        delta = 0.5 * (lr_max - eta_min)\n        for i in range(steps_per_epoch * decay_epochs):\n            t_cur = i / steps_per_epoch\n            t_cur = min(t_cur, decay_epochs)\n            lr_cur = eta_min + delta * (1.0 + math.cos(math.pi * t_cur / decay_epochs))\n            if len(lrs) &lt; tot_steps:\n                lrs.append(lr_cur)\n            else:\n                break\n\n    if epochs &gt; num_cycles * decay_epochs:\n        for i in range((epochs - (num_cycles * decay_epochs)) * steps_per_epoch):\n            lrs.append(eta_min)\n\n    return lrs\n</code></pre>"},{"location":"reference/scheduler/#mindcv.scheduler.dynamic_lr.cyclic_lr","title":"<code>mindcv.scheduler.dynamic_lr.cyclic_lr(base_lr, max_lr, step_size_up=2000, step_size_down=None, mode='triangular', gamma=1.0, scale_fn=None, scale_mode='cycle', *, steps_per_epoch, epochs)</code>","text":"<p>Cyclic learning rate scheduler based on '\"Cyclical Learning Rates for Training Neural Networks\" https://arxiv.org/abs/1708.07120'</p> PARAMETER DESCRIPTION <code>base_lr</code> <p>Lower learning rate boundaries in each cycle.</p> <p> TYPE: <code>float</code> </p> <code>max_lr</code> <p>Upper learning rate boundaries in each cycle.</p> <p> TYPE: <code>float</code> </p> <code>step_size_up</code> <p>Number of steps in the increasing half in each cycle. Default: 2000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>2000</code> </p> <code>step_size_down</code> <p>Number of steps in the increasing half in each cycle. If step_size_down is None, it's set to step_size_up. Default: None.</p> <p> DEFAULT: <code>None</code> </p> <code>div_factor</code> <p>Initial learning rate via initial_lr = max_lr / div_factor. Default: 25.0.</p> <p> </p> <code>final_div_factor</code> <p>Minimum learning rate at the end via min_lr = initial_lr / final_div_factor. Default: 10000.0.</p> <p> </p> <code>mode</code> <p>One of {triangular, triangular2, exp_range}. If scale_fn is not None, it's set to None. Default: 'triangular'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'triangular'</code> </p> <code>gamma</code> <p>Constant in 'exp_range' calculating fuction: gamma**(cycle_iterations). Default: 1.0</p> <p> DEFAULT: <code>1.0</code> </p> <code>scale_fn</code> <p>Custom scaling policy defined by a single argument lambda function. If it's not None, 'mode' is ignored. Default: None</p> <p> DEFAULT: <code>None</code> </p> <code>scale_mode</code> <p>One of {'cycle', 'iterations'}. Determine scale_fn is evaluated on cycle number or cycle iterations. Default: 'cycle'</p> <p> DEFAULT: <code>'cycle'</code> </p> <code>steps_per_epoch</code> <p>Number of steps per epoch.</p> <p> TYPE: <code>int</code> </p> <code>epochs</code> <p>Number of total epochs.</p> <p> TYPE: <code>int</code> </p> Source code in <code>mindcv/scheduler/dynamic_lr.py</code> <pre><code>def cyclic_lr(\n    base_lr: float,\n    max_lr: float,\n    step_size_up: int = 2000,\n    step_size_down=None,\n    mode: str = \"triangular\",\n    gamma=1.0,\n    scale_fn=None,\n    scale_mode=\"cycle\",\n    *,\n    steps_per_epoch: int,\n    epochs: int,\n):\n    \"\"\"\n    Cyclic learning rate scheduler based on\n    '\"Cyclical Learning Rates for Training Neural Networks\" &lt;https://arxiv.org/abs/1708.07120&gt;'\n\n    Args:\n        base_lr: Lower learning rate boundaries in each cycle.\n        max_lr: Upper learning rate boundaries in each cycle.\n        step_size_up: Number of steps in the increasing half in each cycle. Default: 2000.\n        step_size_down: Number of steps in the increasing half in each cycle. If step_size_down\n            is None, it's set to step_size_up. Default: None.\n        div_factor: Initial learning rate via initial_lr = max_lr / div_factor.\n            Default: 25.0.\n        final_div_factor: Minimum learning rate at the end via\n            min_lr = initial_lr / final_div_factor. Default: 10000.0.\n        mode: One of {triangular, triangular2, exp_range}. If scale_fn is not None, it's set to\n            None. Default: 'triangular'.\n        gamma: Constant in 'exp_range' calculating fuction: gamma**(cycle_iterations).\n            Default: 1.0\n        scale_fn: Custom scaling policy defined by a single argument lambda function. If it's\n            not None, 'mode' is ignored. Default: None\n        scale_mode: One of {'cycle', 'iterations'}. Determine scale_fn is evaluated on cycle\n            number or cycle iterations. Default: 'cycle'\n        steps_per_epoch: Number of steps per epoch.\n        epochs: Number of total epochs.\n    \"\"\"\n\n    def _triangular_scale_fn(x):\n        return 1.0\n\n    def _triangular2_scale_fn(x):\n        return 1 / (2.0**(x - 1))\n\n    def _exp_range_scale_fn(x):\n        return gamma**x\n\n    steps = steps_per_epoch * epochs\n    step_size_up = float(step_size_up)\n    step_size_down = float(step_size_down) if step_size_down is not None else step_size_up\n    total_size = step_size_up + step_size_down\n    step_ratio = step_size_up / total_size\n    if scale_fn is None:\n        if mode == \"triangular\":\n            scale_fn = _triangular_scale_fn\n            scale_mode = \"cycle\"\n        elif mode == \"triangular2\":\n            scale_fn = _triangular2_scale_fn\n            scale_mode = \"cycle\"\n        elif mode == \"exp_range\":\n            scale_fn = _exp_range_scale_fn\n            scale_mode = \"iterations\"\n    lrs = []\n    for i in range(steps):\n        cycle = math.floor(1 + i / total_size)\n        x = 1.0 + i / total_size - cycle\n        if x &lt;= step_ratio:\n            scale_factor = x / step_ratio\n        else:\n            scale_factor = (x - 1) / (step_ratio - 1)\n        base_height = (max_lr - base_lr) * scale_factor\n        if scale_mode == \"cycle\":\n            lrs.append(base_lr + base_height * scale_fn(cycle))\n        else:\n            lrs.append(base_lr + base_height * scale_fn(i))\n    return lrs\n</code></pre>"},{"location":"reference/scheduler/#mindcv.scheduler.dynamic_lr.one_cycle_lr","title":"<code>mindcv.scheduler.dynamic_lr.one_cycle_lr(max_lr, pct_start=0.3, anneal_strategy='cos', div_factor=25.0, final_div_factor=10000.0, three_phase=False, *, steps_per_epoch, epochs)</code>","text":"<p>OneCycle learning rate scheduler based on '\"Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates\" https://arxiv.org/abs/1708.07120'</p> PARAMETER DESCRIPTION <code>max_lr</code> <p>Upper learning rate boundaries in the cycle.</p> <p> TYPE: <code>float</code> </p> <code>pct_start</code> <p>The percentage of the number of steps of increasing learning rate in the cycle. Default: 0.3.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.3</code> </p> <code>anneal_strategy</code> <p>Define the annealing strategy: \"cos\" for cosine annealing, \"linear\" for linear annealing. Default: \"cos\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'cos'</code> </p> <code>div_factor</code> <p>Initial learning rate via initial_lr = max_lr / div_factor. Default: 25.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>25.0</code> </p> <code>final_div_factor</code> <p>Minimum learning rate at the end via min_lr = initial_lr / final_div_factor. Default: 10000.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>10000.0</code> </p> <code>three_phase</code> <p>If True, learning rate will be updated by three-phase according to \"final_div_factor\". Otherwise, learning rate will be updated by two-phase. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>steps_per_epoch</code> <p>Number of steps per epoch.</p> <p> TYPE: <code>int</code> </p> <code>epochs</code> <p>Number of total epochs.</p> <p> TYPE: <code>int</code> </p> Source code in <code>mindcv/scheduler/dynamic_lr.py</code> <pre><code>def one_cycle_lr(\n    max_lr: float,\n    pct_start: float = 0.3,\n    anneal_strategy: str = \"cos\",\n    div_factor: float = 25.0,\n    final_div_factor: float = 10000.0,\n    three_phase: bool = False,\n    *,\n    steps_per_epoch: int,\n    epochs: int,\n):\n    \"\"\"\n    OneCycle learning rate scheduler based on\n    '\"Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates\"\n    &lt;https://arxiv.org/abs/1708.07120&gt;'\n\n    Args:\n        max_lr: Upper learning rate boundaries in the cycle.\n        pct_start: The percentage of the number of steps of increasing learning rate\n            in the cycle. Default: 0.3.\n        anneal_strategy: Define the annealing strategy: \"cos\" for cosine annealing,\n            \"linear\" for linear annealing. Default: \"cos\".\n        div_factor: Initial learning rate via initial_lr = max_lr / div_factor.\n            Default: 25.0.\n        final_div_factor: Minimum learning rate at the end via\n            min_lr = initial_lr / final_div_factor. Default: 10000.0.\n        three_phase: If True, learning rate will be updated by three-phase according to\n            \"final_div_factor\". Otherwise, learning rate will be updated by two-phase.\n            Default: False.\n        steps_per_epoch: Number of steps per epoch.\n        epochs: Number of total epochs.\n    \"\"\"\n\n    def _annealing_cos(start, end, pct):\n        cos_out = math.cos(math.pi * pct) + 1\n        return end + (start - end) / 2.0 * cos_out\n\n    def _annealing_linear(start, end, pct):\n        return (end - start) * pct + start\n\n    initial_lr = max_lr / div_factor\n    min_lr = initial_lr / final_div_factor\n    steps = steps_per_epoch * epochs\n    step_size_up = float(pct_start * steps) - 1\n    step_size_down = float(2 * pct_start * steps) - 2\n    step_size_end = float(steps) - 1\n    if anneal_strategy == \"cos\":\n        anneal_func = _annealing_cos\n    elif anneal_strategy == \"linear\":\n        anneal_func = _annealing_linear\n    else:\n        raise ValueError(f\"anneal_strategy must be one of 'cos' or 'linear', but got {anneal_strategy}\")\n    lrs = []\n    for i in range(steps):\n        if three_phase:\n            if i &lt;= step_size_up:\n                lrs.append(anneal_func(initial_lr, max_lr, i / step_size_up))\n            elif step_size_up &lt; i &lt;= step_size_down:\n                lrs.append(anneal_func(max_lr, initial_lr, (i - step_size_up) / (step_size_down - step_size_up)))\n            else:\n                lrs.append(anneal_func(initial_lr, min_lr, (i - step_size_down) / (step_size_end - step_size_down)))\n        else:\n            if i &lt;= step_size_up:\n                lrs.append(anneal_func(initial_lr, max_lr, i / step_size_up))\n            else:\n                lrs.append(anneal_func(max_lr, min_lr, (i - step_size_up) / (step_size_end - step_size_up)))\n    return lrs\n</code></pre>"},{"location":"reference/utils/","title":"Utility","text":""},{"location":"reference/utils/#logger","title":"Logger","text":""},{"location":"reference/utils/#mindcv.utils.logger.set_logger","title":"<code>mindcv.utils.logger.set_logger(name=None, output_dir=None, rank=0, log_level=logging.INFO, color=True)</code>","text":"<p>Initialize the logger.</p> <p>If the logger has not been initialized, this method will initialize the logger by adding one or two handlers, otherwise the initialized logger will be directly returned. During initialization, only logger of the master process is added console handler. If <code>output_dir</code> is specified, all loggers will be added file handler.</p> PARAMETER DESCRIPTION <code>name</code> <p>Logger name. Defaults to None to set up root logger.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>output_dir</code> <p>The directory to save log.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>rank</code> <p>Process rank in the distributed training. Defaults to 0.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>log_level</code> <p>Verbosity level of the logger. Defaults to <code>logging.INFO</code>.</p> <p> TYPE: <code>int</code> DEFAULT: <code>INFO</code> </p> <code>color</code> <p>If True, color the output. Defaults to True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Logger</code> <p>logging.Logger: A initialized logger.</p> Source code in <code>mindcv/utils/logger.py</code> <pre><code>def set_logger(\n    name: Optional[str] = None,\n    output_dir: Optional[str] = None,\n    rank: int = 0,\n    log_level: int = logging.INFO,\n    color: bool = True,\n) -&gt; logging.Logger:\n    \"\"\"Initialize the logger.\n\n    If the logger has not been initialized, this method will initialize the\n    logger by adding one or two handlers, otherwise the initialized logger will\n    be directly returned. During initialization, only logger of the master\n    process is added console handler. If ``output_dir`` is specified, all loggers\n    will be added file handler.\n\n    Args:\n        name: Logger name. Defaults to None to set up root logger.\n        output_dir: The directory to save log.\n        rank: Process rank in the distributed training. Defaults to 0.\n        log_level: Verbosity level of the logger. Defaults to ``logging.INFO``.\n        color: If True, color the output. Defaults to True.\n\n    Returns:\n        logging.Logger: A initialized logger.\n    \"\"\"\n    rank = 0 if rank is None else rank\n    if name in logger_initialized:\n        return logger_initialized[name]\n\n    # get root logger if name is None\n    logger = logging.getLogger(name)\n    logger.setLevel(log_level)\n    # the messages of this logger will not be propagated to its parent\n    logger.propagate = False\n\n    fmt = \"%(asctime)s %(name)s %(levelname)s - %(message)s\"\n    datefmt = \"[%Y-%m-%d %H:%M:%S]\"\n\n    # create console handler for master process\n    if rank == 0:\n        if color:\n            if has_rich:\n                console_handler = RichHandler(level=log_level, log_time_format=datefmt)\n            elif has_termcolor:\n                console_handler = logging.StreamHandler(stream=sys.stdout)\n                console_handler.setLevel(log_level)\n                console_handler.setFormatter(_ColorfulFormatter(fmt=fmt, datefmt=datefmt))\n            else:\n                raise NotImplementedError(\"If you want color, 'rich' or 'termcolor' has to be installed!\")\n        else:\n            console_handler = logging.StreamHandler(stream=sys.stdout)\n            console_handler.setLevel(log_level)\n            console_handler.setFormatter(logging.Formatter(fmt=fmt, datefmt=datefmt))\n        logger.addHandler(console_handler)\n\n    if output_dir is not None:\n        os.makedirs(output_dir, exist_ok=True)\n        file_handler = logging.FileHandler(os.path.join(output_dir, f\"rank{rank}.log\"))\n        file_handler.setLevel(log_level)\n        file_handler.setFormatter(logging.Formatter(fmt=fmt, datefmt=datefmt))\n        logger.addHandler(file_handler)\n\n    logger_initialized[name] = logger\n    return logger\n</code></pre>"},{"location":"reference/utils/#callbacks","title":"Callbacks","text":""},{"location":"reference/utils/#mindcv.utils.callbacks.StateMonitor","title":"<code>mindcv.utils.callbacks.StateMonitor</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Train loss and validation accuracy monitor, after each epoch save the best checkpoint file with the highest validation accuracy.</p> Source code in <code>mindcv/utils/callbacks.py</code> <pre><code>class StateMonitor(Callback):\n    \"\"\"\n    Train loss and validation accuracy monitor, after each epoch save the\n    best checkpoint file with the highest validation accuracy.\n    \"\"\"\n\n    def __init__(\n        self,\n        model,\n        model_name=\"\",\n        model_ema=False,\n        last_epoch=0,\n        dataset_sink_mode=True,\n        dataset_val=None,\n        metric_name=(\"accuracy\",),\n        val_interval=1,\n        val_start_epoch=1,\n        save_best_ckpt=True,\n        ckpt_save_dir=\"./\",\n        ckpt_save_interval=1,\n        ckpt_save_policy=None,\n        ckpt_keep_max=10,\n        summary_dir=\"./\",\n        log_interval=100,\n        rank_id=None,\n        device_num=None,\n    ):\n        super().__init__()\n        # model\n        self.model = model\n        self.model_name = model_name\n        self.model_ema = model_ema\n        self.last_epoch = last_epoch\n        self.dataset_sink_mode = dataset_sink_mode\n        # evaluation\n        self.dataset_val = dataset_val\n        self.metric_name = metric_name\n        self.val_interval = val_interval\n        self.val_start_epoch = val_start_epoch\n        # logging\n        self.best_res = 0\n        self.best_epoch = -1\n        self.save_best_ckpt = save_best_ckpt\n        self.ckpt_save_dir = ckpt_save_dir\n        self.ckpt_save_interval = ckpt_save_interval\n        self.ckpt_save_policy = ckpt_save_policy\n        self.ckpt_keep_max = ckpt_keep_max\n        self.ckpt_manager = CheckpointManager(ckpt_save_policy=self.ckpt_save_policy)\n        self._need_flush_from_cache = True\n        self.summary_dir = summary_dir\n        self.log_interval = log_interval\n        # system\n        self.rank_id = rank_id if rank_id is not None else 0\n        self.device_num = device_num if rank_id is not None else 1\n        if self.rank_id in [0, None]:\n            os.makedirs(ckpt_save_dir, exist_ok=True)\n            self.log_file = os.path.join(ckpt_save_dir, \"result.log\")\n            log_line = \"\".join(\n                f\"{s:&lt;20}\" for s in [\"Epoch\", \"TrainLoss\", *metric_name, \"TrainTime\", \"EvalTime\", \"TotalTime\"]\n            )\n            with open(self.log_file, \"w\", encoding=\"utf-8\") as fp:  # writing the title of result.log\n                fp.write(log_line + \"\\n\")\n        if self.device_num &gt; 1:\n            self.all_reduce = AllReduceSum()\n        # timestamp\n        self.step_ts = None\n        self.epoch_ts = None\n        self.step_time_accum = 0\n        # model_ema\n        if self.model_ema:\n            self.hyper_map = ops.HyperMap()\n            self.online_params = ParameterTuple(self.model.train_network.get_parameters())\n            self.swap_params = self.online_params.clone(\"swap\", \"zeros\")\n\n    def __enter__(self):\n        self.summary_record = SummaryRecord(self.summary_dir)\n        return self\n\n    def __exit__(self, *exc_args):\n        self.summary_record.close()\n\n    def apply_eval(self, run_context):\n        \"\"\"Model evaluation, return validation accuracy.\"\"\"\n        if self.model_ema:\n            cb_params = run_context.original_args()\n            self.hyper_map(ops.assign, self.swap_params, self.online_params)\n            ema_dict = dict()\n            net = self._get_network_from_cbp(cb_params)\n            for param in net.get_parameters():\n                if param.name.startswith(\"ema\"):\n                    new_name = param.name.split(\"ema.\")[1]\n                    ema_dict[new_name] = param.data\n            load_param_into_net(self.model.train_network.network, ema_dict)\n            res_dict = self.model.eval(self.dataset_val, dataset_sink_mode=False)\n            self.hyper_map(ops.assign, self.online_params, self.swap_params)\n        else:\n            res_dict = self.model.eval(self.dataset_val, dataset_sink_mode=False)\n        res_array = ms.Tensor(list(res_dict.values()), ms.float32)\n        if self.device_num &gt; 1:\n            res_array = self.all_reduce(res_array)\n            res_array /= self.device_num\n        res_array = res_array.asnumpy()\n        return res_array\n\n    def on_train_step_begin(self, run_context):\n        self.step_ts = time()\n\n    def on_train_epoch_begin(self, run_context):\n        self.epoch_ts = time()\n\n    def on_train_step_end(self, run_context):\n        cb_params = run_context.original_args()\n        num_epochs = cb_params.epoch_num\n        num_batches = cb_params.batch_num\n        # num_steps = num_batches * num_epochs\n        # cur_x start from 1, end at num_xs, range: [1, num_xs]\n        cur_step = cb_params.cur_step_num + self.last_epoch * num_batches\n        cur_epoch = cb_params.cur_epoch_num + self.last_epoch\n        cur_batch = (cur_step - 1) % num_batches + 1\n\n        self.step_time_accum += time() - self.step_ts\n        if cur_batch % self.log_interval == 0 or cur_batch == num_batches or cur_batch == 1:\n            lr = self._get_lr_from_cbp(cb_params)\n            loss = self._get_loss_from_cbp(cb_params)\n            _logger.info(\n                f\"Epoch: [{cur_epoch}/{num_epochs}], \"\n                f\"batch: [{cur_batch}/{num_batches}], \"\n                f\"loss: {loss.asnumpy():.6f}, \"\n                f\"lr: {lr.asnumpy():.6f}, \"\n                f\"time: {self.step_time_accum:.6f}s\"\n            )\n            self.step_time_accum = 0\n\n    def on_train_epoch_end(self, run_context):\n        \"\"\"\n        After epoch, print train loss and val accuracy,\n        save the best ckpt file with the highest validation accuracy.\n        \"\"\"\n        cb_params = run_context.original_args()\n        num_epochs = cb_params.epoch_num\n        num_batches = cb_params.batch_num\n        cur_step = cb_params.cur_step_num + self.last_epoch * num_batches\n        cur_epoch = cb_params.cur_epoch_num + self.last_epoch\n        cur_batch = (cur_step - 1) % num_batches + 1\n\n        train_time = time() - self.epoch_ts\n        loss = self._get_loss_from_cbp(cb_params)\n\n        val_time = 0\n        res = np.zeros(len(self.metric_name), dtype=np.float32)\n        # val while training if validation loader is not None\n        if (\n            self.dataset_val is not None\n            and cur_epoch &gt;= self.val_start_epoch\n            and (cur_epoch - self.val_start_epoch) % self.val_interval == 0\n        ):\n            val_time = time()\n            res = self.apply_eval(run_context)\n            val_time = time() - val_time\n            # record val acc\n            metric_str = \"Validation \"\n            for i in range(len(self.metric_name)):\n                metric_str += f\"{self.metric_name[i]}: {res[i]:.4%}, \"\n            metric_str += f\"time: {val_time:.6f}s\"\n            _logger.info(metric_str)\n            # save the best ckpt file\n            if res[0] &gt; self.best_res:\n                self.best_res = res[0]\n                self.best_epoch = cur_epoch\n                _logger.info(f\"=&gt; New best val acc: {res[0]:.4%}\")\n\n        # save checkpoint\n        if self.rank_id in [0, None]:\n            if self.save_best_ckpt and self.best_epoch == cur_epoch:  # always save ckpt if cur epoch got best acc\n                best_ckpt_save_path = os.path.join(self.ckpt_save_dir, f\"{self.model_name}_best.ckpt\")\n                save_checkpoint(cb_params.train_network, best_ckpt_save_path, async_save=True)\n            if (cur_epoch % self.ckpt_save_interval == 0) or (cur_epoch == num_epochs):\n                if self._need_flush_from_cache:\n                    self._flush_from_cache(cb_params)\n                # save optim for resume\n                optimizer = self._get_optimizer_from_cbp(cb_params)\n                optim_save_path = os.path.join(self.ckpt_save_dir, f\"optim_{self.model_name}.ckpt\")\n                save_checkpoint(optimizer, optim_save_path, async_save=True)\n                # keep checkpoint files number equal max number.\n                ckpt_save_path = os.path.join(self.ckpt_save_dir, f\"{self.model_name}-{cur_epoch}_{cur_batch}.ckpt\")\n                _logger.info(f\"Saving model to {ckpt_save_path}\")\n                self.ckpt_manager.save_ckpoint(\n                    cb_params.train_network,\n                    num_ckpt=self.ckpt_keep_max,\n                    metric=res[0],\n                    save_path=ckpt_save_path,\n                )\n\n        # logging\n        total_time = time() - self.epoch_ts\n        _logger.info(\n            f\"Total time since last epoch: {total_time:.6f}(train: {train_time:.6f}, val: {val_time:.6f})s, \"\n            f\"ETA: {(num_epochs - cur_epoch) * total_time:.6f}s\"\n        )\n        _logger.info(\"-\" * 80)\n        if self.rank_id in [0, None]:\n            log_line = \"\".join(\n                f\"{s:&lt;20}\"\n                for s in [\n                    f\"{cur_epoch}\",\n                    f\"{loss.asnumpy():.6f}\",\n                    *[f\"{i:.4%}\" for i in res],\n                    f\"{train_time:.2f}\",\n                    f\"{val_time:.2f}\",\n                    f\"{total_time:.2f}\",\n                ]\n            )\n            with open(self.log_file, \"a\", encoding=\"utf-8\") as fp:\n                fp.write(log_line + \"\\n\")\n\n        # summary\n        self.summary_record.add_value(\"scalar\", f\"train_loss_{self.rank_id}\", loss)\n        for i in range(len(res)):\n            self.summary_record.add_value(\n                \"scalar\", f\"val_{self.metric_name[i]}_{self.rank_id}\", Tensor(res[i], dtype=ms.float32)\n            )\n        self.summary_record.record(cur_step)\n\n    def on_train_end(self, run_context):\n        _logger.info(\"Finish training!\")\n        if self.dataset_val is not None:\n            _logger.info(\n                f\"The best validation {self.metric_name[0]} is: {self.best_res:.4%} at epoch {self.best_epoch}.\"\n            )\n        _logger.info(\"=\" * 80)\n\n    def _get_network_from_cbp(self, cb_params):\n        if self.dataset_sink_mode:\n            network = cb_params.train_network.network\n        else:\n            network = cb_params.train_network\n        return network\n\n    def _get_optimizer_from_cbp(self, cb_params):\n        if cb_params.optimizer is not None:\n            optimizer = cb_params.optimizer\n        elif self.dataset_sink_mode:\n            optimizer = cb_params.train_network.network.optimizer\n        else:\n            optimizer = cb_params.train_network.optimizer\n        return optimizer\n\n    def _get_lr_from_cbp(self, cb_params):\n        optimizer = self._get_optimizer_from_cbp(cb_params)\n        if optimizer.global_step &lt; 1:\n            _logger.warning(\n                \"`global_step` of optimizer is less than 1. It seems to be a overflow at the first step. \"\n                \"If you keep seeing this message, it means that the optimizer never actually called.\"\n            )\n            optim_step = Tensor((0,), ms.int32)\n        else:  # if the optimizer is successfully called, the global_step will actually be the value of next step.\n            optim_step = optimizer.global_step - 1\n        if optimizer.dynamic_lr:\n            if isinstance(optimizer.learning_rate, ms.nn.CellList):\n                # return the learning rates of the first parameter if dynamic_lr\n                lr = optimizer.learning_rate[0](optim_step)[0]\n            else:\n                lr = optimizer.learning_rate(optim_step)[0]\n        else:\n            lr = optimizer.learning_rate\n        return lr\n\n    def _get_loss_from_cbp(self, cb_params):\n        \"\"\"\n        Get loss from the network output.\n        Args:\n            cb_params (_InternalCallbackParam): Callback parameters.\n        Returns:\n            Union[Tensor, None], if parse loss success, will return a Tensor value(shape is [1]), else return None.\n        \"\"\"\n        output = cb_params.net_outputs\n        if output is None:\n            _logger.warning(\"Can not find any output by this network, so SummaryCollector will not collect loss.\")\n            return None\n\n        if isinstance(output, (int, float, Tensor)):\n            loss = output\n        elif isinstance(output, (list, tuple)) and output:\n            # If the output is a list, since the default network returns loss first,\n            # we assume that the first one is loss.\n            loss = output[0]\n        else:\n            _logger.warning(\n                \"The output type could not be identified, expect type is one of \"\n                \"[int, float, Tensor, list, tuple], so no loss was recorded in SummaryCollector.\"\n            )\n            return None\n\n        if not isinstance(loss, Tensor):\n            loss = Tensor(loss)\n\n        loss = Tensor(np.mean(loss.asnumpy()))\n        return loss\n\n    def _flush_from_cache(self, cb_params):\n        \"\"\"Flush cache data to host if tensor is cache enable.\"\"\"\n        has_cache_params = False\n        params = cb_params.train_network.get_parameters()\n        for param in params:\n            if param.cache_enable:\n                has_cache_params = True\n                Tensor(param).flush_from_cache()\n        if not has_cache_params:\n            self._need_flush_from_cache = False\n</code></pre>"},{"location":"reference/utils/#mindcv.utils.callbacks.StateMonitor.apply_eval","title":"<code>mindcv.utils.callbacks.StateMonitor.apply_eval(run_context)</code>","text":"<p>Model evaluation, return validation accuracy.</p> Source code in <code>mindcv/utils/callbacks.py</code> <pre><code>def apply_eval(self, run_context):\n    \"\"\"Model evaluation, return validation accuracy.\"\"\"\n    if self.model_ema:\n        cb_params = run_context.original_args()\n        self.hyper_map(ops.assign, self.swap_params, self.online_params)\n        ema_dict = dict()\n        net = self._get_network_from_cbp(cb_params)\n        for param in net.get_parameters():\n            if param.name.startswith(\"ema\"):\n                new_name = param.name.split(\"ema.\")[1]\n                ema_dict[new_name] = param.data\n        load_param_into_net(self.model.train_network.network, ema_dict)\n        res_dict = self.model.eval(self.dataset_val, dataset_sink_mode=False)\n        self.hyper_map(ops.assign, self.online_params, self.swap_params)\n    else:\n        res_dict = self.model.eval(self.dataset_val, dataset_sink_mode=False)\n    res_array = ms.Tensor(list(res_dict.values()), ms.float32)\n    if self.device_num &gt; 1:\n        res_array = self.all_reduce(res_array)\n        res_array /= self.device_num\n    res_array = res_array.asnumpy()\n    return res_array\n</code></pre>"},{"location":"reference/utils/#mindcv.utils.callbacks.StateMonitor.on_train_epoch_end","title":"<code>mindcv.utils.callbacks.StateMonitor.on_train_epoch_end(run_context)</code>","text":"<p>After epoch, print train loss and val accuracy, save the best ckpt file with the highest validation accuracy.</p> Source code in <code>mindcv/utils/callbacks.py</code> <pre><code>def on_train_epoch_end(self, run_context):\n    \"\"\"\n    After epoch, print train loss and val accuracy,\n    save the best ckpt file with the highest validation accuracy.\n    \"\"\"\n    cb_params = run_context.original_args()\n    num_epochs = cb_params.epoch_num\n    num_batches = cb_params.batch_num\n    cur_step = cb_params.cur_step_num + self.last_epoch * num_batches\n    cur_epoch = cb_params.cur_epoch_num + self.last_epoch\n    cur_batch = (cur_step - 1) % num_batches + 1\n\n    train_time = time() - self.epoch_ts\n    loss = self._get_loss_from_cbp(cb_params)\n\n    val_time = 0\n    res = np.zeros(len(self.metric_name), dtype=np.float32)\n    # val while training if validation loader is not None\n    if (\n        self.dataset_val is not None\n        and cur_epoch &gt;= self.val_start_epoch\n        and (cur_epoch - self.val_start_epoch) % self.val_interval == 0\n    ):\n        val_time = time()\n        res = self.apply_eval(run_context)\n        val_time = time() - val_time\n        # record val acc\n        metric_str = \"Validation \"\n        for i in range(len(self.metric_name)):\n            metric_str += f\"{self.metric_name[i]}: {res[i]:.4%}, \"\n        metric_str += f\"time: {val_time:.6f}s\"\n        _logger.info(metric_str)\n        # save the best ckpt file\n        if res[0] &gt; self.best_res:\n            self.best_res = res[0]\n            self.best_epoch = cur_epoch\n            _logger.info(f\"=&gt; New best val acc: {res[0]:.4%}\")\n\n    # save checkpoint\n    if self.rank_id in [0, None]:\n        if self.save_best_ckpt and self.best_epoch == cur_epoch:  # always save ckpt if cur epoch got best acc\n            best_ckpt_save_path = os.path.join(self.ckpt_save_dir, f\"{self.model_name}_best.ckpt\")\n            save_checkpoint(cb_params.train_network, best_ckpt_save_path, async_save=True)\n        if (cur_epoch % self.ckpt_save_interval == 0) or (cur_epoch == num_epochs):\n            if self._need_flush_from_cache:\n                self._flush_from_cache(cb_params)\n            # save optim for resume\n            optimizer = self._get_optimizer_from_cbp(cb_params)\n            optim_save_path = os.path.join(self.ckpt_save_dir, f\"optim_{self.model_name}.ckpt\")\n            save_checkpoint(optimizer, optim_save_path, async_save=True)\n            # keep checkpoint files number equal max number.\n            ckpt_save_path = os.path.join(self.ckpt_save_dir, f\"{self.model_name}-{cur_epoch}_{cur_batch}.ckpt\")\n            _logger.info(f\"Saving model to {ckpt_save_path}\")\n            self.ckpt_manager.save_ckpoint(\n                cb_params.train_network,\n                num_ckpt=self.ckpt_keep_max,\n                metric=res[0],\n                save_path=ckpt_save_path,\n            )\n\n    # logging\n    total_time = time() - self.epoch_ts\n    _logger.info(\n        f\"Total time since last epoch: {total_time:.6f}(train: {train_time:.6f}, val: {val_time:.6f})s, \"\n        f\"ETA: {(num_epochs - cur_epoch) * total_time:.6f}s\"\n    )\n    _logger.info(\"-\" * 80)\n    if self.rank_id in [0, None]:\n        log_line = \"\".join(\n            f\"{s:&lt;20}\"\n            for s in [\n                f\"{cur_epoch}\",\n                f\"{loss.asnumpy():.6f}\",\n                *[f\"{i:.4%}\" for i in res],\n                f\"{train_time:.2f}\",\n                f\"{val_time:.2f}\",\n                f\"{total_time:.2f}\",\n            ]\n        )\n        with open(self.log_file, \"a\", encoding=\"utf-8\") as fp:\n            fp.write(log_line + \"\\n\")\n\n    # summary\n    self.summary_record.add_value(\"scalar\", f\"train_loss_{self.rank_id}\", loss)\n    for i in range(len(res)):\n        self.summary_record.add_value(\n            \"scalar\", f\"val_{self.metric_name[i]}_{self.rank_id}\", Tensor(res[i], dtype=ms.float32)\n        )\n    self.summary_record.record(cur_step)\n</code></pre>"},{"location":"reference/utils/#mindcv.utils.callbacks.ValCallback","title":"<code>mindcv.utils.callbacks.ValCallback</code>","text":"<p>               Bases: <code>Callback</code></p> Source code in <code>mindcv/utils/callbacks.py</code> <pre><code>class ValCallback(Callback):\n    def __init__(self, log_interval=100):\n        super().__init__()\n        self.log_interval = log_interval\n        self.ts = time()\n\n    def on_eval_step_end(self, run_context):\n        cb_params = run_context.original_args()\n        num_batches = cb_params.batch_num\n        cur_step = cb_params.cur_step_num\n\n        if cur_step % self.log_interval == 0 or cur_step == num_batches:\n            print(f\"batch: {cur_step}/{num_batches}, time: {time() - self.ts:.6f}s\")\n            self.ts = time()\n</code></pre>"},{"location":"reference/utils/#train-step","title":"Train Step","text":""},{"location":"reference/utils/#mindcv.utils.train_step.TrainStep","title":"<code>mindcv.utils.train_step.TrainStep</code>","text":"<p>               Bases: <code>TrainOneStepWithLossScaleCell</code></p> <p>Training step with loss scale.</p> The customized trainOneStepCell also supported following algorithms <ul> <li>Exponential Moving Average (EMA)</li> <li>Gradient Clipping</li> <li>Gradient Accumulation</li> </ul> Source code in <code>mindcv/utils/train_step.py</code> <pre><code>class TrainStep(nn.TrainOneStepWithLossScaleCell):\n    \"\"\"Training step with loss scale.\n\n    The customized trainOneStepCell also supported following algorithms:\n        * Exponential Moving Average (EMA)\n        * Gradient Clipping\n        * Gradient Accumulation\n    \"\"\"\n\n    def __init__(\n        self,\n        network,\n        optimizer,\n        scale_sense=1.0,\n        ema=False,\n        ema_decay=0.9999,\n        clip_grad=False,\n        clip_value=15.0,\n        gradient_accumulation_steps=1,\n    ):\n        super(TrainStep, self).__init__(network, optimizer, scale_sense)\n        self.ema = ema\n        self.ema_decay = ema_decay\n        self.updates = Parameter(Tensor(0.0, ms.float32))\n        self.clip_grad = clip_grad\n        self.clip_value = clip_value\n        if self.ema:\n            self.weights_all = ms.ParameterTuple(list(network.get_parameters()))\n            self.ema_weight = self.weights_all.clone(\"ema\", init=\"same\")\n\n        self.accumulate_grad = gradient_accumulation_steps &gt; 1\n        if self.accumulate_grad:\n            self.gradient_accumulation = GradientAccumulation(gradient_accumulation_steps, optimizer, self.grad_reducer)\n\n    def ema_update(self):\n        self.updates += 1\n        # ema factor is corrected by (1 - exp(-t/T)), where `t` means time and `T` means temperature.\n        ema_decay = self.ema_decay * (1 - F.exp(-self.updates / 2000))\n        # update trainable parameters\n        success = self.hyper_map(F.partial(_ema_op, ema_decay), self.ema_weight, self.weights_all)\n        return success\n\n    def construct(self, *inputs):\n        weights = self.weights\n        loss = self.network(*inputs)\n        scaling_sens = self.scale_sense\n\n        status, scaling_sens = self.start_overflow_check(loss, scaling_sens)\n\n        scaling_sens_filled = ops.ones_like(loss) * F.cast(scaling_sens, F.dtype(loss))\n        grads = self.grad(self.network, weights)(*inputs, scaling_sens_filled)\n        grads = self.hyper_map(F.partial(_grad_scale, scaling_sens), grads)\n\n        # todo: When to clip grad? Do we need to clip grad after grad reduction? What if grad accumulation is needed?\n        if self.clip_grad:\n            grads = ops.clip_by_global_norm(grads, clip_norm=self.clip_value)\n\n        if self.loss_scaling_manager:  # scale_sense = update_cell: Cell --&gt; TrainOneStepWithLossScaleCell.construct\n            if self.accumulate_grad:\n                # todo: GradientAccumulation only call grad_reducer at the step where the accumulation is completed.\n                #  So checking the overflow status is after gradient reduction, is this correct?\n                # get the overflow buffer\n                cond = self.get_overflow_status(status, grads)\n                overflow = self.process_loss_scale(cond)\n                # if there is no overflow, do optimize\n                if not overflow:\n                    loss = self.gradient_accumulation(loss, grads)\n                    if self.ema:\n                        loss = F.depend(loss, self.ema_update())\n            else:\n                # apply grad reducer on grads\n                grads = self.grad_reducer(grads)\n                # get the overflow buffer\n                cond = self.get_overflow_status(status, grads)\n                overflow = self.process_loss_scale(cond)\n                # if there is no overflow, do optimize\n                if not overflow:\n                    loss = F.depend(loss, self.optimizer(grads))\n                    if self.ema:\n                        loss = F.depend(loss, self.ema_update())\n        else:  # scale_sense = loss_scale: Tensor --&gt; TrainOneStepCell.construct\n            if self.accumulate_grad:\n                loss = self.gradient_accumulation(loss, grads)\n            else:\n                grads = self.grad_reducer(grads)\n                loss = F.depend(loss, self.optimizer(grads))\n\n            if self.ema:\n                loss = F.depend(loss, self.ema_update())\n\n        return loss\n</code></pre>"},{"location":"reference/utils/#trainer-factory","title":"Trainer Factory","text":""},{"location":"reference/utils/#mindcv.utils.trainer_factory.create_trainer","title":"<code>mindcv.utils.trainer_factory.create_trainer(network, loss, optimizer, metrics, amp_level, amp_cast_list, loss_scale_type, loss_scale=1.0, drop_overflow_update=False, ema=False, ema_decay=0.9999, clip_grad=False, clip_value=15.0, gradient_accumulation_steps=1)</code>","text":"<p>Create Trainer.</p> PARAMETER DESCRIPTION <code>network</code> <p>The backbone network to train, evaluate or predict.</p> <p> TYPE: <code>Cell</code> </p> <code>loss</code> <p>The function of calculating loss.</p> <p> TYPE: <code>Cell</code> </p> <code>optimizer</code> <p>The optimizer for training.</p> <p> TYPE: <code>Cell</code> </p> <code>metrics</code> <p>The metrics for model evaluation.</p> <p> TYPE: <code>Union[dict, set]</code> </p> <code>amp_level</code> <p>The level of auto mixing precision training.</p> <p> TYPE: <code>str</code> </p> <code>amp_cast_list</code> <p>At the cell level, custom casting the cell to FP16.</p> <p> TYPE: <code>str</code> </p> <code>loss_scale_type</code> <p>The type of loss scale.</p> <p> TYPE: <code>str</code> </p> <code>loss_scale</code> <p>The value of loss scale.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>drop_overflow_update</code> <p>Whether to execute optimizer if there is an overflow.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ema</code> <p>Whether to use exponential moving average of model weights.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ema_decay</code> <p>Decay factor for model weights moving average.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.9999</code> </p> <code>clip_grad</code> <p>whether to gradient clip.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>clip_value</code> <p>The value at which to clip gradients.</p> <p> TYPE: <code>float</code> DEFAULT: <code>15.0</code> </p> <code>gradient_accumulation_steps</code> <p>Accumulate the gradients of n batches before update.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> RETURNS DESCRIPTION <p>mindspore.Model</p> Source code in <code>mindcv/utils/trainer_factory.py</code> <pre><code>def create_trainer(\n    network: nn.Cell,\n    loss: nn.Cell,\n    optimizer: nn.Cell,\n    metrics: Union[dict, set],\n    amp_level: str,\n    amp_cast_list: str,\n    loss_scale_type: str,\n    loss_scale: float = 1.0,\n    drop_overflow_update: bool = False,\n    ema: bool = False,\n    ema_decay: float = 0.9999,\n    clip_grad: bool = False,\n    clip_value: float = 15.0,\n    gradient_accumulation_steps: int = 1,\n):\n    \"\"\"Create Trainer.\n\n    Args:\n        network: The backbone network to train, evaluate or predict.\n        loss: The function of calculating loss.\n        optimizer: The optimizer for training.\n        metrics: The metrics for model evaluation.\n        amp_level: The level of auto mixing precision training.\n        amp_cast_list: At the cell level, custom casting the cell to FP16.\n        loss_scale_type: The type of loss scale.\n        loss_scale: The value of loss scale.\n        drop_overflow_update: Whether to execute optimizer if there is an overflow.\n        ema: Whether to use exponential moving average of model weights.\n        ema_decay: Decay factor for model weights moving average.\n        clip_grad: whether to gradient clip.\n        clip_value: The value at which to clip gradients.\n        gradient_accumulation_steps: Accumulate the gradients of n batches before update.\n\n    Returns:\n        mindspore.Model\n\n    \"\"\"\n    if loss_scale &lt; 1.0:\n        raise ValueError(\"Loss scale cannot be less than 1.0!\")\n\n    if drop_overflow_update is False and loss_scale_type.lower() == \"dynamic\":\n        raise ValueError(\"DynamicLossScale ALWAYS drop overflow!\")\n\n    if gradient_accumulation_steps &lt; 1:\n        raise ValueError(\"`gradient_accumulation_steps` must be &gt;= 1!\")\n\n    if not require_customized_train_step(ema, clip_grad, gradient_accumulation_steps, amp_cast_list):\n        mindspore_kwargs = dict(\n            network=network,\n            loss_fn=loss,\n            optimizer=optimizer,\n            metrics=metrics,\n            amp_level=amp_level,\n        )\n        if loss_scale_type.lower() == \"fixed\":\n            mindspore_kwargs[\"loss_scale_manager\"] = FixedLossScaleManager(\n                loss_scale=loss_scale, drop_overflow_update=drop_overflow_update\n            )\n        elif loss_scale_type.lower() == \"dynamic\":\n            mindspore_kwargs[\"loss_scale_manager\"] = DynamicLossScaleManager(\n                init_loss_scale=loss_scale, scale_factor=2, scale_window=2000\n            )\n        elif loss_scale_type.lower() == \"auto\":\n            # We don't explicitly construct LossScaleManager\n            _logger.warning(\n                \"You are using AUTO loss scale, which means the LossScaleManager isn't explicitly pass in \"\n                \"when creating a mindspore.Model instance. \"\n                \"NOTE: mindspore.Model may use LossScaleManager silently. See mindspore.train.amp for details.\"\n            )\n        else:\n            raise ValueError(f\"Loss scale type only support ['fixed', 'dynamic', 'auto'], but got{loss_scale_type}.\")\n        model = Model(**mindspore_kwargs)\n    else:  # require customized train step\n        eval_network = nn.WithEvalCell(network, loss, amp_level in [\"O2\", \"O3\", \"auto\"])\n        auto_mixed_precision(network, amp_level, amp_cast_list)\n        net_with_loss = add_loss_network(network, loss, amp_level)\n        train_step_kwargs = dict(\n            network=net_with_loss,\n            optimizer=optimizer,\n            ema=ema,\n            ema_decay=ema_decay,\n            clip_grad=clip_grad,\n            clip_value=clip_value,\n            gradient_accumulation_steps=gradient_accumulation_steps,\n        )\n        if loss_scale_type.lower() == \"fixed\":\n            loss_scale_manager = FixedLossScaleManager(loss_scale=loss_scale, drop_overflow_update=drop_overflow_update)\n        elif loss_scale_type.lower() == \"dynamic\":\n            loss_scale_manager = DynamicLossScaleManager(init_loss_scale=loss_scale, scale_factor=2, scale_window=2000)\n        else:\n            raise ValueError(f\"Loss scale type only support ['fixed', 'dynamic'], but got{loss_scale_type}.\")\n        update_cell = loss_scale_manager.get_update_cell()\n        # 1. loss_scale_type=\"fixed\", drop_overflow_update=False\n        # --&gt; update_cell=None, TrainStep=TrainOneStepCell(scale_sense=loss_scale)\n        # 2. loss_scale_type: fixed, drop_overflow_update: True\n        # --&gt; update_cell=FixedLossScaleUpdateCell, TrainStep=TrainOneStepWithLossScaleCell(scale_sense=update_cell)\n        # 3. loss_scale_type: dynamic, drop_overflow_update: True\n        # --&gt; update_cell=DynamicLossScaleUpdateCell, TrainStep=TrainOneStepWithLossScaleCell(scale_sense=update_cell)\n        if update_cell is None:\n            train_step_kwargs[\"scale_sense\"] = Tensor(loss_scale, dtype=ms.float32)\n        else:\n            if not context.get_context(\"enable_ge\") and context.get_context(\"device_target\") == \"CPU\":\n                raise ValueError(\n                    \"Only `loss_scale_type` is `fixed` and `drop_overflow_update` is `False`\"\n                    \"are supported on device `CPU`.\"\n                )\n            train_step_kwargs[\"scale_sense\"] = update_cell\n        train_step_cell = TrainStep(**train_step_kwargs).set_train()\n        model = Model(train_step_cell, eval_network=eval_network, metrics=metrics, eval_indexes=[0, 1, 2])\n        # todo: do we need to set model._loss_scale_manager\n    return model\n</code></pre>"},{"location":"tutorials/configuration/","title":"Configuration","text":"<p>MindCV can parse the yaml file of the model through the argparse library and PyYAML library to configure parameters. Let's use squeezenet_1.0 model as an example to explain how to configure the corresponding parameters.</p>"},{"location":"tutorials/configuration/#basic-environment","title":"Basic Environment","text":"<ol> <li>Parameter description</li> </ol> <ul> <li> <p>mode: Use graph mode (0) or pynative mode (1).</p> </li> <li> <p>distribute: Whether to use distributed.</p> </li> </ul> <ol> <li> <p>Sample yaml file</p> <pre><code>mode: 0\ndistribute: True\n...\n</code></pre> </li> <li> <p>Parse parameter setting</p> <pre><code>python train.py --mode 0 --distribute False ...\n</code></pre> </li> <li> <p>Corresponding code example</p> <p><code>args.mode</code> represents the parameter <code>mode</code>, <code>args.distribute</code> represents the parameter <code>distribute</code>.</p> <pre><code>def train(args):\n    ms.set_context(mode=args.mode)\n\n    if args.distribute:\n        init()\n        device_num = get_group_size()\n        rank_id = get_rank()\n        ms.set_auto_parallel_context(device_num=device_num,\n                                     parallel_mode='data_parallel',\n                                     gradients_mean=True)\n    else:\n        device_num = None\n        rank_id = None\n    ...\n</code></pre> </li> </ol>"},{"location":"tutorials/configuration/#dataset","title":"Dataset","text":"<ol> <li>Parameter description</li> </ol> <ul> <li> <p>dataset: dataset name.</p> </li> <li> <p>data_dir: Path of dataset file.</p> </li> <li> <p>shuffle: whether to shuffle the dataset.</p> </li> <li> <p>dataset_download: whether to download the dataset.</p> </li> <li> <p>batch_size: The number of rows in each batch.</p> </li> <li> <p>drop_remainder: Determines whether to drop the last block whose data row number is less than the batch size.</p> </li> <li> <p>num_parallel_workers: Number of workers(threads) to process the dataset in parallel.</p> </li> </ul> <ol> <li> <p>Sample yaml file</p> <pre><code>dataset: 'imagenet'\ndata_dir: './imagenet2012'\nshuffle: True\ndataset_download: False\nbatch_size: 32\ndrop_remainder: True\nnum_parallel_workers: 8\n...\n</code></pre> </li> <li> <p>Parse parameter setting</p> <pre><code>python train.py ... --dataset imagenet --data_dir ./imagenet2012 --shuffle True \\\n    --dataset_download False --batch_size 32 --drop_remainder True \\\n    --num_parallel_workers 8 ...\n</code></pre> </li> <li> <p>Corresponding code example</p> <pre><code>def train(args):\n    ...\n    dataset_train = create_dataset(\n        name=args.dataset,\n        root=args.data_dir,\n        split='train',\n        shuffle=args.shuffle,\n        num_samples=args.num_samples,\n        num_shards=device_num,\n        shard_id=rank_id,\n        num_parallel_workers=args.num_parallel_workers,\n        download=args.dataset_download,\n        num_aug_repeats=args.aug_repeats)\n\n    ...\n    target_transform = transforms.OneHot(num_classes) if args.loss == 'BCE' else None\n\n    loader_train = create_loader(\n        dataset=dataset_train,\n        batch_size=args.batch_size,\n        drop_remainder=args.drop_remainder,\n        is_training=True,\n        mixup=args.mixup,\n        cutmix=args.cutmix,\n        cutmix_prob=args.cutmix_prob,\n        num_classes=args.num_classes,\n        transform=transform_list,\n        target_transform=target_transform,\n        num_parallel_workers=args.num_parallel_workers,\n    )\n    ...\n</code></pre> </li> </ol>"},{"location":"tutorials/configuration/#data-augmentation","title":"Data Augmentation","text":"<ol> <li>Parameter description</li> </ol> <ul> <li> <p>image_resize: the image size after resizing for adapting to the network.</p> </li> <li> <p>scale: random resize scale.</p> </li> <li> <p>ratio: random resize aspect ratio.</p> </li> <li> <p>hfilp: horizontal flip training aug probability.</p> </li> <li> <p>interpolation: image interpolation mode for resize operator.</p> </li> <li> <p>crop_pct: input image center crop percent.</p> </li> <li> <p>color_jitter: color jitter factor.</p> </li> <li> <p>re_prob: the probability of performing erasing.</p> </li> </ul> <ol> <li> <p>Sample yaml file</p> <pre><code>image_resize: 224\nscale: [0.08, 1.0]\nratio: [0.75, 1.333]\nhflip: 0.5\ninterpolation: 'bilinear'\ncrop_pct: 0.875\ncolor_jitter: [0.4, 0.4, 0.4]\nre_prob: 0.5\n...\n</code></pre> </li> <li> <p>Parse parameter setting</p> <pre><code>python train.py ... --image_resize 224 --scale [0.08, 1.0] --ratio [0.75, 1.333] \\\n    --hflip 0.5 --interpolation \"bilinear\" --crop_pct 0.875 \\\n    --color_jitter [0.4, 0.4, 0.4] --re_prob 0.5 ...\n</code></pre> </li> <li> <p>Corresponding code example</p> <pre><code>def train(args):\n    ...\n    transform_list = create_transforms(\n        dataset_name=args.dataset,\n        is_training=True,\n        image_resize=args.image_resize,\n        scale=args.scale,\n        ratio=args.ratio,\n        hflip=args.hflip,\n        vflip=args.vflip,\n        color_jitter=args.color_jitter,\n        interpolation=args.interpolation,\n        auto_augment=args.auto_augment,\n        mean=args.mean,\n        std=args.std,\n        re_prob=args.re_prob,\n        re_scale=args.re_scale,\n        re_ratio=args.re_ratio,\n        re_value=args.re_value,\n        re_max_attempts=args.re_max_attempts\n    )\n    ...\n</code></pre> </li> </ol>"},{"location":"tutorials/configuration/#model","title":"Model","text":"<ol> <li>Parameter description</li> </ol> <ul> <li> <p>model: model name.</p> </li> <li> <p>num_classes: number of label classes.</p> </li> <li> <p>pretrained: whether load pretrained model.</p> </li> <li> <p>ckpt_path: initialize model from this checkpoint.</p> </li> <li> <p>keep_checkpoint_max: max number of checkpoint files.</p> </li> <li> <p>ckpt_save_dir: the path of checkpoint.</p> </li> <li> <p>epoch_size: train epoch size.</p> </li> <li> <p>dataset_sink_mode: the dataset sink mode.</p> </li> <li> <p>amp_level: auto mixed precision level for saving memory and acceleration.</p> </li> </ul> <ol> <li> <p>Sample yaml file</p> <pre><code>model: 'squeezenet1_0'\nnum_classes: 1000\npretrained: False\nckpt_path: './squeezenet1_0_gpu.ckpt'\nkeep_checkpoint_max: 10\nckpt_save_dir: './ckpt/'\nepoch_size: 200\ndataset_sink_mode: True\namp_level: 'O0'\n...\n</code></pre> </li> <li> <p>Parse parameter setting</p> <pre><code>python train.py ... --model squeezenet1_0 --num_classes 1000 --pretrained False \\\n    --ckpt_path ./squeezenet1_0_gpu.ckpt --keep_checkpoint_max 10 \\\n    --ckpt_save_path ./ckpt/ --epoch_size 200 --dataset_sink_mode True \\\n    --amp_level O0 ...\n</code></pre> </li> <li> <p>Corresponding code example</p> <pre><code>def train(args):\n    ...\n    network = create_model(model_name=args.model,\n        num_classes=args.num_classes,\n        in_channels=args.in_channels,\n        drop_rate=args.drop_rate,\n        drop_path_rate=args.drop_path_rate,\n        pretrained=args.pretrained,\n        checkpoint_path=args.ckpt_path,\n        ema=args.ema\n    )\n    ...\n</code></pre> </li> </ol>"},{"location":"tutorials/configuration/#loss-function","title":"Loss Function","text":"<ol> <li>Parameter description</li> </ol> <ul> <li> <p>loss: name of loss function, BCE (BinaryCrossEntropy) or CE (CrossEntropy).</p> </li> <li> <p>label_smoothing: use label smoothing.</p> </li> </ul> <ol> <li> <p>Sample yaml file</p> <pre><code>loss: 'CE'\nlabel_smoothing: 0.1\n...\n</code></pre> </li> <li> <p>Parse parameter setting</p> <pre><code>python train.py ... --loss CE --label_smoothing 0.1 ...\n</code></pre> </li> <li> <p>Corresponding code example</p> <pre><code>def train(args):\n    ...\n    loss = create_loss(name=args.loss,\n        reduction=args.reduction,\n        label_smoothing=args.label_smoothing,\n        aux_factor=args.aux_factor\n     )\n    ...\n</code></pre> </li> </ol>"},{"location":"tutorials/configuration/#learning-rate-scheduler","title":"Learning Rate Scheduler","text":"<ol> <li>Parameter description</li> </ol> <ul> <li> <p>scheduler: name of scheduler.</p> </li> <li> <p>min_lr: the minimum value of learning rate if the scheduler supports.</p> </li> <li> <p>lr: learning rate.</p> </li> <li> <p>warmup_epochs: warmup epochs.</p> </li> <li> <p>decay_epochs: decay epochs.</p> </li> </ul> <ol> <li> <p>Sample yaml file</p> <pre><code>scheduler: 'cosine_decay'\nmin_lr: 0.0\nlr: 0.01\nwarmup_epochs: 0\ndecay_epochs: 200\n...\n</code></pre> </li> <li> <p>Parse parameter setting</p> <pre><code>python train.py ... --scheduler cosine_decay --min_lr 0.0 --lr 0.01 \\\n    --warmup_epochs 0 --decay_epochs 200 ...\n</code></pre> </li> <li> <p>Corresponding code example</p> <pre><code>def train(args):\n    ...\n    lr_scheduler = create_scheduler(num_batches,\n        scheduler=args.scheduler,\n        lr=args.lr,\n        min_lr=args.min_lr,\n        warmup_epochs=args.warmup_epochs,\n        warmup_factor=args.warmup_factor,\n        decay_epochs=args.decay_epochs,\n        decay_rate=args.decay_rate,\n        milestones=args.multi_step_decay_milestones,\n        num_epochs=args.epoch_size,\n        lr_epoch_stair=args.lr_epoch_stair\n    )\n    ...\n</code></pre> </li> </ol>"},{"location":"tutorials/configuration/#optimizer","title":"Optimizer","text":"<ol> <li>Parameter description</li> </ol> <ul> <li> <p>opt: name of optimizer.</p> </li> <li> <p>weight_decay_filter: weight decay filter (filter parameters from weight decay).</p> </li> <li> <p>momentum: Hyperparameter of type float, means momentum for the moving average.</p> </li> <li> <p>weight_decay: weight decay (L2 penalty).</p> </li> <li> <p>loss_scale: gradient scaling factor</p> </li> <li> <p>use_nesterov: whether enables the Nesterov momentum</p> </li> </ul> <ol> <li> <p>Sample yaml file</p> <pre><code>opt: 'momentum'\nweight_decay_filter: 'norm_and_bias'\nmomentum: 0.9\nweight_decay: 0.00007\nloss_scale: 1024\nuse_nesterov: False\n...\n</code></pre> </li> <li> <p>Parse parameter setting</p> <pre><code>python train.py ... --opt momentum --weight_decay_filter 'norm_and_bias' --weight_decay 0.00007 \\\n    --loss_scale 1024 --use_nesterov False ...\n</code></pre> </li> <li> <p>Corresponding code example</p> <pre><code>def train(args):\n    ...\n    if args.ema:\n        optimizer = create_optimizer(network.trainable_params(),\n            opt=args.opt,\n            lr=lr_scheduler,\n            weight_decay=args.weight_decay,\n            momentum=args.momentum,\n            nesterov=args.use_nesterov,\n            weight_decay_filter=args.weight_decay_filter,\n            loss_scale=args.loss_scale,\n            checkpoint_path=opt_ckpt_path,\n            eps=args.eps\n        )\n    else:\n        optimizer = create_optimizer(network.trainable_params(),\n            opt=args.opt,\n            lr=lr_scheduler,\n            weight_decay=args.weight_decay,\n            momentum=args.momentum,\n            nesterov=args.use_nesterov,\n            weight_decay_filter=args.weight_decay_filter,\n            checkpoint_path=opt_ckpt_path,\n            eps=args.eps\n        )\n    ...\n</code></pre> </li> </ol>"},{"location":"tutorials/configuration/#combination-of-yaml-and-parse","title":"Combination of Yaml and Parse","text":"<p>You can override the parameter settings in the yaml file by using parse to set parameters. Take the following shell command as an example,</p> <pre><code>python train.py -c ./configs/squeezenet/squeezenet_1.0_gpu.yaml --data_dir ./data\n</code></pre> <p>The above command overwrites the value of <code>args.data_dir</code> parameter from <code>./imagenet2012</code> in yaml file to <code>./data</code>.</p>"},{"location":"tutorials/finetune/","title":"Model Fine-Tuning Training","text":"<p>In this tutorial, you will learn how to use MindCV for transfer Learning to solve the problem of image classification on custom datasets. In the deep learning task, we often encounter the problem of insufficient training data. At this time, it is difficult to train the entire network directly to achieve the desired accuracy. A better approach is to use a pretrained model on a large dataset (close to the task data), and then use the model to initialize the network's weight parameters or apply it to specific tasks as a fixed feature extractor.</p> <p>This tutorial will use the DenseNet model pretrained on ImageNet as an example to introduce two different fine-tuning strategies to solve the image classification problem of wolves and dogs in the case of small samples:</p> <ol> <li>Overall model fine-tuning.</li> <li>Freeze backbone and only fine-tune the classifier.</li> </ol> <p>For details of transfer learning, see Stanford University CS231n</p>"},{"location":"tutorials/finetune/#data-preparation","title":"Data Preparation","text":""},{"location":"tutorials/finetune/#download-dataset","title":"Download Dataset","text":"<p>Download the dog and wolf classification dataset used in the case. Each category has 120 training images and 30 verification images. Use the <code>mindcv.utils.download</code> interface to download the dataset, and automatically unzip the downloaded dataset to the current directory.</p> <pre><code>import os\nfrom mindcv.utils.download import DownLoad\n\ndataset_url = \"https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/datasets/intermediate/Canidae_data.zip\"\nroot_dir = \"./\"\n\nif not os.path.exists(os.path.join(root_dir, 'data/Canidae')):\n    DownLoad().download_and_extract_archive(dataset_url, root_dir)\n</code></pre> <p>The directory structure of the dataset is as follows:</p> <pre><code>data/\n\u2514\u2500\u2500 Canidae\n    \u251c\u2500\u2500 train\n    \u2502   \u251c\u2500\u2500 dogs\n    \u2502   \u2514\u2500\u2500 wolves\n    \u2514\u2500\u2500 val\n        \u251c\u2500\u2500 dogs\n        \u2514\u2500\u2500 wolves\n</code></pre>"},{"location":"tutorials/finetune/#dataset-loading-and-processing","title":"Dataset Loading and Processing","text":""},{"location":"tutorials/finetune/#loading-custom-datasets","title":"Loading Custom Datasets","text":"<p>By calling the <code>create_dataset</code> function in <code>mindcv.data</code>, we can easily load preset and customized datasets.</p> <ul> <li>When the parameter <code>name</code> is set to null, it is specified as a user-defined dataset. (Default)</li> <li>When the parameter <code>name</code> is set to be <code>MNIST</code>, <code>CIFAR10</code> or other standard dataset names, it is specified as the preset dataset.</li> </ul> <p>At the same time, we need to set the path <code>data_dir</code> of the dataset and the name <code>split</code> of the data segmentation (such as train, val) to load the corresponding training set or validation set.</p> <pre><code>from mindcv.data import create_dataset, create_transforms, create_loader\n\nnum_workers = 8\n\n# path of dataset\ndata_dir = \"./data/Canidae/\"\n\n# load dataset\ndataset_train = create_dataset(root=data_dir, split='train', num_parallel_workers=num_workers)\ndataset_val = create_dataset(root=data_dir, split='val', num_parallel_workers=num_workers)\n</code></pre> <p>Note: The directory structure of the custom dataset should be the same as ImageNet, that is, the hierarchy of root -&gt;split -&gt;class -&gt;image</p> <pre><code>DATASET_NAME\n    \u251c\u2500\u2500 split1(e.g. train)/\n    \u2502  \u251c\u2500\u2500 class1/\n    \u2502  \u2502   \u251c\u2500\u2500 000001.jpg\n    \u2502  \u2502   \u251c\u2500\u2500 000002.jpg\n    \u2502  \u2502   \u2514\u2500\u2500 ....\n    \u2502  \u2514\u2500\u2500 class2/\n    \u2502      \u251c\u2500\u2500 000001.jpg\n    \u2502      \u251c\u2500\u2500 000002.jpg\n    \u2502      \u2514\u2500\u2500 ....\n    \u2514\u2500\u2500 split2/\n       \u251c\u2500\u2500 class1/\n       \u2502   \u251c\u2500\u2500 000001.jpg\n       \u2502   \u251c\u2500\u2500 000002.jpg\n       \u2502   \u2514\u2500\u2500 ....\n       \u2514\u2500\u2500 class2/\n           \u251c\u2500\u2500 000001.jpg\n           \u251c\u2500\u2500 000002.jpg\n           \u2514\u2500\u2500 ....\n</code></pre>"},{"location":"tutorials/finetune/#data-processing-and-augmentation","title":"Data Processing and Augmentation","text":"<p>First, we call the <code>create_transforms</code> function to obtain the preset data processing and augmentation strategy (transform list). In this task, because the file structure of the wolf-dog dataset is consistent with that of the ImageNet dataset, we specify the parameter <code>dataset_name</code> as ImageNet, and directly use the preset ImageNet data processing and image augmentation strategy. <code>create_transforms</code> also supports a variety of customized processing and enhancement operations, as well as automatic enhancement policies (AutoAug). See API description for details.</p> <p>We will transfer the obtained transform list to the <code>create_loader()</code>, specify <code>batch_size</code> and other parameters to complete the preparation of training and validation data, and return the <code>Dataset</code> Object as the input of the model.</p> <pre><code># Define and acquire data processing and augment operations\ntrans_train = create_transforms(dataset_name='ImageNet', is_training=True)\ntrans_val = create_transforms(dataset_name='ImageNet',is_training=False)\n\nloader_train = create_loader(\n    dataset=dataset_train,\n    batch_size=16,\n    is_training=True,\n    num_classes=2,\n    transform=trans_train,\n    num_parallel_workers=num_workers,\n)\nloader_val = create_loader(\n    dataset=dataset_val,\n    batch_size=5,\n    is_training=True,\n    num_classes=2,\n    transform=trans_val,\n    num_parallel_workers=num_workers,\n)\n</code></pre>"},{"location":"tutorials/finetune/#dataset-visualization","title":"Dataset Visualization","text":"<p>For the Dataset object returned by the <code>create_loader</code> interface to complete data loading, we can create a data iterator through the <code>create_tuple_iterator</code> interface, access the dataset using the <code>next</code> iteration, and read a batch of data.</p> <pre><code>images, labels = next(loader_train.create_tuple_iterator())\nprint(\"Tensor of image\", images.shape)\nprint(\"Labels:\", labels)\n</code></pre> <pre><code>Tensor of image (16, 3, 224, 224)\nLabels: [0 1 1 1 1 0 0 0 0 0 1 0 1 0 1 1]\n</code></pre> <p>Visualize the acquired image and label data, and the title is the label name corresponding to the image.</p> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\n# class_name corresponds to label, and labels are marked in the order of folder string from small to large\nclass_name = {0: \"dogs\", 1: \"wolves\"}\n\nplt.figure(figsize=(15, 7))\nfor i in range(len(labels)):\n    # Get the image and its corresponding label\n    data_image = images[i].asnumpy()\n    data_label = labels[i]\n    # Process images for display\n    data_image = np.transpose(data_image, (1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    data_image = std * data_image + mean\n    data_image = np.clip(data_image, 0, 1)\n    # Show Image\n    plt.subplot(3, 6, i + 1)\n    plt.imshow(data_image)\n    plt.title(class_name[int(labels[i].asnumpy())])\n    plt.axis(\"off\")\n\nplt.show()\n</code></pre> <p></p>"},{"location":"tutorials/finetune/#model-fine-tuning","title":"Model Fine-Tuning","text":""},{"location":"tutorials/finetune/#1-overall-model-fine-tuning","title":"1. Overall Model Fine-Tuning","text":""},{"location":"tutorials/finetune/#pretraining-model-loading","title":"Pretraining Model Loading","text":"<p>We use <code>mindcv.models.densenet</code> to define the DenseNet121 network. When the <code>pretrained</code> parameter in the interface is set to True, the network weight can be automatically downloaded. Since the pretraining model is used to classify 1000 categories in the ImageNet dataset, we set <code>num_classes=2</code>, and the output of DenseNet's classifier (the last FC layer) is adjusted to two dimensions. At this time, only the pre-trained weights of the backbone are loaded, while the classifier uses the initial value.</p> <pre><code>from mindcv.models import create_model\n\nnetwork = create_model(model_name='densenet121', num_classes=2, pretrained=True)\n</code></pre> <p>For the specific structure of DenseNet, see the DenseNet paper.</p>"},{"location":"tutorials/finetune/#model-training","title":"Model Training","text":"<p>Use the loaded and processed wolf and dog images with tags to fine-tune the DenseNet network. Note that smaller learning rates should be used when fine-tuning the overall model.</p> <pre><code>from mindcv.loss import create_loss\nfrom mindcv.optim import create_optimizer\nfrom mindcv.scheduler import create_scheduler\nfrom mindspore import Model, LossMonitor, TimeMonitor\n\n# Define optimizer and loss function\nopt = create_optimizer(network.trainable_params(), opt='adam', lr=1e-4)\nloss = create_loss(name='CE')\n\n# Instantiated model\nmodel = Model(network, loss_fn=loss, optimizer=opt, metrics={'accuracy'})\nmodel.train(10, loader_train, callbacks=[LossMonitor(5), TimeMonitor(5)], dataset_sink_mode=False)\n</code></pre> <pre><code>epoch: 1 step: 5, loss is 0.5195528864860535\nepoch: 1 step: 10, loss is 0.2654373049736023\nepoch: 1 step: 15, loss is 0.28758567571640015\nTrain epoch time: 17270.144 ms, per step time: 1151.343 ms\nepoch: 2 step: 5, loss is 0.1807008981704712\nepoch: 2 step: 10, loss is 0.1700802594423294\nepoch: 2 step: 15, loss is 0.09752683341503143\nTrain epoch time: 1372.549 ms, per step time: 91.503 ms\nepoch: 3 step: 5, loss is 0.13594701886177063\nepoch: 3 step: 10, loss is 0.03628234937787056\nepoch: 3 step: 15, loss is 0.039737217128276825\nTrain epoch time: 1453.237 ms, per step time: 96.882 ms\nepoch: 4 step: 5, loss is 0.014213413000106812\nepoch: 4 step: 10, loss is 0.030747078359127045\nepoch: 4 step: 15, loss is 0.0798817127943039\nTrain epoch time: 1331.237 ms, per step time: 88.749 ms\nepoch: 5 step: 5, loss is 0.009510636329650879\nepoch: 5 step: 10, loss is 0.02603740245103836\nepoch: 5 step: 15, loss is 0.051846928894519806\nTrain epoch time: 1312.737 ms, per step time: 87.516 ms\nepoch: 6 step: 5, loss is 0.1163717582821846\nepoch: 6 step: 10, loss is 0.02439398318529129\nepoch: 6 step: 15, loss is 0.02564268559217453\nTrain epoch time: 1434.704 ms, per step time: 95.647 ms\nepoch: 7 step: 5, loss is 0.013310655951499939\nepoch: 7 step: 10, loss is 0.02289542555809021\nepoch: 7 step: 15, loss is 0.1992517113685608\nTrain epoch time: 1275.935 ms, per step time: 85.062 ms\nepoch: 8 step: 5, loss is 0.015928998589515686\nepoch: 8 step: 10, loss is 0.011409260332584381\nepoch: 8 step: 15, loss is 0.008141174912452698\nTrain epoch time: 1323.102 ms, per step time: 88.207 ms\nepoch: 9 step: 5, loss is 0.10395607352256775\nepoch: 9 step: 10, loss is 0.23055407404899597\nepoch: 9 step: 15, loss is 0.04896317049860954\nTrain epoch time: 1261.067 ms, per step time: 84.071 ms\nepoch: 10 step: 5, loss is 0.03162381425499916\nepoch: 10 step: 10, loss is 0.13094250857830048\nepoch: 10 step: 15, loss is 0.020028553903102875\nTrain epoch time: 1217.958 ms, per step time: 81.197 ms\n</code></pre>"},{"location":"tutorials/finetune/#model-evaluation","title":"Model Evaluation","text":"<p>After the training, we evaluate the accuracy of the model on the validation set.</p> <pre><code>res = model.eval(loader_val)\nprint(res)\n</code></pre> <pre><code>{'accuracy': 1.0}\n</code></pre>"},{"location":"tutorials/finetune/#visual-model-inference-results","title":"Visual Model Inference Results","text":"<p>Define <code>visualize_mode</code> function and visualize model prediction.</p> <pre><code>import matplotlib.pyplot as plt\nimport mindspore as ms\n\ndef visualize_model(model, val_dl, num_classes=2):\n    # Load the data of the validation set for validation\n    images, labels= next(val_dl.create_tuple_iterator())\n    # Predict image class\n    output = model.predict(images)\n    pred = np.argmax(output.asnumpy(), axis=1)\n    # Display images and their predicted values\n    images = images.asnumpy()\n    labels = labels.asnumpy()\n    class_name = {0: \"dogs\", 1: \"wolves\"}\n    plt.figure(figsize=(15, 7))\n    for i in range(len(labels)):\n        plt.subplot(3, 6, i + 1)\n        # If the prediction is correct, it is displayed in blue; If the prediction is wrong, it is displayed in red\n        color = 'blue' if pred[i] == labels[i] else 'red'\n        plt.title('predict:{}'.format(class_name[pred[i]]), color=color)\n        picture_show = np.transpose(images[i], (1, 2, 0))\n        mean = np.array([0.485, 0.456, 0.406])\n        std = np.array([0.229, 0.224, 0.225])\n        picture_show = std * picture_show + mean\n        picture_show = np.clip(picture_show, 0, 1)\n        plt.imshow(picture_show)\n        plt.axis('off')\n\n    plt.show()\n</code></pre> <p>Use the finely tuned model piece to predict the wolf and dog image data of the verification set. If the prediction font is blue, the prediction is correct; if the prediction font is red, the prediction is wrong.</p> <pre><code>visualize_model(model, loader_val)\n</code></pre> <p></p>"},{"location":"tutorials/finetune/#2-freeze-backbone-and-fine-tune-the-classifier","title":"2. Freeze Backbone and Fine-Tune the Classifier","text":""},{"location":"tutorials/finetune/#freezing-backbone-parameters","title":"Freezing Backbone Parameters","text":"<p>First, we need to freeze all network layers except the last layer classifier, that is, set the <code>requires_grad</code> attribute of the corresponding layer parameter to False, so that it does not calculate the gradient and update the parameters in the backpropagation.</p> <p>Because all models in <code>mindcv.models</code> use a <code>classifier</code> to identify and name the classifier of the model (i.e., the Dense layer), the parameters of each layer outside the classifier can be filtered through <code>classifier.weight</code> and <code>classifier.bias</code>, and its <code>requires_grad</code> attribute is set to False.</p> <pre><code># freeze backbone\nfor param in network.get_parameters():\n    if param.name not in [\"classifier.weight\", \"classifier.bias\"]:\n        param.requires_grad = False\n</code></pre>"},{"location":"tutorials/finetune/#fine-tune-classifier","title":"Fine-Tune Classifier","text":"<p>Because the feature network has been fixed, we don't have to worry about distortpratised features in the training process. Therefore, compared with the first method, we can increase the learning rate.</p> <p>Compared with no pretraining model, it will save more than half of the time, because partial gradient can not be calculated at this time.</p> <pre><code># dataset load\ndataset_train = create_dataset(root=data_dir, split='train', num_parallel_workers=num_workers)\nloader_train = create_loader(\n    dataset=dataset_train,\n    batch_size=16,\n    is_training=True,\n    num_classes=2,\n    transform=trans_train,\n    num_parallel_workers=num_workers,\n)\n\n# Define optimizer and loss function\nopt = create_optimizer(network.trainable_params(), opt='adam', lr=1e-3)\nloss = create_loss(name='CE')\n\n# Instantiated model\nmodel = Model(network, loss_fn=loss, optimizer=opt, metrics={'accuracy'})\n\nmodel.train(10, loader_train, callbacks=[LossMonitor(5), TimeMonitor(5)], dataset_sink_mode=False)\n</code></pre> <pre><code>epoch: 1 step: 5, loss is 0.051333948969841\nepoch: 1 step: 10, loss is 0.02043312042951584\nepoch: 1 step: 15, loss is 0.16161368787288666\nTrain epoch time: 10228.601 ms, per step time: 681.907 ms\nepoch: 2 step: 5, loss is 0.002121545374393463\nepoch: 2 step: 10, loss is 0.0009798109531402588\nepoch: 2 step: 15, loss is 0.015776708722114563\nTrain epoch time: 562.543 ms, per step time: 37.503 ms\nepoch: 3 step: 5, loss is 0.008056879043579102\nepoch: 3 step: 10, loss is 0.0009347647428512573\nepoch: 3 step: 15, loss is 0.028648357838392258\nTrain epoch time: 523.249 ms, per step time: 34.883 ms\nepoch: 4 step: 5, loss is 0.001014217734336853\nepoch: 4 step: 10, loss is 0.0003159046173095703\nepoch: 4 step: 15, loss is 0.0007699579000473022\nTrain epoch time: 508.886 ms, per step time: 33.926 ms\nepoch: 5 step: 5, loss is 0.0015687644481658936\nepoch: 5 step: 10, loss is 0.012090332806110382\nepoch: 5 step: 15, loss is 0.004598274827003479\nTrain epoch time: 507.243 ms, per step time: 33.816 ms\nepoch: 6 step: 5, loss is 0.010022152215242386\nepoch: 6 step: 10, loss is 0.0066385045647621155\nepoch: 6 step: 15, loss is 0.0036080628633499146\nTrain epoch time: 517.646 ms, per step time: 34.510 ms\nepoch: 7 step: 5, loss is 0.01344013586640358\nepoch: 7 step: 10, loss is 0.0008538365364074707\nepoch: 7 step: 15, loss is 0.14135593175888062\nTrain epoch time: 511.513 ms, per step time: 34.101 ms\nepoch: 8 step: 5, loss is 0.01626245677471161\nepoch: 8 step: 10, loss is 0.02871556021273136\nepoch: 8 step: 15, loss is 0.010110966861248016\nTrain epoch time: 545.678 ms, per step time: 36.379 ms\nepoch: 9 step: 5, loss is 0.008498094975948334\nepoch: 9 step: 10, loss is 0.2588501274585724\nepoch: 9 step: 15, loss is 0.0014278888702392578\nTrain epoch time: 499.243 ms, per step time: 33.283 ms\nepoch: 10 step: 5, loss is 0.021337147802114487\nepoch: 10 step: 10, loss is 0.00829876959323883\nepoch: 10 step: 15, loss is 0.008352771401405334\nTrain epoch time: 465.600 ms, per step time: 31.040 ms\n</code></pre>"},{"location":"tutorials/finetune/#model-evaluation_1","title":"Model Evaluation","text":"<p>After the training, we evaluate the accuracy of the model on the validation set.</p> <pre><code>dataset_val = create_dataset(root=data_dir, split='val', num_parallel_workers=num_workers)\nloader_val = create_loader(\n    dataset=dataset_val,\n    batch_size=5,\n    is_training=True,\n    num_classes=2,\n    transform=trans_val,\n    num_parallel_workers=num_workers,\n)\n\nres = model.eval(loader_val)\nprint(res)\n</code></pre> <pre><code>{'accuracy': 1.0}\n</code></pre>"},{"location":"tutorials/finetune/#visual-model-prediction","title":"Visual Model Prediction","text":"<p>Use the finely tuned model piece to predict the wolf and dog image data of the verification set. If the prediction font is blue, the prediction is correct; if the prediction font is red, the prediction is wrong.</p> <pre><code>visualize_model(model, loader_val)\n</code></pre> <p></p> <p>The prediction results of wolf/dog after fine-tuning are correct.</p>"},{"location":"tutorials/inference/","title":"Image Classification Prediction","text":"<p>This tutorial introduces how to call the pretraining model in MindCV to make classification prediction on the test image.</p>"},{"location":"tutorials/inference/#model-loading","title":"Model Loading","text":""},{"location":"tutorials/inference/#view-all-available-models","title":"View All Available Models","text":"<p>By calling the <code>registry.list_models</code> function in <code>mindcv.models</code>, the names of all network models can be printed. The models of a network in different parameter configurations will also be printed, such as resnet18 / resnet34 / resnet50 / resnet101 / resnet152.</p> <pre><code>import sys\nsys.path.append(\"..\")\nfrom mindcv.models import registry\nregistry.list_models()\n</code></pre> <pre><code>['BiT_resnet50',\n 'repmlp_b224',\n 'repmlp_b256',\n 'repmlp_d256',\n 'repmlp_l256',\n 'repmlp_t224',\n 'repmlp_t256',\n 'convit_base',\n 'convit_base_plus',\n 'convit_small',\n ...\n 'visformer_small',\n 'visformer_small_v2',\n 'visformer_tiny',\n 'visformer_tiny_v2',\n 'vit_b_16_224',\n 'vit_b_16_384',\n 'vit_b_32_224',\n 'vit_b_32_384',\n 'vit_l_16_224',\n 'vit_l_16_384',\n 'vit_l_32_224',\n 'xception']\n</code></pre>"},{"location":"tutorials/inference/#load-pretraining-model","title":"Load Pretraining Model","text":"<p>Taking the resnet50 model as an example, we introduce two methods to load the model checkpoint using the <code>create_model</code> function in <code>mindcv.models</code>.</p> <p>1). When the <code>pretrained</code> parameter in the interface is set to True, network weights can be automatically downloaded.</p> <pre><code>from mindcv.models import create_model\nmodel = create_model(model_name='resnet50', num_classes=1000, pretrained=True)\n# Switch the execution logic of the network to the inference scenario\nmodel.set_train(False)\n</code></pre> <pre><code>102453248B [00:16, 6092186.31B/s]\n\nResNet&lt;\n  (conv1): Conv2d&lt;input_channels=3, output_channels=64, kernel_size=(7, 7), stride=(2, 2), pad_mode=pad, padding=3, dilation=(1, 1), group=1, has_bias=False, weight_init=normal, bias_init=zeros, format=NCHW&gt;\n  (bn1): BatchNorm2d&lt;num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)&gt;\n  (relu): ReLU&lt;&gt;\n  (max_pool): MaxPool2d&lt;kernel_size=3, stride=2, pad_mode=SAME&gt;\n  ...\n  (pool): GlobalAvgPooling&lt;&gt;\n  (classifier): Dense&lt;input_channels=2048, output_channels=1000, has_bias=True&gt;\n  &gt;\n</code></pre> <p>2). When the <code>checkpoint_path</code> parameter in the interface is set to the file path, the model parameter file with the <code>.ckpt</code> can be loaded.</p> <pre><code>from mindcv.models import create_model\nmodel = create_model(model_name='resnet50', num_classes=1000, checkpoint_path='./resnet50_224.ckpt')\n# Switch the execution logic of the network to the inference scenario\nmodel.set_train(False)\n</code></pre>"},{"location":"tutorials/inference/#data-preparation","title":"Data Preparation","text":""},{"location":"tutorials/inference/#create-dataset","title":"Create Dataset","text":"<p>Here, we download a Wikipedia image as a test image, and use the <code>create_dataset</code> function in <code>mindcv.data</code> to construct a custom dataset for a single image.</p> <pre><code>from mindcv.data import create_dataset\nnum_workers = 1\n# path of dataset\ndata_dir = \"./data/\"\ndataset = create_dataset(root=data_dir, split='test', num_parallel_workers=num_workers)\n# Image visualization\nfrom PIL import Image\nImage.open(\"./data/test/dog/dog.jpg\")\n</code></pre> <p></p>"},{"location":"tutorials/inference/#data-preprocessing","title":"Data Preprocessing","text":"<p>Call the <code>create_transforms</code> function to obtain the data processing strategy (transform list) of the ImageNet dataset used by the pre-trained model.</p> <p>We pass the obtained transform list into the <code>create_loader</code> function, specify <code>batch_size=1</code> and other parameters, and then complete the preparation of test data. The <code>Dataset</code> object is returned as the input of the model.</p> <pre><code>from mindcv.data import create_transforms, create_loader\ntransforms_list = create_transforms(dataset_name='imagenet', is_training=False)\ndata_loader = create_loader(\n    dataset=dataset,\n    batch_size=1,\n    is_training=False,\n    num_classes=1000,\n    transform=transforms_list,\n    num_parallel_workers=num_workers\n)\n</code></pre>"},{"location":"tutorials/inference/#model-inference","title":"Model Inference","text":"<p>The picture of the user-defined dataset is transferred to the model to obtain the inference result. Here, use the <code>Squeeze</code> function of <code>mindspore.ops</code> to remove the batch dimension.</p> <pre><code>import mindspore.ops as P\nimport numpy as np\nimages, _ = next(data_loader.create_tuple_iterator())\noutput = P.Squeeze()(model(images))\npred = np.argmax(output.asnumpy())\n</code></pre> <pre><code>with open(\"imagenet1000_clsidx_to_labels.txt\") as f:\n    idx2label = eval(f.read())\nprint('predict: {}'.format(idx2label[pred]))\n</code></pre> <pre><code>predict: Labrador retriever\n</code></pre>"},{"location":"tutorials/quick_start/","title":"Quick Start","text":"<p>MindCV is an open-source toolbox for computer vision research and development based on MindSpore. It collects a series of classic and SoTA vision models, such as ResNet and SwinTransformer, along with their pretrained weights. SoTA methods such as AutoAugment are also provided for performance improvement. With the decoupled module design, it is easy to apply or adapt MindCV to your own CV tasks. In this tutorial, we will provide a quick start guideline for MindCV.</p> <p>This tutorial will take DenseNet classification model as an example to implement transfer training on CIFAR-10 dataset and explain the usage of MindCV modules in this process.</p>"},{"location":"tutorials/quick_start/#environment-setting","title":"Environment Setting","text":"<p>See Installation for details.</p>"},{"location":"tutorials/quick_start/#data","title":"Data","text":""},{"location":"tutorials/quick_start/#dataset","title":"Dataset","text":"<p>Through the <code>create_dataset</code> module in <code>mindcv.data</code>, we can quickly load standard datasets or customized datasets.</p> <pre><code>import os\nfrom mindcv.data import create_dataset, create_transforms, create_loader\n\ncifar10_dir = './datasets/cifar/cifar-10-batches-bin'  # your dataset path\nnum_classes = 10  # num of classes\nnum_workers = 8  # num of parallel workers\n\n# create dataset\ndataset_train = create_dataset(\n    name='cifar10', root=cifar10_dir, split='train', shuffle=True, num_parallel_workers=num_workers\n)\n</code></pre>"},{"location":"tutorials/quick_start/#transform","title":"Transform","text":"<p>Through the <code>create_transforms</code> function, you can directly obtain the appropriate data processing augmentation strategies (transform list) for standard datasets, including common data processing strategies on Cifar10 and Imagenet.</p> <pre><code># create transforms\ntrans = create_transforms(dataset_name='cifar10', image_resize=224)\n</code></pre>"},{"location":"tutorials/quick_start/#loader","title":"Loader","text":"<p>The <code>mindcv.data.create_loader</code> function is used for data conversion and batch split loading. We need to pass in the transform_list returned by <code>create_transforms</code>.</p> <pre><code># Perform data augmentation operations to generate the required dataset.\nloader_train = create_loader(dataset=dataset_train,\n                             batch_size=64,\n                             is_training=True,\n                             num_classes=num_classes,\n                             transform=trans,\n                             num_parallel_workers=num_workers)\n\nnum_batches = loader_train.get_dataset_size()\n</code></pre> <p>Avoid repeatedly executing a single cell of <code>create_loader</code> in notebook, or execute again after executing <code>create_dataset</code>.</p>"},{"location":"tutorials/quick_start/#model","title":"Model","text":"<p>Use the <code>create_model</code> interface to obtain the instantiated DenseNet and load the pretraining weight(obtained from ImageNet dataset training).</p> <pre><code>from mindcv.models import create_model\n\n# instantiate the DenseNet121 model and load the pretraining weights.\nnetwork = create_model(model_name='densenet121', num_classes=num_classes, pretrained=True)\n</code></pre> <p>Because the number of classes required by CIFAR-10 and ImageNet datasets is different, the classifier parameters cannot be shared, and the warning that the classifier parameters cannot be loaded does not affect the fine-tuning.</p>"},{"location":"tutorials/quick_start/#loss","title":"Loss","text":"<p>By <code>create_loss</code> interface obtains loss function.</p> <pre><code>from mindcv.loss import create_loss\n\nloss = create_loss(name='CE')\n</code></pre>"},{"location":"tutorials/quick_start/#learning-rate-scheduler","title":"Learning Rate Scheduler","text":"<p>Use <code>create_scheduler</code> interface to set the learning rate scheduler.</p> <pre><code>from mindcv.scheduler import create_scheduler\n\n# learning rate scheduler\nlr_scheduler = create_scheduler(steps_per_epoch=num_batches,\n                                scheduler='constant',\n                                lr=0.0001)\n</code></pre>"},{"location":"tutorials/quick_start/#optimizer","title":"Optimizer","text":"<p>Use <code>create_optimizer</code> interface creates an optimizer.</p> <pre><code>from mindcv.optim import create_optimizer\n\n# create optimizer\nopt = create_optimizer(network.trainable_params(), opt='adam', lr=lr_scheduler)\n</code></pre>"},{"location":"tutorials/quick_start/#training","title":"Training","text":"<p>Use the mindspore.Model interface to encapsulate trainable instances according to the parameters passed in by the user.</p> <pre><code>from mindspore import Model\n\n# Encapsulates examples that can be trained or inferred\nmodel = Model(network, loss_fn=loss, optimizer=opt, metrics={'accuracy'})\n</code></pre> <p>Use the <code>mindspore.Model.train</code> interface for model training.</p> <pre><code>from mindspore import LossMonitor, TimeMonitor, CheckpointConfig, ModelCheckpoint\n\n# Set the callback function for saving network parameters during training.\nckpt_save_dir = './ckpt'\nckpt_config = CheckpointConfig(save_checkpoint_steps=num_batches)\nckpt_cb = ModelCheckpoint(prefix='densenet121-cifar10',\n                          directory=ckpt_save_dir,\n                          config=ckpt_config)\n\nmodel.train(5, loader_train, callbacks=[LossMonitor(num_batches//5), TimeMonitor(num_batches//5), ckpt_cb], dataset_sink_mode=False)\n</code></pre> <pre><code>[WARNING] DEVICE(1769,ffff87c70ac0,python):2022-12-21-16:04:30.001.890 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/network-WithLossCell/_loss_fn-CrossEntropySmooth/GatherD-op5273] don't support int64, reduce precision from int64 to int32.\n\n\nepoch: 1 step: 156, loss is 2.0816354751586914\nepoch: 1 step: 312, loss is 1.4474115371704102\nepoch: 1 step: 468, loss is 0.8935483694076538\nepoch: 1 step: 624, loss is 0.5588696002960205\nepoch: 1 step: 780, loss is 0.3161369860172272\n\n\n[WARNING] DEVICE(1769,ffff87c70ac0,python):2022-12-21-16:09:20.261.851 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/network-WithLossCell/_loss_fn-CrossEntropySmooth/GatherD-op16720] don't support int64, reduce precision from int64 to int32.\n\n\nTrain epoch time: 416429.509 ms, per step time: 532.519 ms\nepoch: 2 step: 154, loss is 0.19752007722854614\nepoch: 2 step: 310, loss is 0.14635677635669708\nepoch: 2 step: 466, loss is 0.3511860966682434\nepoch: 2 step: 622, loss is 0.12542471289634705\nepoch: 2 step: 778, loss is 0.22351759672164917\nTrain epoch time: 156746.872 ms, per step time: 200.444 ms\nepoch: 3 step: 152, loss is 0.08965137600898743\nepoch: 3 step: 308, loss is 0.22765043377876282\nepoch: 3 step: 464, loss is 0.19035443663597107\nepoch: 3 step: 620, loss is 0.06591956317424774\nepoch: 3 step: 776, loss is 0.0934530645608902\nTrain epoch time: 156574.210 ms, per step time: 200.223 ms\nepoch: 4 step: 150, loss is 0.03782692924141884\nepoch: 4 step: 306, loss is 0.023876197636127472\nepoch: 4 step: 462, loss is 0.038690414279699326\nepoch: 4 step: 618, loss is 0.15388774871826172\nepoch: 4 step: 774, loss is 0.1581358164548874\nTrain epoch time: 158398.108 ms, per step time: 202.555 ms\nepoch: 5 step: 148, loss is 0.06556802988052368\nepoch: 5 step: 304, loss is 0.006707251071929932\nepoch: 5 step: 460, loss is 0.02353120595216751\nepoch: 5 step: 616, loss is 0.014183484017848969\nepoch: 5 step: 772, loss is 0.09367241710424423\nTrain epoch time: 154978.618 ms, per step time: 198.182 ms\n</code></pre>"},{"location":"tutorials/quick_start/#evaluation","title":"Evaluation","text":"<p>Now, let's evaluate the trained model on the validation set of CIFAR-10.</p> <pre><code># Load validation dataset\ndataset_val = create_dataset(\n    name='cifar10', root=cifar10_dir, split='test', shuffle=True, num_parallel_workers=num_workers\n)\n\n# Perform data enhancement operations to generate the required dataset.\nloader_val = create_loader(dataset=dataset_val,\n                           batch_size=64,\n                           is_training=False,\n                           num_classes=num_classes,\n                           transform=trans,\n                           num_parallel_workers=num_workers)\n</code></pre> <p>Load the fine-tuning parameter file (densenet121-cifar10-5_782.ckpt) to the model.</p> <p>Encapsulate inferable instances according to the parameters passed in by the user, load the validation dataset and verify the precision of the fine-tuned DenseNet121 model.</p> <pre><code># Verify the accuracy of DenseNet121 after fine-tune\nacc = model.eval(loader_val, dataset_sink_mode=False)\nprint(acc)\n</code></pre> <pre><code>[WARNING] DEVICE(1769,ffff87c70ac0,python):2022-12-21-16:24:11.927.472 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/network-WithLossCell/_loss_fn-CrossEntropySmooth/GatherD-op24314] don't support int64, reduce precision from int64 to int32.\n\n\n{'accuracy': 0.951}\n\n\n[WARNING] DEVICE(1769,ffff87c70ac0,python):2022-12-21-16:25:01.871.273 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/network-WithLossCell/_loss_fn-CrossEntropySmooth/GatherD-op27139] don't support int64, reduce precision from int64 to int32.\n</code></pre>"},{"location":"tutorials/quick_start/#use-yaml-files-for-model-training-and-validation","title":"Use YAML files for model training and validation","text":"<p>We can also use the yaml file with the model parameters set directly to quickly train and verify the model through <code>train.py</code> and <code>validate.py</code> scripts. The following is an example of training SqueezenetV1 on ImageNet (you need to download ImageNet to the directory in advance).</p> <p>For detailed tutorials, please refer to the tutorial.</p> <pre><code># standalone training on single NPU device\npython train.py -c configs/squeezenet/squeezenet_1.0_gpu.yaml --data_dir /path/to/dataset --distribute False\n</code></pre> <pre><code>python validate.py -c configs/squeezenet/squeezenet_1.0_gpu.yaml --data_dir /path/to/dataset --ckpt_path /path/to/ckpt\n</code></pre>"},{"location":"zh/","title":"\u4e3b\u9875","text":""},{"location":"zh/#_1","title":"\u7b80\u4ecb","text":"<p>MindCV\u662f\u4e00\u4e2a\u57fa\u4e8e MindSpore \u5f00\u53d1\u7684\uff0c\u81f4\u529b\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u76f8\u5173\u6280\u672f\u7814\u53d1\u7684\u5f00\u6e90\u5de5\u5177\u7bb1\u3002\u5b83\u63d0\u4f9b\u5927\u91cf\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u7684\u7ecf\u5178\u6a21\u578b\u548cSoTA\u6a21\u578b\u4ee5\u53ca\u5b83\u4eec\u7684\u9884\u8bad\u7ec3\u6743\u91cd\u548c\u8bad\u7ec3\u7b56\u7565\u3002\u540c\u65f6\uff0c\u8fd8\u63d0\u4f9b\u4e86\u81ea\u52a8\u589e\u5f3a\u7b49SoTA\u7b97\u6cd5\u6765\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002\u901a\u8fc7\u89e3\u8026\u7684\u6a21\u5757\u8bbe\u8ba1\uff0c\u60a8\u53ef\u4ee5\u8f7b\u677e\u5730\u5c06MindCV\u5e94\u7528\u5230\u60a8\u81ea\u5df1\u7684CV\u4efb\u52a1\u4e2d\u3002</p>"},{"location":"zh/#_2","title":"\u4e3b\u8981\u7279\u6027","text":"<ul> <li> <p>\u9ad8\u6613\u7528\u6027 MindCV\u5c06\u89c6\u89c9\u4efb\u52a1\u5206\u89e3\u4e3a\u5404\u79cd\u53ef\u914d\u7f6e\u7684\u7ec4\u4ef6\uff0c\u7528\u6237\u53ef\u4ee5\u8f7b\u677e\u5730\u6784\u5efa\u81ea\u5df1\u7684\u6570\u636e\u5904\u7406\u548c\u6a21\u578b\u8bad\u7ec3\u6d41\u7a0b\u3002</p> <pre><code>&gt;&gt;&gt; import mindcv\n# \u521b\u5efa\u6570\u636e\u96c6\n&gt;&gt;&gt; dataset = mindcv.create_dataset('cifar10', download=True)\n# \u521b\u5efa\u6a21\u578b\n&gt;&gt;&gt; network = mindcv.create_model('resnet50', pretrained=True)\n</code></pre> <p>\u7528\u6237\u53ef\u901a\u8fc7\u9884\u5b9a\u4e49\u7684\u8bad\u7ec3\u548c\u5fae\u8c03\u811a\u672c\uff0c\u5feb\u901f\u914d\u7f6e\u5e76\u5b8c\u6210\u8bad\u7ec3\u6216\u8fc1\u79fb\u5b66\u4e60\u4efb\u52a1\u3002</p> <pre><code># \u914d\u7f6e\u548c\u542f\u52a8\u8fc1\u79fb\u5b66\u4e60\u4efb\u52a1\npython train.py --model swin_tiny --pretrained --opt=adamw --lr=0.001 --data_dir=/path/to/dataset\n</code></pre> </li> <li> <p>\u9ad8\u6027\u80fd MindCV\u96c6\u6210\u4e86\u5927\u91cf\u57fa\u4e8eCNN\u548cTransformer\u7684\u9ad8\u6027\u80fd\u6a21\u578b\uff0c\u5982SwinTransformer\uff0c\u5e76\u63d0\u4f9b\u9884\u8bad\u7ec3\u6743\u91cd\u3001\u8bad\u7ec3\u7b56\u7565\u548c\u6027\u80fd\u62a5\u544a\uff0c\u5e2e\u52a9\u7528\u6237\u5feb\u901f\u9009\u578b\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u89c6\u89c9\u6a21\u578b\u3002</p> </li> <li> <p>\u7075\u6d3b\u9ad8\u6548 MindCV\u57fa\u4e8e\u9ad8\u6548\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6MindSpore\u5f00\u53d1\uff0c\u5177\u6709\u81ea\u52a8\u5e76\u884c\u548c\u81ea\u52a8\u5fae\u5206\u7b49\u7279\u6027\uff0c\u652f\u6301\u4e0d\u540c\u786c\u4ef6\u5e73\u53f0\u4e0a\uff08CPU/GPU/Ascend\uff09\uff0c\u540c\u65f6\u652f\u6301\u6548\u7387\u4f18\u5316\u7684\u9759\u6001\u56fe\u6a21\u5f0f\u548c\u8c03\u8bd5\u7075\u6d3b\u7684\u52a8\u6001\u56fe\u6a21\u5f0f\u3002</p> </li> </ul>"},{"location":"zh/#_3","title":"\u6a21\u578b\u652f\u6301","text":"<p>\u57fa\u4e8eMindCV\u8fdb\u884c\u6a21\u578b\u5b9e\u73b0\u548c\u91cd\u8bad\u7ec3\u7684\u6c47\u603b\u7ed3\u679c\u8be6\u89c1\u6a21\u578b\u4ed3\u5e93, \u6240\u7528\u5230\u7684\u8bad\u7ec3\u7b56\u7565\u548c\u8bad\u7ec3\u540e\u7684\u6a21\u578b\u6743\u91cd\u5747\u53ef\u901a\u8fc7\u8868\u4e2d\u94fe\u63a5\u83b7\u53d6\u3002</p> <p>\u5404\u6a21\u578b\u8bb2\u89e3\u548c\u8bad\u7ec3\u8bf4\u660e\u8be6\u89c1configs\u76ee\u5f55\u3002</p>"},{"location":"zh/#_4","title":"\u5b89\u88c5","text":"<p>\u8be6\u60c5\u8bf7\u89c1\u5b89\u88c5\u9875\u9762\u3002</p>"},{"location":"zh/#_5","title":"\u5feb\u901f\u5165\u95e8","text":""},{"location":"zh/#_6","title":"\u4e0a\u624b\u6559\u7a0b","text":"<p>\u5728\u5f00\u59cb\u4e0a\u624bMindCV\u524d\uff0c\u53ef\u4ee5\u9605\u8bfbMindCV\u7684\u5feb\u901f\u5f00\u59cb\uff0c\u8be5\u6559\u7a0b\u53ef\u4ee5\u5e2e\u52a9\u7528\u6237\u5feb\u901f\u4e86\u89e3MindCV\u7684\u5404\u4e2a\u91cd\u8981\u7ec4\u4ef6\u4ee5\u53ca\u8bad\u7ec3\u3001\u9a8c\u8bc1\u3001\u6d4b\u8bd5\u6d41\u7a0b\u3002</p> <p>\u4ee5\u4e0b\u662f\u4e00\u4e9b\u4f9b\u60a8\u5feb\u901f\u4f53\u9a8c\u7684\u4ee3\u7801\u6837\u4f8b\u3002</p> <pre><code>&gt;&gt;&gt; import mindcv\n# \u5217\u51fa\u6ee1\u8db3\u6761\u4ef6\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u540d\u79f0\n&gt;&gt;&gt; mindcv.list_models(\"swin*\", pretrained=True)\n['swin_tiny']\n# \u521b\u5efa\u6a21\u578b\n&gt;&gt;&gt; network = mindcv.create_model('swin_tiny', pretrained=True)\n# \u9a8c\u8bc1\u6a21\u578b\u7684\u51c6\u786e\u7387\n&gt;&gt;&gt; !python validate.py --model=swin_tiny --pretrained --dataset=imagenet --val_split=validation\n{'Top_1_Accuracy': 0.80824, 'Top_5_Accuracy': 0.94802, 'loss': 1.7331367141008378}\n</code></pre> \u56fe\u7247\u5206\u7c7b\u793a\u4f8b <p>\u53f3\u952e\u70b9\u51fb\u5982\u4e0b\u56fe\u7247\uff0c\u53e6\u5b58\u4e3a<code>dog.jpg</code>\u3002</p> <p><p> </p></p> <p>\u4f7f\u7528\u52a0\u8f7d\u4e86\u9884\u8bad\u7ec3\u53c2\u6570\u7684SoTA\u6a21\u578b\u5bf9\u56fe\u7247\u8fdb\u884c\u63a8\u7406\u3002</p> <pre><code>&gt;&gt;&gt; !python infer.py --model=swin_tiny --image_path='./dog.jpg'\n{'Labrador retriever': 0.5700152, 'golden retriever': 0.034551315, 'kelpie': 0.010108651, 'Chesapeake Bay retriever': 0.008229004, 'Walker hound, Walker foxhound': 0.007791956}\n</code></pre> <p>\u9884\u6d4b\u7ed3\u679c\u6392\u540d\u524d1\u7684\u662f\u62c9\u5e03\u62c9\u591a\u72ac\uff0c\u6b63\u662f\u8fd9\u5f20\u56fe\u7247\u91cc\u7684\u72d7\u72d7\u7684\u54c1\u79cd\u3002</p>"},{"location":"zh/#_7","title":"\u6a21\u578b\u8bad\u7ec3","text":"<p>\u901a\u8fc7<code>train.py</code>\uff0c\u7528\u6237\u53ef\u4ee5\u5f88\u5bb9\u6613\u5730\u5728\u6807\u51c6\u6570\u636e\u96c6\u6216\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u6a21\u578b\uff0c\u7528\u6237\u53ef\u4ee5\u901a\u8fc7\u5916\u90e8\u53d8\u91cf\u6216\u8005yaml\u914d\u7f6e\u6587\u4ef6\u6765\u8bbe\u7f6e\u8bad\u7ec3\u7b56\u7565\uff08\u5982\u6570\u636e\u589e\u5f3a\u3001\u5b66\u4e60\u7387\u7b56\u7565\uff09\u3002</p> <ul> <li> <p>\u5355\u5361\u8bad\u7ec3</p> <pre><code># \u5355\u5361\u8bad\u7ec3\npython train.py --model resnet50 --dataset cifar10 --dataset_download\n</code></pre> <p>\u4ee5\u4e0a\u4ee3\u7801\u662f\u5728CIFAR10\u6570\u636e\u96c6\u4e0a\u5355\u5361\uff08CPU/GPU/Ascend\uff09\u8bad\u7ec3ResNet\u7684\u793a\u4f8b\uff0c\u901a\u8fc7<code>model</code>\u548c<code>dataset</code>\u53c2\u6570\u5206\u522b\u6307\u5b9a\u9700\u8981\u8bad\u7ec3\u7684\u6a21\u578b\u548c\u6570\u636e\u96c6\u3002</p> </li> <li> <p>\u5206\u5e03\u5f0f\u8bad\u7ec3</p> <p>\u5bf9\u4e8e\u50cfImageNet\u8fd9\u6837\u7684\u5927\u578b\u6570\u636e\u96c6\uff0c\u6709\u5fc5\u8981\u5728\u591a\u4e2a\u8bbe\u5907\u4e0a\u4ee5\u5206\u5e03\u5f0f\u6a21\u5f0f\u8fdb\u884c\u8bad\u7ec3\u3002\u57fa\u4e8eMindSpore\u5bf9\u5206\u5e03\u5f0f\u76f8\u5173\u529f\u80fd\u7684\u826f\u597d\u652f\u6301\uff0c\u7528\u6237\u53ef\u4ee5\u4f7f\u7528<code>msrun</code>\u6765\u8fdb\u884c\u6a21\u578b\u7684\u5206\u5e03\u5f0f\u8bad\u7ec3\u3002</p> <pre><code># \u5206\u5e03\u5f0f\u8bad\u7ec3\n# \u5047\u8bbe\u4f60\u67094\u5f20NPU\u5361\nmsrun --bind_core=True --worker_num 4 python train.py --distribute \\\n    --model densenet121 --dataset imagenet --data_dir ./datasets/imagenet\n</code></pre> <p>\u6ce8\u610f\uff0c\u5982\u679c\u5728\u4e24\u5361\u73af\u5883\u4e0b\u9009\u7528msrun\u4f5c\u4e3a\u542f\u52a8\u65b9\u5f0f\uff0c\u8bf7\u6dfb\u52a0\u914d\u7f6e\u9879 <code>--bind_core=True</code> \u589e\u52a0\u7ed1\u6838\u64cd\u4f5c\u4ee5\u4f18\u5316\u4e24\u5361\u6027\u80fd\uff0c\u8303\u4f8b\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code>msrun --bind_core=True --worker_num=2--local_worker_num=2 --master_port=8118 \\\n--log_dir=msrun_log --join=True --cluster_time_out=300 \\\npython train.py --distribute --model=densenet121 --dataset=imagenet --data_dir=/path/to/imagenet\n</code></pre> </li> </ul> <p>\u5982\u9700\u66f4\u591a\u64cd\u4f5c\u6307\u5bfc\uff0c\u8bf7\u53c2\u8003 https://www.mindspore.cn/tutorials/experts/zh-CN/r2.3.1/parallel/startup_method.html</p> <pre><code>\u5b8c\u6574\u7684\u53c2\u6570\u5217\u8868\u53ca\u8bf4\u660e\u5728`config.py`\u4e2d\u5b9a\u4e49\uff0c\u53ef\u8fd0\u884c`python train.py --help`\u5feb\u901f\u67e5\u770b\u3002\n\n\u5982\u9700\u6062\u590d\u8bad\u7ec3\uff0c\u8bf7\u6307\u5b9a`--ckpt_path`\u548c`--ckpt_save_dir`\u53c2\u6570\uff0c\u811a\u672c\u5c06\u52a0\u8f7d\u8def\u5f84\u4e2d\u7684\u6a21\u578b\u6743\u91cd\u548c\u4f18\u5316\u5668\u72b6\u6001\uff0c\u5e76\u6062\u590d\u4e2d\u65ad\u7684\u8bad\u7ec3\u8fdb\u7a0b\u3002\n</code></pre> <ul> <li> <p>\u8d85\u53c2\u914d\u7f6e\u548c\u9884\u8bad\u7ec3\u7b56\u7565</p> <p>\u60a8\u53ef\u4ee5\u7f16\u5199yaml\u6587\u4ef6\u6216\u8bbe\u7f6e\u5916\u90e8\u53c2\u6570\u6765\u6307\u5b9a\u914d\u7f6e\u6570\u636e\u3001\u6a21\u578b\u3001\u4f18\u5316\u5668\u7b49\u7ec4\u4ef6\u53ca\u5176\u8d85\u53c2\u3002\u4ee5\u4e0b\u662f\u4f7f\u7528\u9884\u8bbe\u7684\u8bad\u7ec3\u7b56\u7565\uff08yaml\u6587\u4ef6\uff09\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u7684\u793a\u4f8b\u3002</p> <pre><code>msrun --bind_core=True --worker_num 4 python train.py -c configs/squeezenet/squeezenet_1.0_ascend.yaml\n</code></pre> <p>\u9884\u5b9a\u4e49\u7684\u8bad\u7ec3\u7b56\u7565</p> <p>MindCV\u76ee\u524d\u63d0\u4f9b\u4e86\u8d85\u8fc720\u79cd\u6a21\u578b\u8bad\u7ec3\u7b56\u7565\uff0c\u5728ImageNet\u53d6\u5f97SoTA\u6027\u80fd\u3002 \u5177\u4f53\u7684\u53c2\u6570\u914d\u7f6e\u548c\u8be6\u7ec6\u7cbe\u5ea6\u6027\u80fd\u6c47\u603b\u8bf7\u89c1<code>configs</code>\u6587\u4ef6\u5939\u3002 \u60a8\u53ef\u4ee5\u4fbf\u6377\u5730\u5c06\u8fd9\u4e9b\u8bad\u7ec3\u7b56\u7565\u7528\u4e8e\u60a8\u7684\u6a21\u578b\u8bad\u7ec3\u4e2d\u4ee5\u63d0\u9ad8\u6027\u80fd\uff08\u590d\u7528\u6216\u4fee\u6539\u76f8\u5e94\u7684yaml\u6587\u4ef6\u5373\u53ef\uff09\u3002</p> </li> <li> <p>\u5728ModelArts/OpenI\u5e73\u53f0\u4e0a\u8bad\u7ec3</p> <p>\u5728ModelArts\u6216OpenI\u4e91\u5e73\u53f0\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u9700\u8981\u6267\u884c\u4ee5\u4e0b\u64cd\u4f5c\uff1a</p> <pre><code>1\u3001\u5728\u4e91\u5e73\u53f0\u4e0a\u521b\u5efa\u65b0\u7684\u8bad\u7ec3\u4efb\u52a1\u3002\n2\u3001\u5728\u7f51\u7ad9UI\u754c\u9762\u6dfb\u52a0\u8fd0\u884c\u53c2\u6570`config`\uff0c\u5e76\u6307\u5b9ayaml\u914d\u7f6e\u6587\u4ef6\u7684\u8def\u5f84\u3002\n3\u3001\u5728\u7f51\u7ad9UI\u754c\u9762\u6dfb\u52a0\u8fd0\u884c\u53c2\u6570`enable_modelarts`\u5e76\u8bbe\u7f6e\u4e3aTrue\u3002\n4\u3001\u5728\u7f51\u7ad9\u4e0a\u586b\u5199\u5176\u4ed6\u8bad\u7ec3\u4fe1\u606f\u5e76\u542f\u52a8\u8bad\u7ec3\u4efb\u52a1\u3002\n</code></pre> </li> </ul> <p>\u9759\u6001\u56fe\u548c\u52a8\u6001\u56fe\u6a21\u5f0f</p> <p>\u5728\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u6a21\u578b\u8bad\u7ec3\uff08<code>train.py</code>\uff09\u5728MindSpore\u4e0a\u4ee5\u56fe\u6a21\u5f0f \u8fd0\u884c\uff0c\u8be5\u6a21\u5f0f\u5bf9\u4f7f\u7528\u9759\u6001\u56fe\u7f16\u8bd1\u5bf9\u6027\u80fd\u548c\u5e76\u884c\u8ba1\u7b97\u8fdb\u884c\u4e86\u4f18\u5316\u3002 \u76f8\u6bd4\u4e4b\u4e0b\uff0cpynative\u6a21\u5f0f\u7684\u4f18\u52bf\u5728\u4e8e\u7075\u6d3b\u6027\u548c\u6613\u4e8e\u8c03\u8bd5\u3002\u4e3a\u4e86\u65b9\u4fbf\u8c03\u8bd5\uff0c\u60a8\u53ef\u4ee5\u5c06\u53c2\u6570<code>--mode</code>\u8bbe\u4e3a1\u4ee5\u5c06\u8fd0\u884c\u6a21\u5f0f\u8bbe\u7f6e\u4e3a\u8c03\u8bd5\u6a21\u5f0f\u3002</p> <p>\u6df7\u5408\u6a21\u5f0f</p> <p>\u57fa\u4e8emindspore.jit\u7684\u6df7\u5408\u6a21\u5f0f \u662f\u517c\u987e\u4e86MindSpore\u7684\u6548\u7387\u548c\u7075\u6d3b\u7684\u6df7\u5408\u6a21\u5f0f\u3002\u7528\u6237\u53ef\u901a\u8fc7\u4f7f\u7528<code>train_with_func.py</code>\u6587\u4ef6\u6765\u4f7f\u7528\u8be5\u6df7\u5408\u6a21\u5f0f\u8fdb\u884c\u8bad\u7ec3\u3002</p> <pre><code>python train_with_func.py --model=resnet50 --dataset=cifar10 --dataset_download --epoch_size=10\n</code></pre> <p>\u6ce8\uff1a\u6b64\u4e3a\u8bd5\u9a8c\u6027\u8d28\u7684\u8bad\u7ec3\u811a\u672c\uff0c\u4ecd\u5728\u6539\u8fdb\uff0c\u5728MindSpore 1.8.1\u6216\u66f4\u65e9\u7248\u672c\u4e0a\u4f7f\u7528\u6b64\u6a21\u5f0f\u76ee\u524d\u5e76\u4e0d\u7a33\u5b9a\u3002</p>"},{"location":"zh/#_8","title":"\u6a21\u578b\u9a8c\u8bc1","text":"<p>\u4f7f\u7528<code>validate.py</code>\u53ef\u4ee5\u4fbf\u6377\u5730\u9a8c\u8bc1\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u3002</p> <pre><code># \u9a8c\u8bc1\u6a21\u578b\npython validate.py --model=resnet50 --dataset=imagenet --data_dir=/path/to/data --ckpt_path=/path/to/model.ckpt\n</code></pre> <p>\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u8fdb\u884c\u9a8c\u8bc1</p> <p>\u5f53\u9700\u8981\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u8ddf\u8e2a\u6a21\u578b\u5728\u6d4b\u8bd5\u96c6\u4e0a\u7cbe\u5ea6\u7684\u53d8\u5316\u65f6\uff0c\u8bf7\u542f\u7528\u53c2\u6570<code>--val_while_train</code>\uff0c\u5982\u4e0b</p> <pre><code>python train.py --model=resnet50 --dataset=cifar10 \\\n    --val_while_train --val_split=test --val_interval=1\n</code></pre> <p>\u5404\u8f6e\u6b21\u7684\u8bad\u7ec3\u635f\u5931\u548c\u6d4b\u8bd5\u7cbe\u5ea6\u5c06\u4fdd\u5b58\u5728<code>{ckpt_save_dir}/results.log</code>\u4e2d\u3002</p> <p>\u66f4\u591a\u8bad\u7ec3\u548c\u9a8c\u8bc1\u7684\u793a\u4f8b\u8bf7\u89c1\u793a\u4f8b\u3002</p>"},{"location":"zh/#_9","title":"\u6559\u7a0b","text":"<p>\u6211\u4eec\u63d0\u4f9b\u4e86\u7cfb\u5217\u6559\u7a0b\uff0c\u5e2e\u52a9\u7528\u6237\u5b66\u4e60\u5982\u4f55\u4f7f\u7528MindCV.</p> <ul> <li>\u4e86\u89e3\u6a21\u578b\u914d\u7f6e</li> <li>\u6a21\u578b\u63a8\u7406</li> <li>\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u4e0a\u7684\u6a21\u578b\u5fae\u8c03\u8bad\u7ec3</li> <li>\u5982\u4f55\u81ea\u5b9a\u4e49\u6a21\u578b //coming soon</li> <li>\u89c6\u89c9transformer\u6027\u80fd\u4f18\u5316 //coming soon</li> <li>\u90e8\u7f72\u63a8\u7406\u670d\u52a1</li> </ul>"},{"location":"zh/#_10","title":"\u652f\u6301\u7b97\u6cd5","text":"\u652f\u6301\u7b97\u6cd5\u5217\u8868  <ul> <li>\u6570\u636e\u589e\u5f3a<ul> <li>AutoAugment</li> <li>RandAugment</li> <li>Repeated Augmentation</li> <li>RandErasing (Cutout)</li> <li>CutMix</li> <li>MixUp</li> <li>RandomResizeCrop</li> <li>Color Jitter, Flip, etc</li> </ul> </li> <li>\u4f18\u5316\u5668<ul> <li>Adam</li> <li>AdamW</li> <li>Lion</li> <li>Adan (experimental)</li> <li>AdaGrad</li> <li>LAMB</li> <li>Momentum</li> <li>RMSProp</li> <li>SGD</li> <li>NAdam</li> </ul> </li> <li>\u5b66\u4e60\u7387\u8c03\u5ea6\u5668<ul> <li>Warmup Cosine Decay</li> <li>Step LR</li> <li>Polynomial Decay</li> <li>Exponential Decay</li> </ul> </li> <li>\u6b63\u5219\u5316<ul> <li>Weight Decay</li> <li>Label Smoothing</li> <li>Stochastic Depth (depends on networks)</li> <li>Dropout (depends on networks)</li> </ul> </li> <li>\u635f\u5931\u51fd\u6570<ul> <li>Cross Entropy (w/ class weight and auxiliary logit support)</li> <li>Binary Cross Entropy  (w/ class weight and auxiliary logit support)</li> <li>Soft Cross Entropy Loss (automatically enabled if mixup or label smoothing is used)</li> <li>Soft Binary Cross Entropy Loss (automatically enabled if mixup or label smoothing is used)</li> </ul> </li> <li>\u6a21\u578b\u878d\u5408<ul> <li>Warmup EMA (Exponential Moving Average)</li> </ul> </li> </ul>"},{"location":"zh/#_11","title":"\u8d21\u732e\u65b9\u5f0f","text":"<p>\u6b22\u8fce\u5f00\u53d1\u8005\u7528\u6237\u63d0issue\u6216\u63d0\u4ea4\u4ee3\u7801PR\uff0c\u6216\u8d21\u732e\u66f4\u591a\u7684\u7b97\u6cd5\u548c\u6a21\u578b\uff0c\u4e00\u8d77\u8ba9MindCV\u53d8\u5f97\u66f4\u597d\u3002</p> <p>\u6709\u5173\u8d21\u732e\u6307\u5357\uff0c\u8bf7\u53c2\u9605\u8d21\u732e\u3002 \u8bf7\u9075\u5faa\u6a21\u578b\u7f16\u5199\u6307\u5357\u6240\u89c4\u5b9a\u7684\u89c4\u5219\u6765\u8d21\u732e\u6a21\u578b\u63a5\u53e3\uff1a)</p>"},{"location":"zh/#_12","title":"\u8bb8\u53ef\u8bc1","text":"<p>\u672c\u9879\u76ee\u9075\u5faaApache License 2.0\u5f00\u6e90\u534f\u8bae\u3002</p>"},{"location":"zh/#_13","title":"\u81f4\u8c22","text":"<p>MindCV\u662f\u7531MindSpore\u56e2\u961f\u3001\u897f\u5b89\u7535\u5b50\u79d1\u6280\u5927\u5b66\u3001\u897f\u5b89\u4ea4\u901a\u5927\u5b66\u8054\u5408\u5f00\u53d1\u7684\u5f00\u6e90\u9879\u76ee\u3002 \u8877\u5fc3\u611f\u8c22\u6240\u6709\u53c2\u4e0e\u7684\u7814\u7a76\u4eba\u5458\u548c\u5f00\u53d1\u4eba\u5458\u4e3a\u8fd9\u4e2a\u9879\u76ee\u6240\u4ed8\u51fa\u7684\u52aa\u529b\u3002 \u5341\u5206\u611f\u8c22 OpenI \u5e73\u53f0\u6240\u63d0\u4f9b\u7684\u7b97\u529b\u8d44\u6e90\u3002</p>"},{"location":"zh/#_14","title":"\u5f15\u7528","text":"<p>\u5982\u679c\u4f60\u89c9\u5f97MindCV\u5bf9\u4f60\u7684\u9879\u76ee\u6709\u5e2e\u52a9\uff0c\u8bf7\u8003\u8651\u5f15\u7528\uff1a</p> <pre><code>@misc{MindSpore Computer Vision 2022,\n    title={{MindSpore Computer  Vision}:MindSpore Computer Vision Toolbox and Benchmark},\n    author={MindSpore Vision Contributors},\n    howpublished = {\\url{https://github.com/mindspore-lab/mindcv/}},\n    year={2022}\n}\n</code></pre>"},{"location":"zh/installation/","title":"\u5b89\u88c5","text":""},{"location":"zh/installation/#_1","title":"\u4f9d\u8d56","text":"<ul> <li>mindspore &gt;= 1.8.1</li> <li>numpy &gt;= 1.17.0</li> <li>pyyaml &gt;= 5.3</li> <li>tqdm</li> <li>openmpi 4.0.3 (\u5206\u5e03\u5f0f\u8bad\u7ec3\u6240\u9700)</li> </ul> <p>\u4e3a\u4e86\u5b89\u88c5<code>python</code>\u76f8\u5173\u5e93\u4f9d\u8d56\uff0c\u53ea\u9700\u8fd0\u884c\uff1a</p> <pre><code>pip install -r requirements.txt\n</code></pre> <p>Tip</p> <p>MindSpore\u53ef\u4ee5\u901a\u8fc7\u9075\u5faa\u5b98\u65b9\u6307\u5f15\uff0c\u5728\u4e0d\u540c\u7684\u786c\u4ef6\u5e73\u53f0\u4e0a\u83b7\u5f97\u6700\u4f18\u7684\u5b89\u88c5\u4f53\u9a8c\u3002 \u4e3a\u4e86\u5728\u5206\u5e03\u5f0f\u6a21\u5f0f\u4e0b\u8fd0\u884c\uff0c\u60a8\u8fd8\u9700\u8981\u5b89\u88c5OpenMPI\u3002</p> <p>\u5982\u4e0b\u7684\u6307\u5f15\u5047\u8bbe\u60a8\u5df2\u7ecf\u5b8c\u6210\u4e86\u6240\u6709\u4f9d\u8d56\u5e93\u7684\u5b89\u88c5\u3002</p>"},{"location":"zh/installation/#pypi","title":"PyPI\u6e90\u5b89\u88c5","text":"<p>MindCV\u88ab\u53d1\u5e03\u4e3a\u4e00\u4e2aPython\u5305\u5e76\u80fd\u591f\u901a\u8fc7<code>pip</code>\u8fdb\u884c\u5b89\u88c5\u3002\u6211\u4eec\u63a8\u8350\u60a8\u5728\u865a\u62df\u73af\u5883\u5b89\u88c5\u4f7f\u7528\u3002 \u6253\u5f00\u7ec8\u7aef\uff0c\u8f93\u5165\u4ee5\u4e0b\u6307\u4ee4\u6765\u5b89\u88c5MindCV:</p> stablenightly <pre><code>pip install mindcv\n</code></pre> <pre><code># \u6682\u4e0d\u652f\u6301\n</code></pre> <p>\u4e0a\u8ff0\u547d\u4ee4\u4f1a\u81ea\u52a8\u5b89\u88c5\u4f9d\u8d56\uff1aNumPy\uff0cPyYAML \u548c tqdm\u7684\u517c\u5bb9\u7248\u672c\u3002</p> <p>Tip</p> <p>\u5982\u679c\u60a8\u4e4b\u524d\u6ca1\u6709\u4f7f\u7528 Python \u7684\u7ecf\u9a8c\uff0c\u6211\u4eec\u5efa\u8bae\u60a8\u9605\u8bfb\u4f7f\u7528Python\u7684pip\u6765\u7ba1\u7406\u60a8\u7684\u9879\u76ee\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c \u8fd9\u662f\u5bf9 Python \u5305\u7ba1\u7406\u673a\u5236\u7684\u4e00\u4e2a\u5f88\u597d\u7684\u4ecb\u7ecd\uff0c\u5e76\u4e14\u53ef\u4ee5\u5e2e\u52a9\u60a8\u5728\u9047\u5230\u9519\u8bef\u65f6\u8fdb\u884c\u6545\u969c\u6392\u9664\u3002</p> <p>Warning</p> <p>\u4e0a\u8ff0\u547d\u4ee4 \u4e0d\u4f1a \u5b89\u88c5MindSpore. \u6211\u4eec\u5f3a\u70c8\u63a8\u8350\u60a8\u901a\u8fc7\u5b98\u65b9\u6307\u5f15\u6765\u5b89\u88c5MindSpore\u3002</p>"},{"location":"zh/installation/#_2","title":"\u6e90\u7801\u5b89\u88c5 (\u672a\u7ecf\u6d4b\u8bd5\u7248\u672c)","text":""},{"location":"zh/installation/#from-vsc","title":"from VSC","text":"<pre><code>pip install git+https://github.com/mindspore-lab/mindcv.git\n</code></pre>"},{"location":"zh/installation/#from-local-src","title":"from local src","text":"<p>Tip</p> <p>\u7531\u4e8e\u672c\u9879\u76ee\u5904\u4e8e\u6d3b\u8dc3\u5f00\u53d1\u9636\u6bb5\uff0c\u5982\u679c\u60a8\u662f\u5f00\u53d1\u8005\u6216\u8005\u8d21\u732e\u8005\uff0c\u8bf7\u4f18\u5148\u9009\u62e9\u6b64\u5b89\u88c5\u65b9\u5f0f\u3002</p> <p>MindCV\u53ef\u4ee5\u5728\u7531 GitHub \u514b\u9686\u4ed3\u5e93\u5230\u672c\u5730\u6587\u4ef6\u5939\u540e\u76f4\u63a5\u4f7f\u7528\u3002 \u8fd9\u5bf9\u4e8e\u60f3\u4f7f\u7528\u6700\u65b0\u7248\u672c\u7684\u5f00\u53d1\u8005\u5341\u5206\u65b9\u4fbf:</p> <pre><code>git clone https://github.com/mindspore-lab/mindcv.git\n</code></pre> <p>\u5728\u514b\u9686\u5230\u672c\u5730\u4e4b\u540e\uff0c\u63a8\u8350\u60a8\u4f7f\u7528\"\u53ef\u7f16\u8f91\"\u6a21\u5f0f\u8fdb\u884c\u5b89\u88c5\uff0c\u8fd9\u6709\u52a9\u4e8e\u89e3\u51b3\u6f5c\u5728\u7684\u6a21\u5757\u5bfc\u5165\u95ee\u9898\u3002</p> <pre><code>cd mindcv\npip install -e .\n</code></pre>"},{"location":"zh/modelzoo/","title":"\u6a21\u578b\u4ed3\u5e93","text":"performance tested on Ascend 910(8p) with graph mode model name params(M) cards batch size resolution jit level graph compile ms/step img/s acc@top1 acc@top5 recipe weight bit_resnet50 25.55 8 32 224x224 O2 146s 74.52 3413.33 76.81 93.17 yaml weights cmt_small 26.09 8 128 224x224 O2 1268s 500.64 2048.01 83.24 96.41 yaml weights coat_tiny 5.50 8 32 224x224 O2 543s 254.95 1003.92 79.67 94.88 yaml weights convit_tiny 5.71 8 256 224x224 O2 133s 231.62 8827.59 73.66 91.72 yaml weights convnext_tiny 28.59 8 16 224x224 O2 127s 66.79 1910.45 81.91 95.79 yaml weights convnextv2_tiny 28.64 8 128 224x224 O2 237s 400.20 2560.00 82.43 95.98 yaml weights crossvit_9 8.55 8 256 240x240 O2 206s 550.79 3719.30 73.56 91.79 yaml weights densenet121 8.06 8 32 224x224 O2 191s 43.28 5914.97 75.64 92.84 yaml weights dpn92 37.79 8 32 224x224 O2 293s 78.22 3272.82 79.46 94.49 yaml weights dpn92 37.79 8 32 224x224 O2 293s 78.22 3272.82 79.46 94.49 yaml weights efficientnet_b0 5.33 8 128 224x224 O2 203s 172.78 5926.61 76.89 93.16 yaml weights ghostnet_050 2.60 8 128 224x224 O2 383s 211.13 4850.09 66.03 86.64 yaml weights googlenet 6.99 8 32 224x224 O2 72s 21.40 11962.62 72.68 90.89 yaml weights halonet_50t 22.79 8 64 256x256 O2 261s 421.66 6437.82 79.53 94.79 yaml weights hrnet_w32 41.30 128 8 224x224 O2 1312s 279.10 3668.94 80.64 95.44 yaml weights inception_v3 27.20 8 32 299x299 O2 120s 76.42 3349.91 79.11 94.40 yaml weights inception_v4 42.74 8 32 299x299 O2 177s 76.19 3360.02 80.88 95.34 yaml weights mixnet_s 4.17 8 128 224x224 O2 556s 252.49 4055.61 75.52 92.52 yaml weights mnasnet_075 3.20 8 256 224x224 O2 140s 165.43 12379.86 71.81 90.53 yaml weights mobilenet_v1_025 0.47 8 64 224x224 O2 89s 42.43 12066.93 53.87 77.66 yaml weights mobilenet_v2_075 2.66 8 256 224x224 O2 164s 155.94 13133.26 69.98 89.32 yaml weights mobilenet_v3_small_100 2.55 8 75 224x224 O2 145s 48.14 12463.65 68.10 87.86 yaml weights mobilenet_v3_large_100 5.51 8 75 224x224 O2 271s 47.49 12634.24 75.23 92.31 yaml weights mobilevit_xx_small 1.27 64 8 256x256 O2 301s 53.52 9566.52 68.91 88.91 yaml weights nasnet_a_4x1056 5.33 8 256 224x224 O2 656s 330.89 6189.37 73.65 91.25 yaml weights pit_ti 4.85 8 128 224x224 O2 192s 271.50 3771.64 72.96 91.33 yaml weights poolformer_s12 11.92 8 128 224x224 O2 118s 220.13 4651.80 77.33 93.34 yaml weights pvt_tiny 13.23 8 128 224x224 O2 192s 229.63 4459.35 74.81 92.18 yaml weights pvt_v2_b0 3.67 8 128 224x224 O2 269s 269.38 3801.32 71.50 90.60 yaml weights regnet_x_800mf 7.26 8 64 224x224 O2 99s 42.49 12049.89 76.04 92.97 yaml weights repmlp_t224 38.30 8 128 224x224 O2 289s 578.23 1770.92 76.71 93.30 yaml weights repvgg_a0 9.13 8 32 224x224 O2 50s 20.58 12439.26 72.19 90.75 yaml weights repvgg_a1 14.12 8 32 224x224 O2 29s 20.70 12367.15 74.19 91.89 yaml weights res2net50 25.76 8 32 224x224 O2 119s 39.68 6451.61 79.35 94.64 yaml weights resnest50 27.55 8 128 224x224 O2 83s 244.92 4552.73 80.81 95.16 yaml weights resnet50 25.61 8 32 224x224 O2 43s 31.41 8150.27 76.69 93.50 yaml weights resnetv2_50 25.60 8 32 224x224 O2 52s 32.66 7838.33 76.90 93.37 yaml weights resnext50_32x4d 25.10 8 32 224x224 O2 49s 37.22 6878.02 78.53 94.10 yaml weights rexnet_09 4.13 8 64 224x224 O2 462s 130.10 3935.43 77.06 93.41 yaml weights seresnet18 11.80 8 64 224x224 O2 43s 44.40 11531.53 71.81 90.49 yaml weights shufflenet_v1_g3_05 0.73 8 64 224x224 O2 169s 40.62 12604.63 57.05 79.73 yaml weights shufflenet_v2_x0_5 1.37 8 64 224x224 O2 62s 41.87 12228.33 60.53 82.11 yaml weights skresnet18 11.97 8 64 224x224 O2 60s 45.84 11169.28 73.09 91.20 yaml weights squeezenet1_0 1.25 8 32 224x224 O2 45s 22.36 11449.02 58.67 80.61 yaml weights swin_tiny 33.38 8 256 224x224 O2 226s 454.49 4506.15 80.82 94.80 yaml weights swinv2_tiny_window8 28.78 8 128 256x256 O2 273s 317.19 3228.35 81.42 95.43 yaml weights vgg13 133.04 8 32 224x224 O2 23s 55.20 4637.68 72.87 91.02 yaml weights vgg19 143.66 8 32 224x224 O2 22s 67.42 3797.09 75.21 92.56 yaml weights visformer_tiny 10.33 8 128 224x224 O2 137s 217.92 4698.97 78.28 94.15 yaml weights volo_d1 27 8 128 224x224 O2 275s 270.79 3781.53 82.59 95.99 yaml weights xception 22.91 8 32 299x299 O2 161s 96.78 2645.17 79.01 94.25 yaml weights xcit_tiny_12_p16_224 7.00 8 128 224x224 O2 382s 252.98 4047.75 77.67 93.79 yaml weights performance tested on Ascend Atlas 800T A2 machines with graph mode model name params(M) cards batch size resolution jit level graph compile ms/step img/s acc@top1 acc@top5 recipe weight convit_tiny 5.71 8 256 224x224 O2 153s 226.51 9022.03 73.79 91.70 yaml weights convnext_tiny 28.59 8 16 224x224 O2 137s 48.7 2612.24 81.28 95.61 yaml weights convnextv2_tiny 28.64 8 128 224x224 O2 268s 257.2 3984.44 82.39 95.95 yaml weights crossvit_9 8.55 8 256 240x240 O2 221s 514.36 3984.44 73.38 91.51 yaml weights densenet121 8.06 8 32 224x224 O2 300s 47,34 5446.81 75.67 92.77 yaml weights densenet121 8.06 8 32 224x224 O2 300s 47,34 5446.81 75.67 92.77 yaml weights efficientnet_b0 5.33 8 128 224x224 O2 353s 172.64 5931.42 76.88 93.28 yaml weights googlenet 6.99 8 32 224x224 O2 113s 23.5 10893.62 72.89 90.89 yaml weights googlenet 6.99 8 32 224x224 O2 113s 23.5 10893.62 72.89 90.89 yaml weights inception_v3 27.20 8 32 299x299 O2 172s 70.83 3614.29 79.25 94.47 yaml weights inception_v4 42.74 8 32 299x299 O2 263s 80.97 3161.66 80.98 95.25 yaml weights mixnet_s 4.17 8 128 224x224 O2 706s 228.03 4490.64 75.58 95.54 yaml weights mnasnet_075 3.20 8 256 224x224 O2 144s 175.85 11646.29 71.77 90.52 yaml weights mobilenet_v1_025 0.47 8 64 224x224 O2 195s 47.47 10785.76 54.05 77.74 yaml weights mobilenet_v2_075 2.66 8 256 224x224 O2 233s 174.65 11726.31 69.73 89.35 yaml weights mobilenet_v3_small_100 2.55 8 75 224x224 O2 184s 52.38 11454.75 68.07 87.77 yaml weights mobilenet_v3_large_100 5.51 8 75 224x224 O2 354s 55.89 10735.37 75.59 92.57 yaml weights mobilevit_xx_small 1.27 8 64 256x256 O2 437s 67.24 7614.52 67.11 87.85 yaml weights nasnet_a_4x1056 5.33 8 256 224x224 O2 800s 364.35 5620.97 74.12 91.36 yaml weights pit_ti 4.85 8 128 224x224 O2 212s 266.47 3842.83 73.26 91.57 yaml weights poolformer_s12 11.92 8 128 224x224 O2 177s 211.81 4834.52 77.49 93.55 yaml weights pvt_tiny 13.23 8 128 224x224 O2 212s 237.5 4311.58 74.88 92.12 yaml weights pvt_v2_b0 3.67 8 128 224x224 O2 323s 255.76 4003.75 71.25 90.50 yaml weights regnet_x_800mf 7.26 8 64 224x224 O2 228s 50.74 10090.66 76.11 93.00 yaml weights repmlp_t224 38.30 8 128 224x224 O2 289s 578.23 1770.92 76.71 93.30 yaml weights repvgg_a0 9.13 8 32 224x224 O2 76s 24.12 10613.60 72.29 90.78 yaml weights repvgg_a1 14.12 8 32 224x224 O2 81s 28.29 9096.13 73.68 91.51 yaml weights res2net50 25.76 8 32 224x224 O2 174s 39.6 6464.65 79.33 94.64 yaml weights resnet50 25.61 8 32 224x224 O2 77s 31.9 8025.08 76.76 93.31 yaml weights resnetv2_50 25.60 8 32 224x224 O2 120s 32.19 7781.16 77.03 93.29 yaml weights resnext50_32x4d 25.10 8 32 224x224 O2 156s 44.61 5738.62 78.64 94.18 yaml weights rexnet_09 4.13 8 64 224x224 O2 515s 115.61 3290.28 76.14 92.96 yaml weights seresnet18 11.80 8 64 224x224 O2 90s 51.09 10021.53 72.05 90.59 yaml weights shufflenet_v1_g3_05 0.73 8 64 224x224 O2 191s 47.77 10718.02 57.08 79.89 yaml weights shufflenet_v2_x0_5 1.37 8 64 224x224 O2 100s 47.32 10819.95 60.65 82.26 yaml weights skresnet18 11.97 8 64 224x224 O2 134s 49.83 10274.93 72.85 90.83 yaml weights squeezenet1_0 1.25 8 32 224x224 O2 64s 23.48 10902.90 58.75 80.76 yaml weights swin_tiny 33.38 8 256 224x224 O2 266s 466.6 4389.20 80.90 94.90 yaml weights swinv2_tiny_window8 28.78 8 128 256x256 O2 385s 335.18 3055.07 81.38 95.46 yaml weights vgg13 133.04 8 32 224x224 O2 41s 30.52 8387.94 72.81 91.02 yaml weights vgg19 143.66 8 32 224x224 O2 53s 39.17 6535.61 75.24 92.55 yaml weights visformer_tiny 10.33 8 128 224x224 O2 169s 201.14 5090.98 78.40 94.30 yaml weights xcit_tiny_12_p16_224 7.00 8 128 224x224 O2 330s 229.25 4466.74 77.27 93.56 yaml weights"},{"location":"zh/how_to_guides/feature_extraction/","title":"\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6","text":"<p>\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u60a8\u5c06\u5b66\u4e60\u5982\u4f55\u5bf9MindCV\u4e2d\u7684\u6a21\u578b\u8fdb\u884c\u591a\u5c3a\u5ea6\u7279\u5f81\u62bd\u53d6\u3002 \u5728\u5b9e\u9645\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u9879\u76ee\u4e2d\uff0c\u6211\u4eec\u7ecf\u5e38\u4f7f\u7528\u7ecf\u5178\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\uff0c\u5982ResNet\u3001VGG\uff0c\u4ee5\u83b7\u5f97\u66f4\u597d\u7684\u6027\u80fd\u548c\u66f4\u5feb\u7684\u5f00\u53d1\u8fdb\u5ea6\u3002\u901a\u5e38\u60c5\u51b5\u4e0b\uff0c\u4ec5\u4f7f\u7528\u9aa8\u5e72\u7f51\u7edc\u7684\u6700\u7ec8\u8f93\u51fa\u5f80\u5f80\u662f\u4e0d\u591f\u7684\u3002 \u6211\u4eec\u9700\u8981\u6765\u81ea\u4e2d\u95f4\u5c42\u7684\u8f93\u51fa\uff0c\u5b83\u4eec\u4f5c\u4e3a\u8f93\u5165\u7684\u591a\u5c3a\u5ea6\u62bd\u8c61\uff0c\u53ef\u4ee5\u5e2e\u52a9\u8fdb\u4e00\u6b65\u63d0\u5347\u6211\u4eec\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\u3002 \u4e3a\u6b64\uff0c\u6211\u4eec\u5728MindCV\u4e2d\u8bbe\u8ba1\u4e86\u4e00\u79cd\u4ece\u9aa8\u5e72\u7f51\u7edc\u4e2d\u62bd\u53d6\u591a\u5c3a\u5ea6\u7279\u5f81\u7684\u673a\u5236\u3002\u5728\u7f16\u5199\u672c\u6559\u7a0b\u65f6\uff0cMindCV\u5df2\u7ecf\u652f\u6301\u4eceResNet\u3001MobileNetV3\u3001ConvNeXt\u3001ResNeST\u3001EfficientNet\u3001RepVGG\u3001HRNet\u548cReXNet\u4e2d\u4f7f\u7528\u8fd9\u79cd\u673a\u5236\u62bd\u53d6\u7279\u5f81\u3002\u6709\u5173\u7279\u5f81\u62bd\u53d6\u673a\u5236\u7684\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605<code>FeatureExtractWrapper</code>.</p> <p>\u672c\u6559\u7a0b\u5c06\u5e2e\u52a9\u60a8\u5b66\u4e60\u5982\u4f55\u6dfb\u52a0\u4e00\u4e9b\u4ee3\u7801\u6765\u4ece\u5176\u4f59\u9aa8\u5e72\u7f51\u7edc\u4e2d\u62bd\u53d6\u591a\u5c3a\u5ea6\u7279\u5f81\u3002\u4e3b\u8981\u6709\u4e24\u4e2a\u6b65\u9aa4\uff1a</p> <ol> <li>\u5728\u6a21\u578b\u7684<code>__init__()</code>\u4e2d\uff0c\u6ce8\u518c\u9700\u8981\u63d0\u53d6\u8f93\u51fa\u7279\u5f81\u7684\u4e2d\u95f4\u5c42\uff0c\u5c06\u5176\u6dfb\u52a0\u5230<code>self.feature_info</code>\u4e2d\u3002</li> <li>\u6dfb\u52a0\u4e00\u4e2a\u7528\u4e8e\u6a21\u578b\u521b\u5efa\u7684\u5c01\u88c5\u51fd\u6570\u3002</li> <li>\u5c06\u53c2\u6570<code>feature_only=True</code>\u548c<code>out_indices</code>\u4f20\u5165<code>create_model()</code>.</li> </ol>"},{"location":"zh/how_to_guides/feature_extraction/#_2","title":"\u6ce8\u518c\u4e2d\u95f4\u5c42","text":"<p>\u5728MindCV\u4e2d\u5b9e\u73b0\u7279\u5f81\u62bd\u53d6\u4ee3\u7801\u65f6\uff0c\u4e3b\u8981\u6709\u4e09\u79cd\u6a21\u578b\u5b9e\u73b0\u60c5\u51b5\uff0c\u5373\uff1a * \u5bf9\u4e8e\u6bcf\u4e2a\u4e2d\u95f4\u5c42\uff0c\u6a21\u578b\u90fd\u6709\u5355\u72ec\u5bf9\u5e94\u7684\u987a\u5e8f\u6a21\u5757\u3002 * \u6a21\u578b\u6240\u6709\u5c42\u90fd\u5305\u542b\u5728\u540c\u4e00\u4e2a\u987a\u5e8f\u6a21\u5757\u4e2d\u3002 * \u6a21\u578b\u4e2d\u95f4\u5c42\u662f\u975e\u987a\u5e8f\u6a21\u5757\u3002</p>"},{"location":"zh/how_to_guides/feature_extraction/#1","title":"\u573a\u666f1\uff1a\u6bcf\u4e2a\u4e2d\u95f4\u5c42\u6709\u5355\u72ec\u5bf9\u5e94\u7684\u987a\u5e8f\u6a21\u5757","text":"<p>\u573a\u666f1\u7684\u793a\u4f8b\u5982\u4e0b\u3002</p> <pre><code>class DummyNet1(nn.Cell):\n    def __init__(self, **kwargs):\n        super().__init__()\n\n        # \u6bcf\u4e2a\u5c42\u90fd\u6709\u5355\u72ec\u7684\u987a\u5e8f\u6a21\u5757\n        self.layer1 = Layer()\n        self.layer2 = Layer()\n        self.layer3 = Layer()\n        self.layer4 = Layer()\n\n    def forward_features(self, x):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\n    def construct(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre> <p>\u4e3a\u4e86\u5b9e\u73b0\u573a\u666f1\u7684\u7279\u5f81\u62bd\u53d6\uff0c\u6211\u4eec\u5728<code>__init__()</code>\u4e2d\u6dfb\u52a0\u4e86\u6210\u5458\u53d8\u91cf<code>self.feature_info</code>\u7528\u4e8e\u6ce8\u518c\u53ef\u63d0\u53d6\u7684\u4e2d\u95f4\u5c42\uff0c\u4f8b\u5982\uff0c</p> <pre><code>class DummyNet1(nn.Cell):\n    def __init__(self, **kwargs):\n        super().__init__()\n        self.feature_info = []   # \u7528\u4e8e\u4e2d\u95f4\u5c42\u6ce8\u518c\n\n        self.layer1 = Layer()\n        self.feature_info.append(dict(chs=, reduction=, name=\u201dlayer1\u201d))  # \u6ce8\u518c\u4e2d\u95f4\u5c42\n        self.layer2 = Layer()\n        self.feature_info.append(dict(chs=, reduction=, name=\u201dlayer2\u201d))  # \u6ce8\u518c\u4e2d\u95f4\u5c42\n        self.layer3 = Layer()\n        self.feature_info.append(dict(chs=, reduction=, name=\u201dlayer3\u201d))  # \u6ce8\u518c\u4e2d\u95f4\u5c42\n        self.layer4 = Layer()\n        self.feature_info.append(dict(chs=, reduction=, name=\u201dlayer4\u201d))  # \u6ce8\u518c\u4e2d\u95f4\u5c42\n</code></pre> <p>\u5982\u4e0a\u6240\u793a\uff0c<code>self.feature_info</code>\u662f\u4e00\u4e2a\u5305\u542b\u591a\u4e2a\u5b57\u5178\u7684\u5217\u8868\uff0c\u6bcf\u4e2a\u5b57\u5178\u5305\u542b\u4e09\u5bf9\u952e\u503c\u5bf9\u3002\u5177\u4f53\u800c\u8a00, <code>chs</code> \u8868\u793a\u751f\u6210\u7279\u5f81\u7684\u901a\u9053\u6570\uff0c<code>reduction</code>\u8868\u793a\u5f53\u524d\u5c42\u7684\u603b<code>stide</code>\u6570\uff0c<code>name</code>\u8868\u793a\u8be5\u4e2d\u95f4\u5c42\u5728\u6a21\u578b\u53c2\u6570\u4e2d\u7684\u540d\u79f0\u3002 \u6b64\u540d\u79f0\u53ef\u7528<code>get_parameters()</code>\u627e\u5230\u3002</p> <p>\u6709\u5173\u6b64\u573a\u666f\u7684\u771f\u5b9e\u793a\u4f8b\uff0c\u8bf7\u53c2\u9605ResNet\u3002</p>"},{"location":"zh/how_to_guides/feature_extraction/#2","title":"\u573a\u666f2\uff1a\u6240\u6709\u4e2d\u95f4\u5c42\u5728\u540c\u4e00\u4e2a\u987a\u5e8f\u6a21\u5757\u4e2d","text":"<p>\u5bf9\u4e8e\u67d0\u4e9b\u6a21\u578b\uff0c\u6240\u6709\u4e2d\u95f4\u5c42\u90fd\u88ab\u5305\u542b\u5728\u540c\u4e00\u4e2a\u987a\u5e8f\u6a21\u5757\u4e2d\u3002\u573a\u666f2\u7684\u793a\u4f8b\u5982\u4e0b\u3002</p> <pre><code>class DummyNet2(nn.Cell):\n    def __init__(self, **kwargs):\n        super().__init__()\n        layers = []\n\n        for i in range(4):\n            layers.append(Layer())\n\n        # \u6240\u6709\u4e2d\u95f4\u5c42\u5728\u540c\u4e00\u4e2a\u987a\u5e8f\u6a21\u5757\u4e2d\n        self.layers = nn.SequentialCell(layers)\n\n    def forward_features(self, x):\n        x = self.layers(x)\n        return x\n\n    def construct(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre> <p>\u4e3a\u4e86\u5b9e\u73b0\u573a\u666f2\u7684\u7279\u5f81\u62bd\u53d6\uff0c\u6211\u4eec\u540c\u6837\u9700\u8981\u5728<code>__init__()</code>\u4e2d\u6dfb\u52a0<code>self.feature_info</code>\uff0c\u540c\u65f6\u8fd8\u8981\u521b\u5efa\u4e00\u4e2a\u6210\u5458\u53d8\u91cf<code>self.flatten_sequential = True</code>\uff0c\u7528\u4e8e\u8868\u793a\u5728\u63d0\u53d6\u7279\u5f81\u4e4b\u524d\u9700\u8981\u5c06\u6b64\u6a21\u578b\u4e2d\u7684\u987a\u5e8f\u6a21\u5757\u5c55\u5f00\uff0c\u4f8b\u5982\uff0c</p> <pre><code>class DummyNet2(nn.Cell):\n    def __init__(self, **kwargs):\n        super().__init__()\n        self.feature_info = []  # \u7528\u4e8e\u4e2d\u95f4\u5c42\u6ce8\u518c\n        self.flatten_sequential = True  # \u8868\u793a\u9700\u8981\u5c55\u5f00\u987a\u5e8f\u6a21\u5757\n\n        layers = []\n\n        for i in range(4):\n            layers.append(Layer())\n            self.feature_info.append(dict(chs=, reduction=, name=f\u201dlayer{i}\u201d))  # \u6ce8\u518c\u4e2d\u95f4\u5c42\n\n        self.layers = nn.SequentialCell(layers)\n</code></pre> <p>\u8bf7\u6ce8\u610f\uff0c<code>__init__()</code>\u4e2d\u6a21\u5757\u5b9e\u4f8b\u5316\u7684\u987a\u5e8f\u975e\u5e38\u91cd\u8981\u3002\u987a\u5e8f\u5fc5\u987b\u4e0e\u5728<code>forward_features()</code>\u548c<code>construct()</code>\u4e2d\u8c03\u7528\u8fd9\u4e9b\u6a21\u5757\u7684\u987a\u5e8f\u4fdd\u6301\u4e00\u81f4\u3002\u6b64\u5916\uff0c\u53ea\u6709\u5728<code>forward_features()</code>\u548c<code>construct()</code>\u4e2d\u88ab\u8c03\u7528\u7684<code>nn.Cell</code>\u7c7b\u578b\u6a21\u5757\uff0c\u624d\u80fd\u4f5c\u4e3a\u6210\u5458\u53d8\u91cf\u88ab\u5b9e\u4f8b\u5316\u3002\u5426\u5219\uff0c\u7279\u5f81\u62bd\u53d6\u673a\u5236\u5c06\u65e0\u6cd5\u6b63\u5e38\u5de5\u4f5c\u3002</p> <p>\u6709\u5173\u6b64\u573a\u666f\u7684\u771f\u5b9e\u793a\u4f8b\uff0c\u8bf7\u53c2\u9605MobileNetV3\u3002</p>"},{"location":"zh/how_to_guides/feature_extraction/#3","title":"\u573a\u666f3\uff1a\u6a21\u578b\u4e2d\u95f4\u5c42\u4e3a\u975e\u987a\u5e8f\u6a21\u5757","text":"<p>\u6a21\u578b\u4e2d\u95f4\u5c42\u6709\u65f6\u662f\u975e\u987a\u5e8f\u6a21\u5757\u3002\u573a\u666f3\u7684\u793a\u4f8b\u5982\u4e0b\u3002</p> <pre><code>class DummyNet3(nn.Cell):\n    def __init__(self, **kwargs):\n        super().__init__()\n        layer1 = []\n\n        for i in range(3):\n                layer1.append(Layer())\n\n        # self.layer1 \u4e2d\u7684\u5c42\u4e0d\u662f\u987a\u5e8f\u6a21\u5757\n        self.layer1 = nn.CellList(layer1)\n\n        self.stage1 = Stage()\n\n        layer2 = []\n\n        for i in range(3):\n                layer2.append(Layer())\n\n        # self.layer2 \u4e2d\u7684\u5c42\u4e0d\u662f\u987a\u5e8f\u6a21\u5757\n        self.layer2 = nn.CellList(layer2)\n\n        self.stage2 = Stage()\n\n    def forward_features(self, x):\n        x_list = []\n\n        # \u4e2d\u95f4\u5c42\u662f\u5e76\u884c\u7684\uff0c\u800c\u4e0d\u662f\u987a\u5e8f\u7684\n        for i in range(3):\n                x_list.append(self.layer1[i](x))\n\n        x = self.stage1(x_list)\n\n        x_list = []\n\n        # \u4e2d\u95f4\u5c42\u662f\u5e76\u884c\u7684\uff0c\u800c\u4e0d\u662f\u987a\u5e8f\u7684\n        for i in range(3):\n                x_list.append(self.layer2[i](x))\n\n        x = self.stage2(x_list)\n\n        return x\n\n    def construct(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre> <p>\u4e3a\u4e86\u5b9e\u73b0\u573a\u666f3\u7684\u7279\u5f81\u62bd\u53d6\uff0c\u6211\u4eec\u9700\u8981\u9996\u5148\u901a\u8fc7\u7ee7\u627f\u539f\u59cb\u6a21\u578b\u7c7b\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u6a21\u578b\u7279\u5f81\u7c7b\u3002\u7136\u540e\uff0c\u5728<code>__init__()</code>\u4e2d\u6dfb\u52a0<code>self.feature_info</code>\u548c\u4e00\u4e2a\u6210\u5458\u53d8\u91cf<code>self.is_rewritten = True</code>\uff0c\u4ee5\u6307\u793a\u8be5\u7c7b\u662f\u4e3a\u7279\u5f81\u62bd\u53d6\u800c\u91cd\u5199\u7684\u3002\u6700\u540e\uff0c\u6211\u4eec\u4f7f\u7528\u7279\u5f81\u62bd\u53d6\u903b\u8f91\u91cd\u65b0\u5b9e\u73b0<code>forward_features()</code>\u548c<code>construct()</code>\u3002\u4ee5\u4e0b\u662f\u4e00\u4e2a\u793a\u4f8b\u3002</p> <pre><code>class DummyFeatureNet3(DummyNet3):\n    def __init__(self, **kwargs):\n        super(DummyFeatureNet3, self).__init__(**kwargs)\n        self.feature_info = []  # \u7528\u4e8e\u4e2d\u95f4\u5c42\u6ce8\u518c\n        self.is_rewritten = True  # \u8868\u793a\u4e3a\u7279\u5f81\u62bd\u53d6\u800c\u91cd\u5199\u7684\n\n        self.feature_info.append(dict(chs=, reduction=, name=\u201dstage1\u201d)  # \u6ce8\u518c\u4e2d\u95f4\u5c42\n        self.feature_info.append(dict(chs=, reduction=, name=\u201dstage2\u201d)  # \u6ce8\u518c\u4e2d\u95f4\u5c42\n\n    def forward_features(self, x):  # \u91cd\u65b0\u5b9e\u73b0\u7279\u5f81\u62bd\u53d6\u903b\u8f91\n        out = []\n        x_list = []\n\n        for i in range(3):\n                x_list.append(self.layer1[i](x))\n\n        x = self.stage1(x_list)\n        out.append(x)\n\n        x_list = []\n\n        for i in range(3):\n                x_list.append(self.layer2[i](x))\n\n        x = self.stage2(x_list)\n        out.append(x)\n\n        return out\n\n    def construct(self, x):\n        x = self.forward_features(x)\n        return x\n</code></pre> <p>\u6709\u5173\u6b64\u60c5\u51b5\u7684\u771f\u5b9e\u793a\u4f8b\uff0c\u8bf7\u53c2\u9605HRNet\u3002</p>"},{"location":"zh/how_to_guides/feature_extraction/#_3","title":"\u6dfb\u52a0\u6a21\u578b\u521b\u5efa\u7684\u5c01\u88c5\u51fd\u6570","text":"<p>\u5728\u6dfb\u52a0\u4e2d\u95f4\u5c42\u6ce8\u518c\u540e\uff0c\u6211\u4eec\u9700\u8981\u518d\u6dfb\u52a0\u4e00\u4e2a\u7b80\u5355\u7684\u6a21\u578b\u521b\u5efa\u5c01\u88c5\u51fd\u6570\uff0c\u4ee5\u4fbf\u5c06\u6a21\u578b\u5b9e\u4f8b\u4f20\u9012\u7ed9<code>build_model_with_cfg()</code>\u8fdb\u884c\u7279\u5f81\u62bd\u53d6\u3002</p> <p>\u901a\u5e38\uff0cMindCV\u4e2d\u6a21\u578b\u7684\u539f\u59cb\u521b\u5efa\u51fd\u6570\u5982\u4e0b\u6240\u793a\uff1a</p> <pre><code>@register_model\ndef dummynet18(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; DummyNet:\n    default_cfg = default_cfgs[\"dummynet18\"]\n    model = DummyNet(..., num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <p>\u5bf9\u4e8e\u5c5e\u4e8e\u573a\u666f1\u548c\u573a\u666f2\u7684\u6a21\u578b\uff0c\u5728\u6dfb\u52a0\u7684\u6a21\u578b\u521b\u5efa\u5c01\u88c5\u51fd\u6570\u4e2d\uff0c\u53ea\u9700\u5c06\u539f\u6765\u7684\u53c2\u6570\u4f20\u7ed9<code>build_model_with_cfg()</code>\u5373\u53ef\uff0c\u4f8b\u5982\uff0c</p> <pre><code>def _create_dummynet(pretrained=False, **kwargs):\n    return build_model_with_cfg(DummyNet, pretrained, **kwargs)\n\n@register_model\ndef dummynet18(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; DummyNet:\n    default_cfg = default_cfgs[\"dummynet18\"]\n    model_args = dict(..., num_classes=num_classes, in_channels=in_channels, **kwargs)\n    return _create_dummynet(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre> <p>\u5bf9\u4e8e\u5c5e\u4e8e\u573a\u666f3\u7684\u6a21\u578b\uff0c\u5c01\u88c5\u51fd\u6570\u5927\u81f4\u4e0e\u573a\u666f1\u548c\u573a\u666f2\u7c7b\u4f3c\u3002\u4e0d\u540c\u4e4b\u5904\u5728\u4e8e\u9700\u8981\u51b3\u5b9a\u5b9e\u4f8b\u5316\u54ea\u4e2a\u6a21\u578b\u7c7b\uff0c\u8fd9\u53d6\u51b3\u4e8e<code>feature_only</code>\u3002\u4f8b\u5982\uff0c</p> <pre><code>def _create_dummynet(pretrained=False, **kwargs):\n    if not kwargs.get(\"features_only\", False):\n        return build_model_with_cfg(DummyNet3, pretrained, **kwargs)\n    else:\n        return build_model_with_cfg(DummyFeatureNet3, pretrained, **kwargs)\n\n@register_model\ndef dummynet18(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; DummyNet:\n    default_cfg = default_cfgs[\"dummynet18\"]\n    model_args = dict(..., num_classes=num_classes, in_channels=in_channels, **kwargs)\n    return _create_dummynet(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre> <p>\u6709\u5173\u5b9e\u9645\u793a\u4f8b\uff0c\u8bf7\u53c2\u9605ResNet\u4e0eHRNet\u3002</p>"},{"location":"zh/how_to_guides/feature_extraction/#create_model","title":"<code>create_model()</code>\u4f20\u5165\u53c2\u6570","text":"<p>\u5b8c\u6210\u524d\u9762\u4e24\u4e2a\u6b65\u9aa4\u540e\uff0c\u6211\u4eec\u53ef\u5c06<code>feature_only=True</code>\u548c<code>out_indices</code>\u4f20\u9012\u7ed9<code>create_model()</code>\u6765\u521b\u5efa\u53ef\u8f93\u51fa\u6240\u9700\u7279\u5f81\u7684\u9aa8\u5e72\u7f51\u7edc\uff0c\u4f8b\u5982\uff0c</p> <pre><code>from mindcv.models import create_model\n\n\nbackbone = create_model(\n    model_name=\"dummynet18\",\n    features_only=True,  # \u8bbe\u7f6efeatures_only\u4e3a True\n    out_indices=[0, 1, 2],  # \u6307\u5b9a\u7279\u5f81\u62bd\u53d6\u7684\u4e2d\u95f4\u5c42\u5728feature_info\u4e2d\u7684\u7d22\u5f15\n)\n</code></pre> <p>\u6b64\u5916\uff0c\u5982\u679c\u6211\u4eec\u60f3\u8981\u5c06checkpoint\u52a0\u8f7d\u5230\u7528\u4e8e\u7279\u5f81\u62bd\u53d6\u7684\u9aa8\u5e72\u7f51\u7edc\u4e2d\uff0c\u5e76\u4e14\u6b64\u9aa8\u5e72\u7f51\u7edc\u5c5e\u4e8e\u573a\u666f2\uff0c\u90a3\u4e48\uff0c\u6211\u4eec\u8fd8\u9700\u8bbe\u7f6e<code>auto_mapping=True</code>\uff0c\u4f8b\u5982\uff0c</p> <pre><code>from mindcv.models import create_model\n\n\nbackbone = create_model(\n    model_name=\"dummynet18\",\n    checkpoint_path=\"/path/to/dummynet18.ckpt\",\n    auto_mapping=True,  # \u5f53\u4e3a\u573a\u666f2\u7684\u6a21\u578b\u52a0\u8f7dcheckpoint\u65f6\uff0c\u5c06auto_mapping\u8bbe\u4e3aTrue\n    features_only=True,  # \u8bbe\u7f6efeatures_only\u4e3a True\n    out_indices=[0, 1, 2],  # \u6307\u5b9a\u7279\u5f81\u62bd\u53d6\u7684\u4e2d\u95f4\u5c42\u5728feature_info\u4e2d\u7684\u7d22\u5f15\n)\n</code></pre> <p>\u606d\u559c\u60a8\uff01\u73b0\u5728\u60a8\u5df2\u7ecf\u5b66\u4f1a\u4e86\u5982\u4f55\u5bf9MindCV\u4e2d\u7684\u6a21\u578b\u8fdb\u884c\u591a\u5c3a\u5ea6\u7279\u5f81\u62bd\u53d6\u3002</p>"},{"location":"zh/how_to_guides/finetune_with_a_custom_dataset/","title":"\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u7684\u6a21\u578b\u5fae\u8c03\u6307\u5357","text":"<p>\u672c\u6587\u6863\u63d0\u4f9b\u4e86\u4f7f\u7528MindCV\u5728\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u7684\u53c2\u8003\u6d41\u7a0b\u4ee5\u53ca\u5728\u7ebf\u8bfb\u53d6\u6570\u636e\u96c6\u3001\u5206\u5c42\u8bbe\u7f6e\u5b66\u4e60\u7387\u3001\u51bb\u7ed3\u90e8\u5206\u7279\u5f81\u7f51\u7edc\u7b49\u5fae\u8c03\u6280\u5de7\u7684\u5b9e\u73b0\u65b9\u6cd5\uff0c\u4e3b\u8981\u4ee3\u7801\u5b9e\u73b0\u96c6\u6210\u5728./example/finetune.py\u4e2d\uff0c\u60a8\u53ef\u4ee5\u57fa\u4e8e\u6b64\u6559\u7a0b\u6839\u636e\u9700\u8981\u81ea\u884c\u6539\u52a8\u3002</p> <p>\u63a5\u4e0b\u6765\u5c06\u4ee5FGVC-Aircraft\u6570\u636e\u96c6\u4e3a\u4f8b\u5c55\u793a\u5982\u4f55\u5bf9\u9884\u8bad\u7ec3\u6a21\u578bmobilenet v3-small\u8fdb\u884c\u5fae\u8c03\u3002Fine-Grained Visual Classification of Aircraft\u662f\u5e38\u7528\u7684\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b 10000 \u5f20\u98de\u673a\u56fe\u7247\uff0c100 \u79cd\u4e0d\u540c\u7684\u98de\u673a\u578b\u53f7(variant)\uff0c\u5176\u4e2d\u6bcf\u79cd\u98de\u673a\u578b\u53f7\u5747\u6709 100 \u5f20\u56fe\u7247\u3002</p> <p>\u9996\u5148\u5c06\u4e0b\u8f7d\u540e\u7684\u6570\u636e\u96c6\u89e3\u538b\u5230./data\u6587\u4ef6\u5939\u4e0b\uff0cAircraft\u6570\u636e\u96c6\u7684\u76ee\u5f55\u4e3a\uff1a</p> <pre><code>aircraft\n\u2514\u2500\u2500 data\n    \u251c\u2500\u2500 images\n    \u2502   \u251c\u2500\u2500 image1.jpg\n    \u2502   \u251c\u2500\u2500 image2.jpg\n    \u2502   \u2514\u2500\u2500 ....\n    \u251c\u2500\u2500 images_variant_test.txt\n    \u251c\u2500\u2500 images_variant_trainval.txt\n    \u2514\u2500\u2500 ....\n</code></pre> <p>\u5176\u4e2dimages\u6587\u4ef6\u5939\u5305\u542b\u5168\u90e810000\u5f20\u56fe\u7247\uff0c\u6bcf\u5f20\u56fe\u7247\u6240\u5c5e\u7684\u98de\u673a\u578b\u53f7\u548c\u5b50\u96c6\u7531images_variant_*.txt\u6807\u6ce8\u3002\u5728\u6a21\u578b\u5fae\u8c03\u9636\u6bb5\uff0c\u8bad\u7ec3\u96c6\u4e00\u822c\u7531images_variant_trainval.txt \u786e\u5b9a\u3002\u7ecf\u8fc7\u62c6\u5206\u540e\uff0c\u8bad\u7ec3\u96c6\u5e94\u5f53\u5305\u542b6667\u5f20\u56fe\u7247\uff0c\u6d4b\u8bd5\u96c6\u5305\u542b3333\u5f20\u56fe\u7247\u3002</p>"},{"location":"zh/how_to_guides/finetune_with_a_custom_dataset/#_2","title":"\u6570\u636e\u9884\u5904\u7406","text":""},{"location":"zh/how_to_guides/finetune_with_a_custom_dataset/#_3","title":"\u8bfb\u53d6\u6570\u636e\u96c6","text":"<p>\u5bf9\u4e8e\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u800c\u8a00\uff0c\u65e2\u53ef\u4ee5\u5148\u5728\u672c\u5730\u5c06\u6570\u636e\u6587\u4ef6\u76ee\u5f55\u6574\u7406\u6210\u4e0eImageNet\u7c7b\u4f3c\u7684\u6811\u72b6\u7ed3\u6784\uff0c\u518d\u4f7f\u7528<code>create_dataset</code>\u8bfb\u53d6\u6570\u636e\u96c6\uff08\u79bb\u7ebf\u65b9\u5f0f\uff0c\u4ec5\u9002\u7528\u4e8e\u5c0f\u578b\u6570\u636e\u96c6\uff09\uff0c\u53c8\u53ef\u4ee5\u76f4\u63a5\u5c06\u539f\u59cb\u6570\u636e\u96c6\u8bfb\u53d6\u6210\u53ef\u8fed\u4ee3/\u53ef\u6620\u5c04\u5bf9\u8c61\uff0c\u66ff\u4ee3\u6587\u4ef6\u62c6\u5206\u4e0e<code>create_dataset</code>\u6b65\u9aa4\uff08\u5728\u7ebf\u65b9\u5f0f\uff09\u3002</p>"},{"location":"zh/how_to_guides/finetune_with_a_custom_dataset/#_4","title":"\u79bb\u7ebf\u65b9\u5f0f","text":"<p>MindCV\u7684<code>create_dataset</code>\u63a5\u53e3\u4f7f\u7528<code>mindspore.dataset.ImageFolderDataset</code>\u51fd\u6570\u6784\u5efa\u6570\u636e\u5bf9\u8c61\uff0c\u540c\u4e00\u4e2a\u6587\u4ef6\u5939\u5185\u7684\u6240\u6709\u56fe\u7247\u5c06\u4f1a\u6839\u636e\u6587\u4ef6\u5939\u540d\u5b57\u88ab\u5206\u914d\u76f8\u540c\u7684\u6807\u7b7e\u3002\u56e0\u6b64\uff0c\u4f7f\u7528\u8be5\u6d41\u7a0b\u7684\u524d\u63d0\u6761\u4ef6\u662f\u6e90\u6570\u636e\u96c6\u7684\u6587\u4ef6\u76ee\u5f55\u5e94\u5f53\u9075\u5faa\u5982\u4e0b\u6811\u72b6\u7ed3\u6784\uff1a</p> <pre><code>DATASET_NAME\n    \u251c\u2500\u2500 split1(e.g. train)/\n    \u2502  \u251c\u2500\u2500 class1/\n    \u2502  \u2502   \u251c\u2500\u2500 000001.jpg\n    \u2502  \u2502   \u251c\u2500\u2500 000002.jpg\n    \u2502  \u2502   \u2514\u2500\u2500 ....\n    \u2502  \u2514\u2500\u2500 class2/\n    \u2502      \u251c\u2500\u2500 000001.jpg\n    \u2502      \u251c\u2500\u2500 000002.jpg\n    \u2502      \u2514\u2500\u2500 ....\n    \u2514\u2500\u2500 split2/\n       \u251c\u2500\u2500 class1/\n       \u2502   \u251c\u2500\u2500 000001.jpg\n       \u2502   \u251c\u2500\u2500 000002.jpg\n       \u2502   \u2514\u2500\u2500 ....\n       \u2514\u2500\u2500 class2/\n           \u251c\u2500\u2500 000001.jpg\n           \u251c\u2500\u2500 000002.jpg\n           \u2514\u2500\u2500 ....\n</code></pre> <p>\u63a5\u4e0b\u6765\u4ee5\u8bf4\u660e\u6587\u4ef6./aircraft/data/images_variant_trainval.txt \u4e3a\u4f8b\uff0c\u5728\u672c\u5730\u751f\u6210\u6ee1\u8db3\u524d\u8ff0\u6811\u72b6\u7ed3\u6784\u7684\u8bad\u7ec3\u96c6\u6587\u4ef6 ./aircraft/data/images/trainval/\u3002</p> <pre><code>\"\"\" Extract images and generate ImageNet-style dataset directory \"\"\"\nimport os\nimport shutil\n\n\n# only for Aircraft dataset but not a general one\ndef extract_images(images_path, subset_name, annotation_file_path, copy=True):\n    # read the annotation file to get the label of each image\n    def annotations(annotation_file_path):\n        image_label = {}\n        with open(annotation_file_path, \"r\") as f:\n            lines = f.readlines()\n            for line in lines:\n                label = \" \".join(line.split(\" \")[1:]).replace(\"\\n\", \"\").replace(\"/\", \"_\")\n                if label not in image_label.keys():\n                    image_label[label] = []\n                    image_label[label].append(line.split(\" \")[0])\n                else:\n                    image_label[label].append(line.split(\" \")[0])\n        return image_label\n\n    # make a new folder for subset\n    subset_path = images_path + subset_name\n    os.mkdir(subset_path)\n\n    # extract and copy/move images to the new folder\n    image_label = annotations(annotation_file_path)\n    for label in image_label.keys():\n        label_folder = subset_path + \"/\" + label\n        os.mkdir(label_folder)\n        for image in image_label[label]:\n            image_name = image + \".jpg\"\n            if copy:\n                shutil.copy(images_path + image_name, label_folder + image_name)\n            else:\n                shutil.move(images_path + image_name, label_folder)\n\n\n# take train set of aircraft dataset as an example\nimages_path = \"./aircraft/data/images/\"\nsubset_name = \"trainval\"\nannotation_file_path = \"./aircraft/data/images_variant_trainval.txt\"\nextract_images(images_path, subset_name, annotation_file_path)\n</code></pre> <p>\u6d4b\u8bd5\u96c6\u7684\u62c6\u5206\u65b9\u5f0f\u4e0e\u8bad\u7ec3\u96c6\u4e00\u81f4\uff0c\u6574\u7406\u5b8c\u6210\u7684Aircraft\u6570\u636e\u96c6\u6587\u4ef6\u7ed3\u6784\u5e94\u4e3a\uff1a</p> <pre><code>aircraft\n\u2514\u2500\u2500 data\n    \u2514\u2500\u2500 images\n        \u251c\u2500\u2500 trainval\n        \u2502   \u251c\u2500\u2500 707-320\n        \u2502   \u2502   \u251c\u2500\u2500 0056978.jpg\n        \u2502   \u2502   \u2514\u2500\u2500 ....\n        \u2502   \u251c\u2500\u2500 727-200\n        \u2502   \u2502   \u251c\u2500\u2500 0048341.jpg\n        \u2502   \u2502   \u2514\u2500\u2500 ....\n        \u2502   \u2514\u2500\u2500 ....\n        \u2514\u2500\u2500 test\n            \u251c\u2500\u2500 707-320\n            \u2502   \u251c\u2500\u2500 0062765.jpg\n            \u2502   \u2514\u2500\u2500 ....\n            \u251c\u2500\u2500 727-200\n            \u2502   \u251c\u2500\u2500 0061581.jpg\n            \u2502   \u2514\u2500\u2500 ....\n            \u2514\u2500\u2500 ....\n</code></pre> <p>\u7531\u4e8e\u6a21\u578b\u5fae\u8c03\u6587\u4ef6./example/finetune.py\u4e2d\u96c6\u6210\u4e86<code>create_dataset</code>-&gt;<code>create_transforms</code>-&gt;<code>create_loader</code>-&gt;<code>create_model</code>-&gt;...\u7b49\u6240\u6709\u4ece\u9884\u5904\u7406\u5230\u5efa\u7acb\u3001\u9a8c\u8bc1\u6a21\u578b\u7684\u8bad\u7ec3\u6d41\u7a0b\uff0c\u4f7f\u7528\u79bb\u7ebf\u65b9\u5f0f\u6574\u7406\u5b8c\u6587\u4ef6\u76ee\u5f55\u7ed3\u6784\u7684\u6570\u636e\u96c6\u53ef\u4ee5**\u76f4\u63a5\u901a\u8fc7\u8fd0\u884c<code>python ./example/finetune.py</code>\u547d\u4ee4\u5b8c\u6210\u540e\u7eed\u8bfb\u53d6\u6570\u636e\u4e0e\u8bad\u7ec3\u6a21\u578b**\u8fd9\u4e00\u6574\u5957\u64cd\u4f5c\u3002\u5bf9\u4e8e\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u800c\u8a00\uff0c\u8fd8\u9700\u6ce8\u610f\u63d0\u524d\u5c06\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684<code>dataset</code>\u53c2\u6570\u8bbe\u7f6e\u4e3a\u7a7a\u5b57\u7b26\u4e32<code>\"\"</code>\u3002</p>"},{"location":"zh/how_to_guides/finetune_with_a_custom_dataset/#_5","title":"\u5728\u7ebf\u65b9\u5f0f","text":"<p>\u79bb\u7ebf\u65b9\u5f0f\u7684\u6570\u636e\u8bfb\u53d6\u4f1a\u5728\u672c\u5730\u5360\u7528\u989d\u5916\u7684\u78c1\u76d8\u7a7a\u95f4\u5b58\u50a8\u65b0\u751f\u6210\u7684\u6570\u636e\u6587\u4ef6\uff0c\u56e0\u6b64\u5728\u672c\u5730\u5b58\u50a8\u7a7a\u95f4\u4e0d\u8db3\u6216\u65e0\u6cd5\u5c06\u6570\u636e\u5907\u4efd\u5230\u672c\u5730\u7b49\u5176\u4ed6\u7279\u6b8a\u60c5\u51b5\u4e0b\uff0c\u65e0\u6cd5\u76f4\u63a5\u4f7f\u7528<code>create_dataset</code>\u63a5\u53e3\u8bfb\u53d6\u672c\u5730\u6570\u636e\u6587\u4ef6\u65f6\uff0c\u53ef\u4ee5\u91c7\u7528\u5728\u7ebf\u65b9\u5f0f\u81ea\u884c\u7f16\u5199\u51fd\u6570\u8bfb\u53d6\u6570\u636e\u96c6\u3002</p> <p>\u4ee5\u751f\u6210\u50a8\u5b58\u8bad\u7ec3\u96c6\u56fe\u7247\u548c\u7d22\u5f15\u5230\u56fe\u7247\u6837\u672c\u6620\u5c04\u7684\u53ef\u968f\u673a\u8bbf\u95ee\u6570\u636e\u96c6\u4e3a\u4f8b\uff1a</p> <ul> <li> <p>\u9996\u5148\u5b9a\u4e49\u4e00\u4e2a\u8bfb\u53d6\u539f\u59cb\u6570\u636e\u5e76\u5c06\u5176\u8f6c\u6362\u6210\u53ef\u968f\u673a\u8bbf\u95ee\u7684\u6570\u636e\u96c6\u5bf9\u8c61<code>ImageClsDataset</code>\uff1a</p> <ul> <li> <p>\u5728\u8be5\u7c7b\u7684\u521d\u59cb\u5316\u51fd\u6570<code>__init__()</code>\u4e2d\uff0c\u4ee5./aircraft/data/images_variant_trainval.txt\u4e3a\u4f8b\u7684\u6807\u6ce8\u6587\u4ef6\u7684\u6587\u4ef6\u8def\u5f84\u5c06\u88ab\u5f53\u505a\u8f93\u5165\uff0c\u7528\u4e8e\u751f\u6210\u50a8\u5b58\u56fe\u7247\u4e0e\u6807\u7b7e\u4e00\u4e00\u5bf9\u5e94\u5173\u7cfb\u7684\u5b57\u5178<code>self.annotation</code>\uff1b</p> </li> <li> <p>\u7531\u4e8e\u5728<code>create_loader</code>\u4e2d\u5c06\u4f1a\u5bf9\u6b64\u5bf9\u8c61\u8fdb\u884cmap\u64cd\u4f5c\uff0c\u800c\u8be5\u64cd\u4f5c\u4e0d\u652f\u6301\u5b57\u7b26\u4e32\u683c\u5f0f\u7684\u6807\u7b7e\uff0c\u56e0\u6b64\u8fd8\u9700\u8981\u751f\u6210<code>self.label2id</code>\u5e76\u5c06<code>self.annotation</code>\u4e2d\u5b57\u7b26\u4e32\u683c\u5f0f\u7684\u6807\u7b7e\u8f6c\u6362\u6210\u6574\u6570\u683c\u5f0f\uff1b</p> </li> <li> <p>\u6839\u636e<code>self.annotation</code>\u4e2d\u50a8\u5b58\u7684\u4fe1\u606f\uff0c\u4ece\u6587\u4ef6\u5939./aircraft/data/images/\u4e2d\u5c06\u8bad\u7ec3\u96c6\u56fe\u7247\u8bfb\u53d6\u6210\u4e00\u7ef4\u6570\u7ec4\u5f62\u5f0f\uff08\u7531\u4e8e<code>create_loader</code>\u4e2dmap\u64cd\u4f5c\u9650\u5236\uff0c\u6b64\u5904\u56fe\u7247\u6570\u636e\u5fc5\u987b\u88ab\u8bfb\u53d6\u4e3a\u4e00\u7ef4\u683c\u5f0f\uff09\uff0c\u5e76\u5c06\u56fe\u7247\u4fe1\u606f\u4e0e\u6807\u7b7e\u5206\u522b\u5b58\u653e\u5230<code>self._data</code>\u4e0e<code>self._label</code>\u4e2d\uff1b</p> </li> <li> <p>\u63a5\u4e0b\u6765\u4f7f\u7528<code>__getitem__</code>\u65b9\u6cd5\u6784\u9020\u53ef\u968f\u673a\u8bbf\u95ee\u7684\u6570\u636e\u96c6\u5bf9\u8c61\u3002</p> </li> </ul> </li> </ul> <p>-   \u6784\u9020\u5b8c<code>ImageClsDataset</code>\u7c7b\u4e4b\u540e\uff0c\u5411\u5176\u4f20\u5165\u6807\u6ce8\u6587\u4ef6\u7684\u8def\u5f84\u4ee5\u5b9e\u4f8b\u5316\u8be5\u7c7b\uff0c\u5e76\u901a\u8fc7<code>mindspore.dataset.GeneratorDataset</code>\u51fd\u6570\u5c06\u8be5\u53ef\u6620\u5c04\u5bf9\u8c61\u52a0\u8f7d\u6210\u6570\u636e\u96c6\u5373\u53ef\uff0c\u6ce8\u610f\u8be5\u51fd\u6570\u7684\u53c2\u6570<code>column_names</code>\u5fc5\u987b\u88ab\u8bbe\u7f6e\u4e3a[\"image\", \"label\"]\u4ee5\u4fbf\u540e\u7eed\u5176\u4ed6\u63a5\u53e3\u8bfb\u53d6\u6570\u636e\uff0c\u6b64\u65f6\u5f97\u5230\u7684<code>dataset_train</code>\u5e94\u5f53\u4e0e\u901a\u8fc7<code>create_dataset</code>\u8bfb\u53d6\u7684\u8bad\u7ec3\u96c6\u5b8c\u5168\u4e00\u81f4\u3002</p> <pre><code>import numpy as np\n\nfrom mindspore.dataset import GeneratorDataset\n\n\nclass ImageClsDataset:\n    def __init__(self, annotation_dir, images_dir):\n        # Read annotations\n        self.annotation = {}\n        with open(annotation_dir, \"r\") as f:\n            lines = f.readlines()\n            for line in lines:\n                image_label = line.replace(\"\\n\", \"\").replace(\"/\", \"_\").split(\" \")\n                image = image_label[0] + \".jpg\"\n                label = \" \".join(image_label[1:])\n                self.annotation[image] = label\n\n        # Transfer string-type label to int-type label\n        self.label2id = {}\n        labels = sorted(list(set(self.annotation.values())))\n        for i in labels:\n            self.label2id[i] = labels.index(i)\n\n        for image, label in self.annotation.items():\n            self.annotation[image] = self.label2id[label]\n\n        # Read image-labels as mappable object\n        label2images = {key: [] for key in self.label2id.values()}\n        for image, label in self.annotation.items():\n            read_image = np.fromfile(images_dir + image, dtype=np.uint8)\n            label2images[label].append(read_image)\n\n        self._data = sum(list(label2images.values()), [])\n        self._label = sum([[i] * len(label2images[i]) for i in label2images.keys()], [])\n\n    # make class ImageClsDataset a mappable object\n    def __getitem__(self, index):\n        return self._data[index], self._label[index]\n\n    def __len__(self):\n        return len(self._data)\n\n\n# take aircraft dataset as an example\nannotation_dir = \"./aircraft/data/images_variant_trainval.txt\"\nimages_dir = \"./aircraft/data/images/\"\ndataset = ImageClsDataset(annotation_dir, images_dir)\ndataset_train = GeneratorDataset(source=dataset, column_names=[\"image\", \"label\"], shuffle=True)\n</code></pre> <p>\u4e0e\u79bb\u7ebf\u65b9\u5f0f\u8bfb\u53d6\u6570\u636e\u96c6\u76f8\u6bd4\uff0c\u5728\u7ebf\u8bfb\u53d6\u65b9\u5f0f\u7701\u7565\u4e86\u5728\u672c\u5730\u62c6\u5206\u6570\u636e\u6587\u4ef6\u5e76\u7528<code>create_dataset</code>\u63a5\u53e3\u8bfb\u53d6\u672c\u5730\u6587\u4ef6\u7684\u6b65\u9aa4\uff0c\u56e0\u6b64\u5728\u540e\u7eed\u7684\u8bad\u7ec3\u4e2d\uff0c\u53ea\u9700**\u5c06finetune.py\u4e2d\u4f7f\u7528<code>create_dataset</code>\u63a5\u53e3\u7684\u90e8\u5206\u66ff\u6362\u6210\u4e0a\u8ff0\u4ee3\u7801**\uff0c\u5c31\u53ef\u4ee5\u4e0e\u79bb\u7ebf\u65b9\u5f0f\u4e00\u6837\uff0c\u76f4\u63a5\u8fd0\u884cfinetune.py\u5f00\u59cb\u8bad\u7ec3\u3002</p>"},{"location":"zh/how_to_guides/finetune_with_a_custom_dataset/#_6","title":"\u6570\u636e\u589e\u5f3a\u4e0e\u5206\u6279","text":"<p>MindCV\u4f7f\u7528<code>create_loader</code>\u51fd\u6570\u5bf9\u4e0a\u4e00\u7ae0\u8282\u8bfb\u53d6\u7684\u6570\u636e\u96c6\u8fdb\u884c\u56fe\u50cf\u589e\u5f3a\u4e0e\u5206\u6279\u5904\u7406\uff0c\u56fe\u50cf\u589e\u5f3a\u7b56\u7565\u901a\u8fc7<code>create_transforms</code>\u51fd\u6570\u4e8b\u5148\u5b9a\u4e49\uff0c\u5206\u6279\u5904\u7406\u64cd\u4f5c\u901a\u8fc7<code>create_loader</code>\u51fd\u6570\u4e2d\u7684\u53c2\u6570<code>batch_size</code>\u5b9a\u4e49\uff0c\u4ee5\u4e0a\u6d89\u53ca\u5230\u7684**\u6240\u6709\u8d85\u53c2\u6570\u5747\u53ef\u4ee5\u901a\u8fc7\u6a21\u578b\u914d\u7f6e\u6587\u4ef6\u4f20\u9012**\uff0c\u8d85\u53c2\u6570\u5177\u4f53\u4f7f\u7528\u65b9\u6cd5\u89c1API\u8bf4\u660e\u3002</p> <p>\u5bf9\u4e8e\u89c4\u6a21\u8f83\u5c0f\u7684\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\uff0c\u5efa\u8bae\u53ef\u4ee5\u5728\u8fd9\u4e00\u90e8\u5206\u5bf9\u8bad\u7ec3\u96c6\u505a\u989d\u5916\u7684\u6570\u636e\u589e\u5f3a\u5904\u7406\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u7684\u6cdb\u5316\u6027\uff0c\u9632\u6b62\u8fc7\u62df\u5408\u3002\u5bf9\u4e8e\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u7684\u6570\u636e\u96c6\uff0c\u6bd4\u5982\u672c\u6587\u4e2d\u7684Aircraft\u6570\u636e\u96c6\uff0c\u7531\u4e8e\u6570\u636e\u7c7b\u5185\u65b9\u5dee\u8f83\u5927\u53ef\u80fd\u5bfc\u81f4\u5206\u7c7b\u6548\u679c\u8f83\u5dee\uff0c\u8fd8\u53ef\u4ee5\u901a\u8fc7\u8c03\u6574\u8d85\u53c2\u6570<code>image_resize</code>\u9002\u5f53\u589e\u5927\u56fe\u7247\u5c3a\u5bf8\uff08\u5982\uff1a448\u3001512\u3001600\u7b49\u7b49\uff09\u3002</p>"},{"location":"zh/how_to_guides/finetune_with_a_custom_dataset/#_7","title":"\u6a21\u578b\u5fae\u8c03","text":"<p>\u53c2\u8003Stanford University CS231n\uff0c\u6574\u4f53\u5fae\u8c03\u3001\u51bb\u7ed3\u7279\u5f81\u7f51\u7edc\u5fae\u8c03\u3001\u4e0e**\u5206\u5c42\u8bbe\u7f6e\u5b66\u4e60\u7387\u5fae\u8c03**\u662f\u5e38\u7528\u7684\u5fae\u8c03\u6a21\u5f0f\u3002\u6a21\u578b\u7684\u6574\u4f53\u5fae\u8c03\u4f7f\u7528\u9884\u8bad\u7ec3\u6743\u91cd\u521d\u59cb\u5316\u76ee\u6807\u6a21\u578b\u7684\u53c2\u6570\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u9488\u5bf9\u65b0\u6570\u636e\u96c6\u7ee7\u7eed\u8bad\u7ec3\u3001\u66f4\u65b0\u6240\u6709\u53c2\u6570\uff0c\u56e0\u6b64\u8ba1\u7b97\u91cf\u8f83\u5927\uff0c\u8017\u65f6\u8f83\u957f\u4f46\u4e00\u822c\u7cbe\u5ea6\u8f83\u9ad8\uff1b\u51bb\u7ed3\u7279\u5f81\u7f51\u7edc\u5219\u5206\u4e3a\u51bb\u7ed3\u6240\u6709\u7279\u5f81\u7f51\u7edc\u4e0e\u51bb\u7ed3\u90e8\u5206\u7279\u5f81\u7f51\u7edc\u4e24\u79cd\uff0c\u524d\u8005\u5c06\u9884\u8bad\u7ec3\u6a21\u578b\u4f5c\u4e3a\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u4ec5\u66f4\u65b0\u5168\u8fde\u63a5\u5c42\u53c2\u6570\uff0c\u8017\u65f6\u77ed\u4f46\u7cbe\u5ea6\u4f4e\uff0c\u540e\u8005\u4e00\u822c\u56fa\u5b9a\u5b66\u4e60\u57fa\u7840\u7279\u5f81\u7684\u6d45\u5c42\u53c2\u6570\uff0c\u53ea\u66f4\u65b0\u5b66\u4e60\u7cbe\u7ec6\u7279\u5f81\u7684\u6df1\u5c42\u7f51\u7edc\u53c2\u6570\u4e0e\u5168\u8fde\u63a5\u5c42\u53c2\u6570\uff1b\u5206\u5c42\u8bbe\u7f6e\u5b66\u4e60\u7387\u4e0e\u4e4b\u76f8\u4f3c\uff0c\u4f46\u662f\u66f4\u52a0\u7cbe\u7ec6\u5730\u6307\u5b9a\u4e86\u7f51\u7edc\u5185\u90e8\u67d0\u4e9b\u7279\u5b9a\u5c42\u5728\u8bad\u7ec3\u4e2d\u66f4\u65b0\u53c2\u6570\u6240\u4f7f\u7528\u7684\u5b66\u4e60\u7387\u3002</p> <p>\u5bf9\u4e8e\u5b9e\u9645\u5fae\u8c03\u8bad\u7ec3\u4e2d\u6240\u4f7f\u7528\u7684\u7684\u8d85\u53c2\u6570\u914d\u7f6e\uff0c\u53ef\u4ee5\u53c2\u8003./configs\u4e2d\u57fa\u4e8eImageNet-1k\u6570\u636e\u96c6\u9884\u8bad\u7ec3\u7684\u914d\u7f6e\u6587\u4ef6\u3002\u6ce8\u610f\u5bf9\u6a21\u578b\u5fae\u8c03\u800c\u8a00\uff0c\u5e94\u4e8b\u5148\u5c06\u8d85\u53c2\u6570<code>pretrained</code>\u8bbe\u7f6e\u4e3a<code>True</code>\u4ee5\u52a0\u8f7d\u9884\u8bad\u7ec3\u6743\u91cd\uff0c\u5c06<code>num_classes</code>\u8bbe\u7f6e\u4e3a\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u7684\u6807\u7b7e\u4e2a\u6570\uff08\u6bd4\u5982Aircfrat\u6570\u636e\u96c6\u662f100\uff09\uff0c\u8fd8\u53ef\u4ee5\u57fa\u4e8e\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u89c4\u6a21\uff0c\u9002\u5f53\u8c03\u5c0f<code>batch_size</code>\u4e0e<code>epoch_size</code>\u3002\u6b64\u5916\uff0c\u7531\u4e8e\u9884\u8bad\u7ec3\u6743\u91cd\u4e2d\u5df2\u7ecf\u5305\u542b\u4e86\u8bb8\u591a\u8bc6\u522b\u56fe\u50cf\u7684\u521d\u59cb\u4fe1\u606f\uff0c\u4e3a\u4e86\u4e0d\u8fc7\u5206\u7834\u574f\u8fd9\u4e9b\u4fe1\u606f\uff0c\u8fd8\u9700\u5c06\u5b66\u4e60\u7387<code>lr</code>\u8c03\u5c0f\uff0c\u5efa\u8bae\u81f3\u591a\u4ece\u9884\u8bad\u7ec3\u5b66\u4e60\u7387\u7684\u5341\u5206\u4e4b\u4e00\u62160.0001\u5f00\u59cb\u8bad\u7ec3\u3001\u8c03\u53c2\u3002\u8fd9\u4e9b\u53c2\u6570\u90fd\u53ef\u4ee5\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u4fee\u6539\uff0c\u4e5f\u53ef\u4ee5\u5982\u4e0b\u6240\u793a\u5728shell\u547d\u4ee4\u4e2d\u6dfb\u52a0\uff0c\u8bad\u7ec3\u7ed3\u679c\u53ef\u5728./ckpt/results.txt\u6587\u4ef6\u4e2d\u67e5\u770b\u3002</p> <pre><code>python .examples/finetune/finetune.py --config=./configs/mobilenetv3/mobilnet_v3_small_ascend.yaml --data_dir=./aircraft/data --pretrained=True\n</code></pre> <p>\u672c\u6587\u5728\u57fa\u4e8eAircraft\u6570\u636e\u96c6\u5bf9mobilenet v3-small\u5fae\u8c03\u65f6\u4e3b\u8981\u5bf9\u8d85\u53c2\u6570\u505a\u4e86\u5982\u4e0b\u6539\u52a8\uff1a</p> Hyper-parameter Pretrain Fine-tune dataset \"imagenet\" \"\" batch_size 75 8 image_resize 224 600 auto_augment - \"randaug-m7-mstd0.5\" num_classes 1000 100 pretrained False True epoch_size 470 50 lr 0.77 0.002"},{"location":"zh/how_to_guides/finetune_with_a_custom_dataset/#_8","title":"\u6574\u4f53\u5fae\u8c03","text":"<p>\u7531\u4e8e\u6574\u4f53\u5fae\u8c03\u7684\u8bad\u7ec3\u6d41\u7a0b\u4e0e\u4ece\u5934\u8bad\u7ec3\u4e00\u81f4\uff0c\u56e0\u6b64\u53ea\u9700\u901a\u8fc7**\u8fd0\u884cfinetune.py\u542f\u52a8\u8bad\u7ec3**\u5e76\u8ddf\u4ece\u5934\u8bad\u7ec3\u4e00\u6837\u8c03\u53c2\u5373\u53ef\u3002</p>"},{"location":"zh/how_to_guides/finetune_with_a_custom_dataset/#_9","title":"\u51bb\u7ed3\u7279\u5f81\u7f51\u7edc","text":""},{"location":"zh/how_to_guides/finetune_with_a_custom_dataset/#_10","title":"\u51bb\u7ed3\u6240\u6709\u7279\u5f81\u7f51\u7edc","text":"<p>\u6211\u4eec\u901a\u8fc7\u5bf9\u9664\u5168\u8fde\u63a5\u5c42\u5916\u7684\u6240\u6709\u53c2\u6570\u8bbe\u7f6e<code>requires_grad=False</code>\u6765\u9632\u6b62\u5176\u53c2\u6570\u66f4\u65b0\u3002\u5728finetune.py\u4e2d\uff0c\u53ea\u9700\u5728\u521b\u5efa\u6a21\u578b<code>create_model</code>\u4e4b\u540e\u52a0\u5165\u5982\u4e0b\u4ee3\u7801\u5373\u53ef\u5b9e\u73b0\uff1a</p> <pre><code>from mindcv.models.registry import _model_pretrained_cfgs\n\n# ...create_model()\n\n# number of parameters to be updated\nnum_params = 2\n\n# read names of parameters in FC layer\nclassifier_names = [_model_pretrained_cfgs[args.model][\"classifier\"] + \".weight\",\n                    _model_pretrained_cfgs[args.model][\"classifier\"] + \".bias\"]\n\n# prevent parameters in network(except the classifier) from updating\nfor param in network.trainable_params():\n    if param.name not in classifier_names:\n        param.requires_grad = False\n</code></pre>"},{"location":"zh/how_to_guides/finetune_with_a_custom_dataset/#_11","title":"\u51bb\u7ed3\u90e8\u5206\u7279\u5f81\u7f51\u7edc","text":"<p>\u4e3a\u4e86\u5e73\u8861\u5fae\u8c03\u8bad\u7ec3\u7684\u901f\u5ea6\u548c\u7cbe\u5ea6\uff0c\u6211\u4eec\u8fd8\u53ef\u4ee5\u56fa\u5b9a\u90e8\u5206\u76ee\u6807\u7f51\u7edc\u53c2\u6570\uff0c\u6709\u9488\u5bf9\u6027\u5730\u8bad\u7ec3\u7f51\u7edc\u4e2d\u7684\u6df1\u5c42\u53c2\u6570\u3002\u5b9e\u73b0\u8fd9\u4e00\u64cd\u4f5c\u53ea\u9700\u8981\u63d0\u53d6\u51fa\u8981\u51bb\u7ed3\u7684\u5c42\u4e2d\u7684\u53c2\u6570\u540d\u79f0\uff0c\u5e76\u5728\u4e0a\u8ff0\u51bb\u7ed3\u6240\u6709\u7279\u5f81\u7f51\u7edc\u7684\u4ee3\u7801\u57fa\u7840\u4e0a\u7a0d\u4f5c\u4fee\u6539\u5373\u53ef\u3002\u901a\u8fc7\u6253\u5370<code>create_model</code>\u7684\u7ed3\u679c\u2014\u2014<code>network</code>\u53ef\u77e5\uff0cMindCV\u4e2d\u5bf9mobilenet v3-small\u7684\u6bcf\u5c42\u7f51\u7edc\u547d\u540d\u4e3a<code>\"features.*\"</code>\uff0c\u5047\u8bbe\u6211\u4eec\u4ec5\u51bb\u7ed3\u7f51\u7edc\u524d7\u5c42\uff0c\u5728finetune.py\u4e2d\u521b\u5efa\u6a21\u578b<code>create_model</code>\u540e\u52a0\u5165\u5982\u4e0b\u4ee3\u7801\u5373\u53ef\uff1a</p> <pre><code># ...create_model()\n\n# read names of network layers\nfreeze_layer=[\"features.\"+str(i) for i in range(7)]\n\n# prevent parameters in first 7 layers of network from updating\nfor param in network.trainable_params():\n    for layer in freeze_layer:\n        if layer in param.name:\n            param.requires_grad = False\n</code></pre>"},{"location":"zh/how_to_guides/finetune_with_a_custom_dataset/#_12","title":"\u5206\u5c42\u8bbe\u7f6e\u5b66\u4e60\u7387","text":"<p>\u4e3a\u4e86\u8fdb\u4e00\u6b65\u63d0\u5347\u5fae\u8c03\u7f51\u7edc\u7684\u8bad\u7ec3\u6548\u679c\uff0c\u8fd8\u53ef\u4ee5\u5206\u5c42\u8bbe\u7f6e\u8bad\u7ec3\u4e2d\u7684\u5b66\u4e60\u7387\u3002\u8fd9\u662f\u7531\u4e8e\u6d45\u5c42\u7f51\u7edc\u4e00\u822c\u662f\u8bc6\u522b\u901a\u7528\u7684\u8f6e\u5ed3\u7279\u5f81\uff0c\u6240\u4ee5\u5373\u4fbf\u91cd\u65b0\u66f4\u65b0\u8be5\u90e8\u5206\u53c2\u6570\uff0c\u5b66\u4e60\u7387\u4e5f\u5e94\u8be5\u88ab\u8bbe\u7f6e\u5f97\u6bd4\u8f83\u5c0f\uff1b\u6df1\u5c42\u90e8\u5206\u4e00\u822c\u8bc6\u522b\u7269\u4f53\u7cbe\u7ec6\u7684\u4e2a\u6027\u7279\u5f81\uff0c\u5b66\u4e60\u7387\u4e5f\u56e0\u6b64\u53ef\u4ee5\u8bbe\u7f6e\u5f97\u6bd4\u8f83\u5927\uff1b\u800c\u76f8\u5bf9\u4e8e\u9700\u8981\u5c3d\u91cf\u4fdd\u7559\u9884\u8bad\u7ec3\u4fe1\u606f\u7684\u7279\u5f81\u7f51\u7edc\u800c\u8a00\uff0c\u5206\u7c7b\u5668\u9700\u8981\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\uff0c\u4e5f\u53ef\u4ee5\u9002\u5f53\u5c06\u5b66\u4e60\u7387\u8c03\u5927\u3002\u7531\u4e8e\u9488\u5bf9\u7279\u5b9a\u7f51\u7edc\u5c42\u7684\u5b66\u4e60\u7387\u8c03\u6574\u64cd\u4f5c\u6bd4\u8f83\u7cbe\u7ec6\uff0c\u6211\u4eec\u9700\u8981\u8fdb\u5165finetune.py\u4e2d\u81ea\u884c\u6307\u5b9a\u53c2\u6570\u540d\u4e0e\u5bf9\u5e94\u7684\u5b66\u4e60\u7387\u3002</p> <p>MindCV\u4f7f\u7528<code>create_optimizer</code>\u51fd\u6570\u6784\u9020\u4f18\u5316\u5668\uff0c\u5e76\u5c06\u5b66\u4e60\u7387\u4f20\u5230\u4f18\u5316\u5668\u4e2d\u53bb\u3002\u8981\u8bbe\u7f6e\u5206\u5c42\u5b66\u4e60\u7387\uff0c\u53ea\u9700**\u5c06finetune.py\u4e2d<code>create_optimizer</code>\u51fd\u6570\u7684<code>params</code>\u53c2\u6570\u4ece<code>network.trainable_params()</code>\u6539\u4e3a\u5305\u542b\u7279\u5b9a\u5c42\u53c2\u6570\u540d\u4e0e\u5bf9\u5e94\u5b66\u4e60\u7387\u7684\u5217\u8868\u5373\u53ef**\uff0c\u53c2\u8003MindSpore\u5404\u4f18\u5316\u5668\u8bf4\u660e\u6587\u6863\uff0c\u5176\u4e2d\u7f51\u7edc\u5177\u4f53\u7ed3\u6784\u4e0e\u6bcf\u5c42\u4e2d\u7684\u53c2\u6570\u540d\u5747\u53ef\u4ee5\u901a\u8fc7\u6253\u5370<code>create_model</code>\u7684\u7ed3\u679c\u2014\u2014<code>network</code>\u67e5\u770b\u3002</p> <p>Tips: \u60a8\u8fd8\u53ef\u4ee5\u4f7f\u7528\u540c\u6837\u7684\u64cd\u4f5c\u5206\u5c42\u8bbe\u7f6eweight_decay.</p>"},{"location":"zh/how_to_guides/finetune_with_a_custom_dataset/#_13","title":"\u5355\u72ec\u8c03\u6574\u5206\u7c7b\u5668\u7684\u5b66\u4e60\u7387","text":"<p>\u4ee5mobilenet v3-small\u4e3a\u4f8b\uff0c\u8be5\u6a21\u578b\u5206\u7c7b\u5668\u540d\u79f0\u4ee5\u201cclassifier\u201d\u5f00\u5934\uff0c\u56e0\u6b64\u5982\u679c\u4ec5\u8c03\u5927\u5206\u7c7b\u5668\u7684\u5b66\u4e60\u7387\uff0c\u6211\u4eec\u9700\u8981\u6307\u5b9a\u5206\u7c7b\u5668\u5728\u6bcf\u4e00\u6b65\u8bad\u7ec3\u4e2d\u7684\u5b66\u4e60\u7387\u3002<code>lr_scheduler</code>\u662f\u7531<code>create_scheduler</code>\u751f\u6210\u7684\u5b66\u4e60\u7387\u8c03\u6574\u7b56\u7565\uff0c\u662f\u4e00\u4e2a\u5305\u542b\u7f51\u7edc\u6bcf\u6b65\u8bad\u7ec3\u4e2d\u5177\u4f53\u5b66\u4e60\u7387\u503c\u7684\u5217\u8868\uff0c\u5047\u8bbe\u6211\u4eec\u5c06\u5206\u7c7b\u5668\u7684\u5b66\u4e60\u7387\u8c03\u6574\u81f3\u7279\u5f81\u7f51\u7edc\u5b66\u4e60\u7387\u76841.2\u500d\uff0cfinetune.py\u4e2d\u521b\u5efa\u4f18\u5316\u5668\u90e8\u5206\u4ee3\u7801\u7684\u6539\u52a8\u5982\u4e0b\uff1a</p> <pre><code># ...\n\n\n# Note: a)the params-lr dict must contain all the parameters. b)Also, you're recommended to set a dict with a key \"order_params\" to make sure the parameters will be updated in a right order.\nparams_lr_group = [{\"params\": list(filter(lambda x: 'classifier' in x.name, network.trainable_params())),\n                    \"lr\": [i*1.2 for i in lr_scheduler]},\n                   {\"params\": list(filter(lambda x: 'classifier' not in x.name, network.trainable_params())),\n                    \"lr\": lr_scheduler},\n                   {\"order_params\": network.trainable_params()}]\n\noptimizer = create_optimizer(params_lr_group,\n                             opt=args.opt,\n                             lr=lr_scheduler,\n                             ...)\n</code></pre>"},{"location":"zh/how_to_guides/finetune_with_a_custom_dataset/#_14","title":"\u8bbe\u7f6e\u7279\u5f81\u7f51\u7edc\u4efb\u610f\u5c42\u7684\u5b66\u4e60\u7387","text":"<p>\u4e0e\u5355\u72ec\u8c03\u6574\u5206\u7c7b\u5668\u7684\u5b66\u4e60\u7387\u7c7b\u4f3c\uff0c\u5206\u5c42\u8bbe\u7f6e\u7279\u5f81\u7f51\u7edc\u5b66\u4e60\u7387\u9700\u8981\u6307\u5b9a\u7279\u5b9a\u5c42\u7684\u5b66\u4e60\u7387\u53d8\u5316\u5217\u8868\u3002\u5047\u8bbe\u6211\u4eec\u4ec5\u589e\u5927\u7279\u5f81\u7f51\u7edc\u6700\u540e\u4e09\u5c42\u53c2\u6570\uff08features.13, features.14, features.15\uff09\u66f4\u65b0\u7684\u5b66\u4e60\u7387\uff0c\u5bf9finetune.py\u4e2d\u521b\u5efa\u4f18\u5316\u5668\u90e8\u5206\u4ee3\u7801\u7684\u6539\u52a8\u5982\u4e0b\uff1a</p> <pre><code># ...\n\n\n# Note: a)the params-lr dict must contain all the parameters. b)Also, you're recommended to set a dict with a key \"order_params\" to make sure the parameters will be updated in a right order.\nparams_lr_group = [{\"params\": list(filter(lambda x: 'features.13' in x.name, network.trainable_params())),\n                    \"lr\": [i * 1.05 for i in lr_scheduler]},\n                   {\"params\": list(filter(lambda x: 'features.14' in x.name, network.trainable_params())),\n                    \"lr\": [i * 1.1 for i in lr_scheduler]},\n                   {\"params\": list(filter(lambda x: 'features.15' in x.name, network.trainable_params())),\n                    \"lr\": [i * 1.15 for i in lr_scheduler]},\n                   {\"params\": list(filter(\n                       lambda x: \".\".join(x.name.split(\".\")[:2]) not in [\"features.13\", \"features.14\", \"features.15\"],\n                       network.trainable_params())),\n                    \"lr\": lr_scheduler},\n                   {\"order_params\": network.trainable_params()}]\n\noptimizer = create_optimizer(params_lr_group,\n                             opt=args.opt,\n                             lr=lr_scheduler,\n                             ...)\n</code></pre>"},{"location":"zh/how_to_guides/finetune_with_a_custom_dataset/#_15","title":"\u6a21\u578b\u8bc4\u4f30","text":"<p>\u8bad\u7ec3\u7ed3\u675f\u540e\uff0c\u4f7f\u7528./ckpt\u6587\u4ef6\u5939\u4e2d\u4ee5<code>*_best.ckpt</code>\u683c\u5f0f\u50a8\u5b58\u7684\u6a21\u578b\u6743\u91cd\u6765\u8bc4\u4f30\u6a21\u578b\u5728\u6d4b\u8bd5\u96c6\u4e0a\u7684\u6700\u4f18\u8868\u73b0\uff0c\u53ea\u9700**\u76f4\u63a5\u8fd0\u884cvalidate.py**\u5e76\u5411\u5176\u4f20\u5165\u6a21\u578b\u914d\u7f6e\u6587\u4ef6\u8def\u5f84\u4e0e\u6743\u91cd\u7684\u6587\u4ef6\u8def\u5f84\u5373\u53ef\uff1a</p> <pre><code>python validate.py --config=./configs/mobilenetv3/mobilnet_v3_small_ascend.yaml --data_dir=./aircraft/data --ckpt_path=./ckpt/mobilenet_v3_small_100_best.ckpt\n</code></pre> <p>\u6a21\u578b\u5fae\u8c03\u7ae0\u8282\u5c55\u793a\u4e86\u591a\u79cd\u5fae\u8c03\u6280\u5de7\uff0c\u4e0b\u8868\u603b\u7ed3\u4e86\u5728\u4f7f\u7528\u76f8\u540c\u8bad\u7ec3\u914d\u7f6e\u4e0d\u540c\u5fae\u8c03\u65b9\u5f0f\u4e0bmobilenet v3-small\u6a21\u578b\u5728Aircraft\u6570\u636e\u96c6\u4e0a\u7684Top-1 \u7cbe\u5ea6\u8868\u73b0\uff1a</p> \u6a21\u578b \u51bb\u7ed3\u6240\u6709\u7279\u5f81\u7f51\u7edc \u51bb\u7ed3\u6d45\u5c42\u7279\u5f81\u7f51\u7edc \u5168\u91cf\u5fae\u8c03+\u56fa\u5b9a\u5b66\u4e60\u7387 \u5168\u91cf\u5fae\u8c03+\u8c03\u5927\u5206\u7c7b\u5668\u5b66\u4e60\u7387 \u5168\u91cf\u5fae\u8c03+\u8c03\u5927\u6df1\u5c42\u7f51\u7edc\u5b66\u4e60\u7387 mobilenet v3-small 48.66% 76.83% 88.35% 88.89% 88.68%"},{"location":"zh/how_to_guides/finetune_with_a_custom_dataset/#_16","title":"\u6a21\u578b\u9884\u6d4b","text":"<p>\u53c2\u8003MindCV\u5fae\u8c03\u6559\u7a0b\u4e2d\u53ef\u89c6\u5316\u6a21\u578b\u63a8\u7406\u7ed3\u679c\u5c0f\u8282\uff0c\u6216\u662f\u5728validate.py\u4e2d\u52a0\u5165\u5982\u4e0b\u4ee3\u7801\u751f\u6210\u50a8\u5b58\u6d4b\u8bd5\u96c6\u771f\u5b9e\u503c\u4e0e\u9884\u6d4b\u503c\u7684\u6587\u672c\u6587\u4ef6./ckpt/pred.txt\uff1a</p> <pre><code># ... after model.eval()\n\n# predited label\npred = np.argmax(model.predict(images).asnumpy(), axis=1)\n\n# real label\nimages, labels = next(loader_eval.create_tuple_iterator())\n\n# write pred.txt\nprediction = np.array([pred, labels]).transpose()\nnp.savetxt(\"./ckpt/pred.txt\", prediction, fmt=\"%s\", header=\"pred \\t real\")\n</code></pre>"},{"location":"zh/how_to_guides/finetune_with_a_custom_dataset/#_17","title":"\u9644\u5f55","text":"<p>\u4ee5\u4e0b\u8868\u683c\u5c55\u793a\u4e86\u4f7f\u7528MindCV\u5728\u591a\u4e2aCNN\u6a21\u578b\u4e0a\u5bf9Aircraft\u6570\u636e\u96c6\u8fdb\u884c\u5168\u91cf\u5fae\u8c03\u7684\u7cbe\u5ea6\uff08Top 1%\uff09\u5bf9\u6bd4\u4fe1\u606f\uff0c\u8be5\u6570\u636e\u96c6\u4e0a\u53ef\u5b9e\u73b0\u7684\u5206\u7c7b\u7cbe\u5ea6\u53c2\u89c1Aircraft leaderboard\u548cpaperwithcode\u7f51\u7ad9\u3002</p> \u6a21\u578b MindCV\u5168\u91cf\u5fae\u8c03\u7cbe\u5ea6 \u53c2\u8003\u7cbe\u5ea6 mobilenet v3-small 88.35% - mobilenet v3-large 92.22% 83.8% convnext-tiny 93.69% 84.23% resnest50 86.82% -"},{"location":"zh/how_to_guides/write_a_new_model/","title":"\u6a21\u578b\u7f16\u5199\u6307\u5357","text":"<p>\u672c\u6587\u6863\u63d0\u4f9b\u4e86\u7f16\u5199MindSpore\u5957\u4ef6\u4e2d\u7684\u6a21\u578b\u5b9a\u4e49\u6587\u4ef6<code>model.py</code>\u7684\u53c2\u8003\u6a21\u677f\uff0c\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u7edf\u4e00\u7684\u4ee3\u7801\u98ce\u683c\u3002</p> <p>\u63a5\u4e0b\u6765\u6211\u4eec\u4ee5\u76f8\u5bf9\u7b80\u5355\u7684\u65b0\u6a21\u578b<code>MLP-Mixer</code>\u4f5c\u4e3a\u793a\u4f8b\u3002</p>"},{"location":"zh/how_to_guides/write_a_new_model/#_2","title":"\u6587\u4ef6\u5934","text":"<p>\u8be5\u6587\u4ef6\u7684**\u7b80\u8981\u63cf\u8ff0**\u3002\u5305\u542b\u6a21\u578b\u540d\u79f0\u548c\u8bba\u6587\u9898\u76ee\u3002\u5982\u4e0b\u6240\u793a\uff1a</p> <pre><code>\"\"\"\nMindSpore implementation of `${MODEL_NAME}`.\nRefer to ${PAPER_NAME}.\n\"\"\"\n</code></pre>"},{"location":"zh/how_to_guides/write_a_new_model/#_3","title":"\u6a21\u5757\u5bfc\u5165","text":"<p>\u6a21\u5757\u5bfc\u5165\u5206\u4e3a\u4e09\u79cd\u7c7b\u578b\u3002\u5206\u522b\u4e3a</p> <ul> <li>Python\u539f\u751f\u6216\u7b2c\u4e09\u65b9\u5e93\u3002\u5982<code>import math</code>\u3001<code>import numpy as np</code>\u7b49\u7b49\u3002\u5e94\u5f53\u653e\u5728\u7b2c\u4e00\u68af\u961f\u3002</li> <li>MindSpore\u76f8\u5173\u6a21\u5757\u3002\u5982<code>import mindspore.nn as nn</code>\u3001<code>import mindspore.ops as ops</code>\u7b49\u7b49\u3002\u5e94\u5f53\u653e\u5728\u7b2c\u4e8c\u68af\u961f\u3002</li> <li>\u5957\u4ef6\u5305\u5185\u6a21\u5757\u3002\u5982<code>from .layers.classifier import ClassifierHead</code>\u7b49\u7b49\u3002\u5e94\u5f53\u653e\u5728\u7b2c\u4e09\u68af\u961f\uff0c\u5e76\u4f7f\u7528\u76f8\u5bf9\u5bfc\u5165\u3002</li> </ul> <p>\u793a\u4f8b\u5982\u4e0b\uff1a</p> <pre><code>import math\nfrom collections import OrderedDict\n\nimport mindspore.nn as nn\nimport mindspore.ops as ops\nimport mindspore.common.initializer as init\n\nfrom .utils import load_pretrained\nfrom .layers.classifier import ClassifierHead\n</code></pre> <p>\u4ec5\u5bfc\u5165\u5fc5\u987b\u7684\u6a21\u5757\u6216\u5305\uff0c\u907f\u514d\u5bfc\u5165\u65e0\u7528\u5305\u3002</p>"},{"location":"zh/how_to_guides/write_a_new_model/#__all__","title":"<code>__all__</code>","text":"<p>Python \u6ca1\u6709\u539f\u751f\u7684\u53ef\u89c1\u6027\u63a7\u5236\uff0c\u5176\u53ef\u89c1\u6027\u7684\u7ef4\u62a4\u662f\u9760\u4e00\u5957\u9700\u8981\u5927\u5bb6\u81ea\u89c9\u9075\u5b88\u7684\u201c\u7ea6\u5b9a\u201d\u3002<code>__all__</code> \u662f\u9488\u5bf9\u6a21\u5757\u516c\u5f00\u63a5\u53e3\u7684\u4e00\u79cd\u7ea6\u5b9a\uff0c\u4ee5\u63d0\u4f9b\u4e86\u201d\u767d\u540d\u5355\u201c\u7684\u5f62\u5f0f\u66b4\u9732\u63a5\u53e3\u3002\u5982\u679c\u5b9a\u4e49\u4e86<code>__all__</code>\uff0c\u5176\u4ed6\u6587\u4ef6\u4e2d\u4f7f\u7528<code>from xxx import *</code>\u5bfc\u5165\u8be5\u6587\u4ef6\u65f6\uff0c\u53ea\u4f1a\u5bfc\u5165<code>__all__</code>\u5217\u51fa\u7684\u6210\u5458\uff0c\u53ef\u4ee5\u5176\u4ed6\u6210\u5458\u90fd\u88ab\u6392\u9664\u5728\u5916\u3002</p> <p>\u6211\u4eec\u7ea6\u5b9a\u6a21\u578b\u4e2d\u5bf9\u5916\u66b4\u9732\u7684\u63a5\u53e3\u5305\u62ec\u4e3b\u6a21\u578b\u7c7b\u4ee5\u53ca\u8fd4\u56de\u4e0d\u540c\u89c4\u683c\u6a21\u578b\u7684\u51fd\u6570\uff0c\u4f8b\u5982\uff1a</p> <pre><code>__all__ = [\n    \"MLPMixer\",\n    \"mlp_mixer_s_p32\",\n    \"mlp_mixer_s_p16\",\n    ...\n]\n</code></pre> <p>\u5176\u4e2d<code>\"MLPMixer\"</code>\u662f\u4e3b\u6a21\u578b\u7c7b\uff0c<code>\"mlp_mixer_s_p32\"</code>\u548c<code>\"mlp_mixer_s_p16\"</code>\u7b49\u662f\u8fd4\u56de\u4e0d\u540c\u89c4\u683c\u6a21\u578b\u7684\u51fd\u6570\u3002\u4e00\u822c\u6765\u8bf4\u5b50\u6a21\u578b\uff0c\u5373\u67d0<code>Layer</code>\u6216\u67d0<code>Block</code>\u662f\u4e0d\u5e94\u8be5\u88ab\u5176\u4ed6\u6587\u4ef6\u6240\u5171\u7528\u7684\u3002\u5982\u82e5\u6b64\uff0c\u5e94\u5f53\u8003\u8651\u5c06\u8be5\u5b50\u6a21\u578b\u63d0\u53d6\u5230<code>${MINDCLS}/models/layers</code>\u4e0b\u9762\u4f5c\u4e3a\u516c\u7528\u6a21\u5757\uff0c\u5982<code>SEBlock</code>\u7b49\u3002</p>"},{"location":"zh/how_to_guides/write_a_new_model/#_4","title":"\u5b50\u6a21\u578b","text":"<p>\u6211\u4eec\u90fd\u77e5\u9053\u4e00\u4e2a\u6df1\u5ea6\u6a21\u578b\u662f\u7531\u591a\u5c42\u7ec4\u6210\u7684\u7f51\u7edc\u3002\u5176\u4e2d\u67d0\u4e9b\u5c42\u53ef\u4ee5\u7ec4\u6210\u76f8\u540c\u62d3\u6251\u7ed3\u6784\u7684\u5b50\u6a21\u578b\uff0c\u6211\u4eec\u4e00\u822c\u79f0\u5176\u4e3a<code>Layer</code>\u6216\u8005<code>Block</code>\uff0c\u4f8b\u5982<code>ResidualBlock</code>\u7b49\u3002\u8fd9\u79cd\u62bd\u8c61\u6709\u5229\u4e8e\u6211\u4eec\u7406\u89e3\u6574\u4e2a\u6a21\u578b\u7ed3\u6784\uff0c\u4e5f\u6709\u5229\u4e8e\u4ee3\u7801\u7684\u7f16\u5199\u3002</p> <p>\u6211\u4eec\u5e94\u5f53\u901a\u8fc7\u7c7b\u6ce8\u91ca\u5bf9\u5b50\u6a21\u578b\u8fdb\u884c\u529f\u80fd\u7684\u7b80\u8981\u63cf\u8ff0\u3002\u5728<code>MindSpore</code>\u4e2d\uff0c\u6a21\u578b\u7684\u7c7b\u7ee7\u627f\u4e8e<code>nn.Cell</code>\uff0c\u4e00\u822c\u6765\u8bf4\u6211\u4eec\u9700\u8981\u91cd\u8f7d\u4ee5\u4e0b\u4e24\u4e2a\u51fd\u6570\uff1a</p> <ul> <li>\u5728<code>__init__</code>\u51fd\u6570\u4e2d\uff0c\u6211\u4eec\u5e94\u5f53\u5b9a\u4e49\u6a21\u578b\u4e2d\u9700\u8981\u7528\u5230\u7684\u795e\u7ecf\u7f51\u7edc\u5c42\uff08<code>__init__</code>\u4e2d\u7684\u53c2\u6570\u8981\u8fdb\u884c\u53c2\u6570\u7c7b\u578b\u58f0\u660e\uff0c\u5373type hint\uff09\u3002</li> <li>\u5728<code>construct</code>\u51fd\u6570\u4e2d\u6211\u4eec\u5b9a\u4e49\u6a21\u578b\u524d\u5411\u903b\u8f91\u3002</li> </ul> <p>\u793a\u4f8b\u5982\u4e0b\uff1a</p> <pre><code>class MixerBlock(nn.Cell):\n    \"\"\"Mixer Layer with token-mixing MLP and channel-mixing MLP\"\"\"\n\n    def __init__(self,\n                 n_patches: int,\n                 n_channels: int,\n                 token_dim: int,\n                 channel_dim: int,\n                 dropout: float = 0.\n                 ) -&gt; None:\n        super().__init__()\n        self.token_mix = nn.SequentialCell(\n            nn.LayerNorm((n_channels,)),\n            TransPose((0, 2, 1)),\n            FeedForward(n_patches, token_dim, dropout),\n            TransPose((0, 2, 1))\n        )\n        self.channel_mix = nn.SequentialCell(\n            nn.LayerNorm((n_channels,)),\n            FeedForward(n_channels, channel_dim, dropout),\n        )\n\n    def construct(self, x):\n        x = x + self.token_mix(x)\n        x = x + self.channel_mix(x)\n        return x\n</code></pre> <p>\u5728<code>nn.Cell</code>\u7c7b\u7684\u7f16\u5199\u8fc7\u7a0b\u4e2d\uff0c\u6709\u4e24\u4e2a\u503c\u5f97\u6ce8\u610f\u7684\u65b9\u9762</p> <ul> <li> <p>CellList &amp; SequentialCell</p> </li> <li> <p>CellList is just a container that contains a list of neural network layers(Cell). The Cells contained by it can be properly registered, and will be visible by all Cell methods. We must overwrite the forward calculation, that is, the construct function.</p> </li> <li> <p>SequentialCell is a container that holds a sequential list of layers(Cell). The Cells may have a name(OrderedDict) or not(List). We don't need to implement forward computation, which is done according to the order of the sequential list.</p> </li> <li> <p>construct</p> </li> <li> <p>Assert is not supported. [RuntimeError: ParseStatement] Unsupported statement 'Assert'.</p> </li> <li> <p>Usage of single operator\u3002\u8c03\u7528\u7b97\u5b50\u65f6\uff08\u5982concat, reshape, mean\uff09\uff0c\u4f7f\u7528\u51fd\u6570\u5f0f\u63a5\u53e3 mindspore.ops.functional (\u5982 output=ops.concat((x1, x2)))\uff0c\u907f\u514d\u5148\u5728__init__\u4e2d\u5b9e\u4f8b\u5316\u539f\u59cb\u7b97\u5b50 ops.Primitive  (\u5982self.concat=ops.Concat()) \u518d\u5728construct\u4e2d\u8c03\u7528\uff08output=self.concat((x1, x2))\uff09\u3002</p> </li> </ul>"},{"location":"zh/how_to_guides/write_a_new_model/#_5","title":"\u4e3b\u6a21\u578b","text":"<p>\u4e3b\u6a21\u578b\u662f\u8bba\u6587\u4e2d\u6240\u63d0\u51fa\u7684\u7f51\u7edc\u6a21\u578b\u5b9a\u4e49\uff0c\u7531\u591a\u4e2a\u5b50\u6a21\u578b\u5806\u53e0\u800c\u6210\u3002\u5b83\u662f\u9002\u7528\u4e8e\u5206\u7c7b\u3001\u68c0\u6d4b\u7b49\u4efb\u52a1\u7684\u6700\u9876\u5c42\u7f51\u7edc\u3002\u5b83\u5728\u4ee3\u7801\u4e66\u5199\u4e0a\u4e0e\u5b50\u6a21\u578b\u4e0a\u57fa\u672c\u7c7b\u4f3c\uff0c\u4f46\u6709\u51e0\u5904\u4e0d\u540c\u3002</p> <ul> <li>\u7c7b\u6ce8\u91ca\u3002\u6211\u4eec\u5e94\u5f53\u5728\u6b64\u7ed9\u51fa\u8bba\u6587\u7684\u9898\u76ee\u548c\u94fe\u63a5\u3002\u53e6\u5916\u7531\u4e8e\u8be5\u7c7b\u5bf9\u5916\u66b4\u9732\uff0c\u6211\u4eec\u6700\u597d\u4e5f\u52a0\u4e0a\u7c7b\u521d\u59cb\u5316\u53c2\u6570\u7684\u8bf4\u660e\u3002\u8be6\u89c1\u4e0b\u65b9\u4ee3\u7801\u3002</li> <li><code>forward_features</code>\u51fd\u6570\u3002\u5728\u51fd\u6570\u5185\u5bf9\u6a21\u578b\u7684\u7279\u5f81\u7f51\u7edc\u7684\u8fd0\u7b97\u5b9a\u4e49\u3002</li> <li><code>forward_head</code>\u51fd\u6570\u3002\u5728\u51fd\u6570\u5185\u5bf9\u6a21\u578b\u7684\u5206\u7c7b\u5668\u7684\u8fd0\u7b97\u8fdb\u884c\u5b9a\u4e49\u3002</li> <li><code>construct</code>\u51fd\u6570\u3002\u5728\u51fd\u6570\u8c03\u7528\u7279\u5f81\u7f51\u7edc\u548c\u5206\u7c7b\u5668\u7684\u8fd0\u7b97\u3002</li> <li><code>_initialize_weights</code>\u51fd\u6570\u3002\u6211\u4eec\u7ea6\u5b9a\u6a21\u578b\u53c2\u6570\u7684\u968f\u673a\u521d\u59cb\u5316\u7531\u8be5\u6210\u5458\u51fd\u6570\u5b8c\u6210\u3002\u8be6\u89c1\u4e0b\u65b9\u4ee3\u7801\u3002</li> </ul> <p>\u793a\u4f8b\u5982\u4e0b\uff1a</p> <pre><code>class MLPMixer(nn.Cell):\n    r\"\"\"MLP-Mixer model class, based on\n    `\"MLP-Mixer: An all-MLP Architecture for Vision\" &lt;https://arxiv.org/abs/2105.01601&gt;`_\n\n    Args:\n        depth (int) : number of MixerBlocks.\n        patch_size (Union[int, tuple]) : size of a single image patch.\n        n_patches (int) : number of patches.\n        n_channels (int) : channels(dimension) of a single embedded patch.\n        token_dim (int) : hidden dim of token-mixing MLP.\n        channel_dim (int) : hidden dim of channel-mixing MLP.\n        in_channels(int): number the channels of the input. Default: 3.\n        n_classes (int) : number of classification classes. Default: 1000.\n    \"\"\"\n\n    def __init__(self,\n                 depth: int,\n                 patch_size: Union[int, tuple],\n                 n_patches: int,\n                 n_channels: int,\n                 token_dim: int,\n                 channel_dim: int,\n                 in_channels: int = 3,\n                 n_classes: int = 1000,\n                 ) -&gt; None:\n        super().__init__()\n        self.n_patches = n_patches\n        self.n_channels = n_channels\n        # patch with shape of (3, patch_size, patch_size) is embedded to n_channels dim feature.\n        self.to_patch_embedding = nn.SequentialCell(\n            nn.Conv2d(in_chans, n_channels, patch_size, patch_size, pad_mode=\"pad\", padding=0),\n            TransPose(permutation=(0, 2, 1), embedding=True),\n        )\n        self.mixer_blocks = nn.SequentialCell()\n        for _ in range(depth):\n            self.mixer_blocks.append(MixerBlock(n_patches, n_channels, token_dim, channel_dim))\n        self.layer_norm = nn.LayerNorm((n_channels,))\n        self.mlp_head = nn.Dense(n_channels, n_classes)\n        self._initialize_weights()\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.to_patch_embedding(x)\n        x = self.mixer_blocks(x)\n        x = self.layer_norm(x)\n        return ops.mean(x, 1)\n\n    def forward_head(self, x: Tensor)-&gt; Tensor:\n        return self.mlp_head(x)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        return self.forward_head(x)\n\n    def _initialize_weights(self) -&gt; None:\n        for name, m in self.cells_and_names():\n            if isinstance(m, nn.Conv2d):\n                m.weight.set_data(init.initializer(init.Normal(0.01, 0), m.weight.shape))\n                if m.bias is not None:\n                    m.bias.set_data(init.initializer(init.Constant(0), m.bias.shape))\n            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n                m.gamma.set_data(init.initializer(init.Constant(1), m.gamma.shape))\n                if m.beta is not None:\n                    m.beta.set_data(init.initializer(init.Constant(0.0001), m.beta.shape))\n            elif isinstance(m, nn.Dense):\n                m.weight.set_data(init.initializer(init.Normal(0.01, 0), m.weight.shape))\n                if m.bias is not None:\n                    m.bias.set_data(init.initializer(init.Constant(0), m.bias.shape))\n</code></pre>"},{"location":"zh/how_to_guides/write_a_new_model/#_6","title":"\u89c4\u683c\u51fd\u6570","text":"<p>\u8bba\u6587\u4e2d\u6240\u63d0\u51fa\u7684\u6a21\u578b\u53ef\u80fd\u6709\u4e0d\u540c\u89c4\u683c\u7684\u53d8\u79cd\uff0c\u5982<code>channel</code>\u7684\u5927\u5c0f\u3001<code>depth</code>\u7684\u5927\u5c0f\u7b49\u7b49\u3002\u8fd9\u4e9b\u53d8\u79cd\u7684\u5177\u4f53\u914d\u7f6e\u5e94\u8be5\u901a\u8fc7\u89c4\u683c\u51fd\u6570\u4f53\u73b0\uff0c\u89c4\u683c\u7684\u63a5\u53e3\u53c2\u6570\uff1a pretrained, num_classes, in_channels \u547d\u540d\u8981\u7edf\u4e00\uff0c\u540c\u65f6\u5728\u89c4\u683c\u51fd\u6570\u5185\u8fd8\u8981\u8fdb\u884cpretrain loading\u64cd\u4f5c\u3002\u6bcf\u4e00\u4e2a\u89c4\u683c\u51fd\u6570\u5bf9\u5e94\u4e00\u79cd\u786e\u5b9a\u914d\u7f6e\u7684\u89c4\u683c\u53d8\u79cd\u3002\u914d\u7f6e\u901a\u8fc7\u5165\u53c2\u4f20\u5165\u4e3b\u6a21\u578b\u7c7b\u7684\u5b9a\u4e49\uff0c\u5e76\u8fd4\u56de\u5b9e\u4f8b\u5316\u7684\u4e3b\u6a21\u578b\u7c7b\u3002\u53e6\u5916\uff0c\u8fd8\u9700\u901a\u8fc7\u6dfb\u52a0\u88c5\u9970\u5668<code>@register_model</code>\u5c06\u8be5\u6a21\u578b\u7684\u6b64\u89c4\u683c\u6ce8\u518c\u5230\u5305\u5185\u3002</p> <p>\u793a\u4f8b\u5982\u4e0b\uff1a</p> <pre><code>@register_model\ndef mlp_mixer_s_p16(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs):\n    nl, pr, ls, hs, ds, dc = 8, 16, 196, 512, 256, 2048\n    _check_resolution_and_length_of_patch(pr, ls)\n    model = MLPMixer(depth=nl, patch_size=pr, n_patches=ls, n_channels=hs, token_dim=ds,\n                    channel_dim=dc, in_chans=in_chans, n_classes=num_classes, **kwargs)\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n    return model\n\n@register_model\ndef mlp_mixer_b_p32(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs):\n    nl, pr, ls, hs, ds, dc = 12, 32, 49, 768, 384, 3072\n    _check_resolution_and_length_of_patch(pr, ls)\n    model = MLPMixer(depth=nl, patch_size=pr, n_patches=ls, n_channels=hs, token_dim=ds,\n                    channel_dim=dc, in_chans=in_chans, n_classes=num_classes, **kwargs)\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n    return model\n</code></pre>"},{"location":"zh/how_to_guides/write_a_new_model/#main","title":"\u9a8c\u8bc1main\uff08\u53ef\u9009\uff09","text":"<p>\u521d\u59cb\u7f16\u5199\u9636\u6bb5\u5e94\u5f53\u4fdd\u8bc1\u6a21\u578b\u662f\u53ef\u8fd0\u884c\u7684\u3002\u53ef\u901a\u8fc7\u4e0b\u8ff0\u4ee3\u7801\u5757\u8fdb\u884c\u57fa\u7840\u9a8c\u8bc1\uff1a</p> <pre><code>if __name__ == '__main__':\n    import numpy as np\n    import mindspore\n    from mindspore import Tensor\n\n    model = mlp_mixer_s_p16()\n    print(model)\n    dummy_input = Tensor(np.random.rand(8, 3, 224, 224), dtype=mindspore.float32)\n    y = model(dummy_input)\n    print(y.shape)\n</code></pre>"},{"location":"zh/how_to_guides/write_a_new_model/#_7","title":"\u53c2\u8003\u793a\u4f8b","text":"<ul> <li>densenet.py</li> <li>shufflenetv1.py</li> <li>shufflenetv2.py</li> <li>mixnet.py</li> <li>mlp_mixer.py</li> </ul>"},{"location":"zh/notes/changelog/","title":"\u66f4\u65b0\u65e5\u5fd7","text":"<p>\u5373\u5c06\u5230\u6765</p>"},{"location":"zh/notes/code_of_conduct/","title":"\u884c\u4e3a\u51c6\u5219","text":"<p>\u5373\u5c06\u5230\u6765</p>"},{"location":"zh/notes/faq/","title":"\u5e38\u89c1\u95ee\u9898","text":"<p>\u5373\u5c06\u5230\u6765</p>"},{"location":"zh/tutorials/configuration/","title":"\u914d\u7f6e","text":"<p>MindCV\u5957\u4ef6\u53ef\u4ee5\u901a\u8fc7python\u7684argparse\u5e93\u548cPyYAML\u5e93\u89e3\u6790\u6a21\u578b\u7684yaml\u6587\u4ef6\u6765\u8fdb\u884c\u53c2\u6570\u7684\u914d\u7f6e\u3002 \u4e0b\u9762\u6211\u4eec\u4ee5squeezenet_1.0\u6a21\u578b\u4e3a\u4f8b\uff0c\u89e3\u91ca\u5982\u4f55\u914d\u7f6e\u76f8\u5e94\u7684\u53c2\u6570\u3002</p>"},{"location":"zh/tutorials/configuration/#_2","title":"\u57fa\u7840\u73af\u5883","text":"<ol> <li>\u53c2\u6570\u8bf4\u660e</li> </ol> <ul> <li> <p>mode\uff1a\u4f7f\u7528\u9759\u6001\u56fe\u6a21\u5f0f\uff080\uff09\u6216\u52a8\u6001\u56fe\u6a21\u5f0f\uff081\uff09\u3002</p> </li> <li> <p>distribute\uff1a\u662f\u5426\u4f7f\u7528\u5206\u5e03\u5f0f\u3002</p> </li> </ul> <ol> <li> <p>yaml\u6587\u4ef6\u6837\u4f8b</p> <pre><code>mode: 0\ndistribute: True\n...\n</code></pre> </li> <li> <p>parse\u53c2\u6570\u8bbe\u7f6e</p> <pre><code>python train.py --mode 0 --distribute False ...\n</code></pre> </li> <li> <p>\u5bf9\u5e94\u7684\u4ee3\u7801\u793a\u4f8b</p> <p><code>args.mode</code>\u4ee3\u8868\u53c2\u6570<code>mode</code>, <code>args.distribute</code>\u4ee3\u8868\u53c2\u6570<code>distribute</code>\u3002</p> <pre><code>def train(args):\n    ms.set_context(mode=args.mode)\n\n    if args.distribute:\n        init()\n        device_num = get_group_size()\n        rank_id = get_rank()\n        ms.set_auto_parallel_context(device_num=device_num,\n                                     parallel_mode='data_parallel',\n                                     gradients_mean=True)\n    else:\n        device_num = None\n        rank_id = None\n    ...\n</code></pre> </li> </ol>"},{"location":"zh/tutorials/configuration/#_3","title":"\u6570\u636e\u96c6","text":"<ol> <li>\u53c2\u6570\u8bf4\u660e</li> </ol> <ul> <li> <p>dataset\uff1a\u6570\u636e\u96c6\u540d\u79f0\u3002</p> </li> <li> <p>data_dir\uff1a\u6570\u636e\u96c6\u6587\u4ef6\u6240\u5728\u8def\u5f84\u3002</p> </li> <li> <p>shuffle\uff1a\u662f\u5426\u8fdb\u884c\u6570\u636e\u6df7\u6d17\u3002</p> </li> <li> <p>dataset_download\uff1a\u662f\u5426\u4e0b\u8f7d\u6570\u636e\u96c6\u3002</p> </li> <li> <p>batch_size\uff1a\u6bcf\u4e2a\u6279\u5904\u7406\u6570\u636e\u5305\u542b\u7684\u6570\u636e\u6761\u76ee\u3002</p> </li> <li> <p>drop_remainder\uff1a\u5f53\u6700\u540e\u4e00\u4e2a\u6279\u5904\u7406\u6570\u636e\u5305\u542b\u7684\u6570\u636e\u6761\u76ee\u5c0f\u4e8e batch_size \u65f6\uff0c\u662f\u5426\u5c06\u8be5\u6279\u5904\u7406\u4e22\u5f03\u3002</p> </li> <li> <p>num_parallel_workers\uff1a\u8bfb\u53d6\u6570\u636e\u7684\u5de5\u4f5c\u7ebf\u7a0b\u6570\u3002</p> </li> </ul> <ol> <li> <p>yaml\u6587\u4ef6\u6837\u4f8b</p> <pre><code>dataset: 'imagenet'\ndata_dir: './imagenet2012'\nshuffle: True\ndataset_download: False\nbatch_size: 32\ndrop_remainder: True\nnum_parallel_workers: 8\n...\n</code></pre> </li> <li> <p>parse\u53c2\u6570\u8bbe\u7f6e</p> <pre><code>python train.py ... --dataset imagenet --data_dir ./imagenet2012 --shuffle True \\\n    --dataset_download False --batch_size 32 --drop_remainder True \\\n    --num_parallel_workers 8 ...\n</code></pre> </li> <li> <p>\u5bf9\u5e94\u7684\u4ee3\u7801\u793a\u4f8b</p> <pre><code>def train(args):\n    ...\n    dataset_train = create_dataset(\n        name=args.dataset,\n        root=args.data_dir,\n        split='train',\n        shuffle=args.shuffle,\n        num_samples=args.num_samples,\n        num_shards=device_num,\n        shard_id=rank_id,\n        num_parallel_workers=args.num_parallel_workers,\n        download=args.dataset_download,\n        num_aug_repeats=args.aug_repeats)\n\n    ...\n    target_transform = transforms.OneHot(num_classes) if args.loss == 'BCE' else None\n\n    loader_train = create_loader(\n        dataset=dataset_train,\n        batch_size=args.batch_size,\n        drop_remainder=args.drop_remainder,\n        is_training=True,\n        mixup=args.mixup,\n        cutmix=args.cutmix,\n        cutmix_prob=args.cutmix_prob,\n        num_classes=args.num_classes,\n        transform=transform_list,\n        target_transform=target_transform,\n        num_parallel_workers=args.num_parallel_workers,\n    )\n    ...\n</code></pre> </li> </ol>"},{"location":"zh/tutorials/configuration/#_4","title":"\u6570\u636e\u589e\u5f3a","text":"<ol> <li>\u53c2\u6570\u8bf4\u660e</li> </ol> <ul> <li> <p>image_resize\uff1a\u56fe\u50cf\u7684\u8f93\u51fa\u5c3a\u5bf8\u5927\u5c0f\u3002</p> </li> <li> <p>scale\uff1a\u8981\u88c1\u526a\u7684\u539f\u59cb\u5c3a\u5bf8\u5927\u5c0f\u7684\u5404\u4e2a\u5c3a\u5bf8\u7684\u8303\u56f4\u3002</p> </li> <li> <p>ratio\uff1a\u88c1\u526a\u5bbd\u9ad8\u6bd4\u7684\u8303\u56f4\u3002</p> </li> <li> <p>hfilp\uff1a\u56fe\u50cf\u88ab\u7ffb\u8f6c\u7684\u6982\u7387\u3002</p> </li> <li> <p>interpolation\uff1a\u56fe\u50cf\u63d2\u503c\u65b9\u5f0f\u3002</p> </li> <li> <p>crop_pct\uff1a\u8f93\u5165\u56fe\u50cf\u4e2d\u5fc3\u88c1\u526a\u767e\u5206\u6bd4\u3002</p> </li> <li> <p>color_jitter\uff1a\u989c\u8272\u6296\u52a8\u56e0\u5b50\uff08\u4eae\u5ea6\u8c03\u6574\u56e0\u5b50\uff0c\u5bf9\u6bd4\u5ea6\u8c03\u6574\u56e0\u5b50\uff0c\u9971\u548c\u5ea6\u8c03\u6574\u56e0\u5b50\uff09\u3002</p> </li> <li> <p>re_prob\uff1a\u6267\u884c\u968f\u673a\u64e6\u9664\u7684\u6982\u7387\u3002</p> </li> </ul> <ol> <li> <p>yaml\u6587\u4ef6\u6837\u4f8b</p> <pre><code>image_resize: 224\nscale: [0.08, 1.0]\nratio: [0.75, 1.333]\nhflip: 0.5\ninterpolation: 'bilinear'\ncrop_pct: 0.875\ncolor_jitter: [0.4, 0.4, 0.4]\nre_prob: 0.5\n...\n</code></pre> </li> <li> <p>parse\u53c2\u6570\u8bbe\u7f6e</p> <pre><code>python train.py ... --image_resize 224 --scale [0.08, 1.0] --ratio [0.75, 1.333] \\\n    --hflip 0.5 --interpolation \"bilinear\" --crop_pct 0.875 \\\n    --color_jitter [0.4, 0.4, 0.4] --re_prob 0.5 ...\n</code></pre> </li> <li> <p>\u5bf9\u5e94\u7684\u4ee3\u7801\u793a\u4f8b</p> <pre><code>def train(args):\n    ...\n    transform_list = create_transforms(\n        dataset_name=args.dataset,\n        is_training=True,\n        image_resize=args.image_resize,\n        scale=args.scale,\n        ratio=args.ratio,\n        hflip=args.hflip,\n        vflip=args.vflip,\n        color_jitter=args.color_jitter,\n        interpolation=args.interpolation,\n        auto_augment=args.auto_augment,\n        mean=args.mean,\n        std=args.std,\n        re_prob=args.re_prob,\n        re_scale=args.re_scale,\n        re_ratio=args.re_ratio,\n        re_value=args.re_value,\n        re_max_attempts=args.re_max_attempts\n    )\n    ...\n</code></pre> </li> </ol>"},{"location":"zh/tutorials/configuration/#_5","title":"\u6a21\u578b","text":"<ol> <li>\u53c2\u6570\u8bf4\u660e</li> </ol> <ul> <li> <p>model\uff1a\u6a21\u578b\u540d\u79f0\u3002</p> </li> <li> <p>num_classes\uff1a\u5206\u7c7b\u7684\u7c7b\u522b\u6570\u3002</p> </li> <li> <p>pretrained\uff1a\u662f\u5426\u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\u3002</p> </li> <li> <p>ckpt_path\uff1a\u53c2\u6570\u6587\u4ef6\u6240\u5728\u7684\u8def\u5f84\u3002</p> </li> <li> <p>keep_checkpoint_max\uff1a\u6700\u591a\u4fdd\u5b58\u591a\u5c11\u4e2acheckpoint\u6587\u4ef6\u3002</p> </li> <li> <p>ckpt_save_dir\uff1a\u4fdd\u5b58\u53c2\u6570\u6587\u4ef6\u7684\u8def\u5f84\u3002</p> </li> <li> <p>epoch_size\uff1a\u8bad\u7ec3\u6267\u884c\u8f6e\u6b21\u3002</p> </li> <li> <p>dataset_sink_mode\uff1a\u6570\u636e\u662f\u5426\u76f4\u63a5\u4e0b\u6c89\u81f3\u5904\u7406\u5668\u8fdb\u884c\u5904\u7406\u3002</p> </li> <li> <p>amp_level\uff1a\u6df7\u5408\u7cbe\u5ea6\u7b49\u7ea7\u3002</p> </li> </ul> <ol> <li> <p>yaml\u6587\u4ef6\u6837\u4f8b</p> <pre><code>model: 'squeezenet1_0'\nnum_classes: 1000\npretrained: False\nckpt_path: './squeezenet1_0_gpu.ckpt'\nkeep_checkpoint_max: 10\nckpt_save_dir: './ckpt/'\nepoch_size: 200\ndataset_sink_mode: True\namp_level: 'O0'\n...\n</code></pre> </li> <li> <p>parse\u53c2\u6570\u8bbe\u7f6e</p> <pre><code>python train.py ... --model squeezenet1_0 --num_classes 1000 --pretrained False \\\n    --ckpt_path ./squeezenet1_0_gpu.ckpt --keep_checkpoint_max 10 \\\n    --ckpt_save_path ./ckpt/ --epoch_size 200 --dataset_sink_mode True \\\n    --amp_level O0 ...\n</code></pre> </li> <li> <p>\u5bf9\u5e94\u7684\u4ee3\u7801\u793a\u4f8b</p> <pre><code>def train(args):\n    ...\n    network = create_model(model_name=args.model,\n        num_classes=args.num_classes,\n        in_channels=args.in_channels,\n        drop_rate=args.drop_rate,\n        drop_path_rate=args.drop_path_rate,\n        pretrained=args.pretrained,\n        checkpoint_path=args.ckpt_path,\n        ema=args.ema\n    )\n    ...\n</code></pre> </li> </ol>"},{"location":"zh/tutorials/configuration/#_6","title":"\u635f\u5931\u51fd\u6570","text":"<ol> <li>\u53c2\u6570\u8bf4\u660e</li> </ol> <ul> <li> <p>loss\uff1a\u635f\u5931\u51fd\u6570\u7684\u7b80\u79f0\u3002</p> </li> <li> <p>label_smoothing\uff1a\u6807\u7b7e\u5e73\u6ed1\u503c\uff0c\u7528\u4e8e\u8ba1\u7b97Loss\u65f6\u9632\u6b62\u6a21\u578b\u8fc7\u62df\u5408\u7684\u6b63\u5219\u5316\u624b\u6bb5\u3002</p> </li> </ul> <ol> <li> <p>yaml\u6587\u4ef6\u6837\u4f8b</p> <pre><code>loss: 'CE'\nlabel_smoothing: 0.1\n...\n</code></pre> </li> <li> <p>parse\u53c2\u6570\u8bbe\u7f6e</p> <pre><code>python train.py ... --loss CE --label_smoothing 0.1 ...\n</code></pre> </li> <li> <p>\u5bf9\u5e94\u7684\u4ee3\u7801\u793a\u4f8b</p> <pre><code>def train(args):\n    ...\n    loss = create_loss(name=args.loss,\n        reduction=args.reduction,\n        label_smoothing=args.label_smoothing,\n        aux_factor=args.aux_factor\n    )\n    ...\n</code></pre> </li> </ol>"},{"location":"zh/tutorials/configuration/#_7","title":"\u5b66\u4e60\u7387\u7b56\u7565","text":"<ol> <li>\u53c2\u6570\u8bf4\u660e</li> </ol> <ul> <li> <p>scheduler\uff1a\u5b66\u4e60\u7387\u7b56\u7565\u7684\u540d\u79f0\u3002</p> </li> <li> <p>min_lr\uff1a\u5b66\u4e60\u7387\u7684\u6700\u5c0f\u503c\u3002</p> </li> <li> <p>lr\uff1a\u5b66\u4e60\u7387\u7684\u6700\u5927\u503c\u3002</p> </li> <li> <p>warmup_epochs\uff1a\u5b66\u4e60\u7387warmup\u7684\u8f6e\u6b21\u3002</p> </li> <li> <p>decay_epochs\uff1a\u8fdb\u884c\u8870\u51cf\u7684step\u6570\u3002</p> </li> </ul> <ol> <li> <p>yaml\u6587\u4ef6\u6837\u4f8b</p> <pre><code>scheduler: 'cosine_decay'\nmin_lr: 0.0\nlr: 0.01\nwarmup_epochs: 0\ndecay_epochs: 200\n...\n</code></pre> </li> <li> <p>parse\u53c2\u6570\u8bbe\u7f6e</p> <pre><code>python train.py ... --scheduler cosine_decay --min_lr 0.0 --lr 0.01 \\\n    --warmup_epochs 0 --decay_epochs 200 ...\n</code></pre> </li> <li> <p>\u5bf9\u5e94\u7684\u4ee3\u7801\u793a\u4f8b</p> <pre><code>def train(args):\n    ...\n    lr_scheduler = create_scheduler(num_batches,\n        scheduler=args.scheduler,\n        lr=args.lr,\n        min_lr=args.min_lr,\n        warmup_epochs=args.warmup_epochs,\n        warmup_factor=args.warmup_factor,\n        decay_epochs=args.decay_epochs,\n        decay_rate=args.decay_rate,\n        milestones=args.multi_step_decay_milestones,\n        num_epochs=args.epoch_size,\n        lr_epoch_stair=args.lr_epoch_stair\n    )\n    ...\n</code></pre> </li> </ol>"},{"location":"zh/tutorials/configuration/#_8","title":"\u4f18\u5316\u5668","text":"<ol> <li>\u53c2\u6570\u8bf4\u660e</li> </ol> <ul> <li> <p>opt\uff1a\u4f18\u5316\u5668\u540d\u79f0\u3002</p> </li> <li> <p>weight_decay_filter\uff1a\u6743\u91cd\u8870\u51cf\u8fc7\u6ee4\u5668 \uff08\u8fc7\u6ee4\u4e00\u4e9b\u53c2\u6570\uff0c \u4f7f\u5176\u5728\u8ddf\u65b0\u65f6\u4e0d\u505a\u6743\u91cd\u8870\u51cf\uff09\u3002</p> </li> <li> <p>momentum\uff1a\u79fb\u52a8\u5e73\u5747\u7684\u52a8\u91cf\u3002</p> </li> <li> <p>weight_decay\uff1a\u6743\u91cd\u8870\u51cf\uff08L2 penalty\uff09\u3002</p> </li> <li> <p>loss_scale\uff1a\u68af\u5ea6\u7f29\u653e\u7cfb\u6570</p> </li> <li> <p>use_nesterov\uff1a\u662f\u5426\u4f7f\u7528Nesterov Accelerated Gradient (NAG)\u7b97\u6cd5\u66f4\u65b0\u68af\u5ea6\u3002</p> </li> </ul> <ol> <li> <p>yaml\u6587\u4ef6\u6837\u4f8b</p> <pre><code>opt: 'momentum'\nweight_decay_filter: 'norm_and_bias'\nmomentum: 0.9\nweight_decay: 0.00007\nloss_scale: 1024\nuse_nesterov: False\n...\n</code></pre> </li> <li> <p>parse\u53c2\u6570\u8bbe\u7f6e</p> <pre><code>python train.py ... --opt momentum --weight_decay_filter 'norm_and_bias\" --weight_decay 0.00007 \\\n    --loss_scale 1024 --use_nesterov False ...\n</code></pre> </li> <li> <p>\u5bf9\u5e94\u7684\u4ee3\u7801\u793a\u4f8b</p> <pre><code>def train(args):\n    ...\n    if args.ema:\n        optimizer = create_optimizer(network.trainable_params(),\n            opt=args.opt,\n            lr=lr_scheduler,\n            weight_decay=args.weight_decay,\n            momentum=args.momentum,\n            nesterov=args.use_nesterov,\n            weight_decay_filter=args.weight_decay_filter,\n            loss_scale=args.loss_scale,\n            checkpoint_path=opt_ckpt_path,\n            eps=args.eps\n        )\n    else:\n        optimizer = create_optimizer(network.trainable_params(),\n            opt=args.opt,\n            lr=lr_scheduler,\n            weight_decay=args.weight_decay,\n            momentum=args.momentum,\n            nesterov=args.use_nesterov,\n            weight_decay_filter=args.weight_decay_filter,\n            checkpoint_path=opt_ckpt_path,\n            eps=args.eps\n        )\n    ...\n</code></pre> </li> </ol>"},{"location":"zh/tutorials/configuration/#yamlparse","title":"Yaml\u548cParse\u7ec4\u5408\u4f7f\u7528","text":"<p>\u4f7f\u7528parse\u8bbe\u7f6e\u53c2\u6570\u53ef\u4ee5\u8986\u76d6yaml\u6587\u4ef6\u4e2d\u7684\u53c2\u6570\u8bbe\u7f6e\u3002\u4ee5\u4e0b\u9762\u7684shell\u547d\u4ee4\u4e3a\u4f8b\uff0c</p> <pre><code>python train.py -c ./configs/squeezenet/squeezenet_1.0_gpu.yaml --data_dir ./data\n</code></pre> <p>\u4e0a\u9762\u7684\u547d\u4ee4\u5c06<code>args.data_dir</code>\u53c2\u6570\u7684\u503c\u7531yaml\u6587\u4ef6\u4e2d\u7684 <code>./imagenet2012</code> \u8986\u76d6\u4e3a <code>./data</code>\u3002</p>"},{"location":"zh/tutorials/finetune/","title":"\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u4e0a\u7684\u6a21\u578b\u5fae\u8c03\u8bad\u7ec3","text":"<p>\u5728\u6b64\u6559\u7a0b\u4e2d\uff0c\u60a8\u5c06\u5b66\u4f1a\u5982\u4f55\u4f7f\u7528MindCV\u5957\u4ef6\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60\uff0c\u4ee5\u89e3\u51b3\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u4e0a\u7684\u56fe\u50cf\u5206\u7c7b\u95ee\u9898\u3002 \u5728\u6df1\u5ea6\u5b66\u4e60\u4efb\u52a1\u4e2d\uff0c\u5e38\u89c1\u9047\u5230\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u6b64\u65f6\u76f4\u63a5\u8bad\u7ec3\u6574\u4e2a\u7f51\u7edc\u5f80\u5f80\u96be\u4ee5\u8fbe\u5230\u7406\u60f3\u7684\u7cbe\u5ea6\u3002 \u4e00\u4e2a\u6bd4\u8f83\u597d\u7684\u505a\u6cd5\u662f\uff0c\u4f7f\u7528\u4e00\u4e2a\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a(\u4e0e\u4efb\u52a1\u6570\u636e\u8f83\u4e3a\u63a5\u8fd1)\u9884\u8bad\u7ec3\u597d\u7684\u6a21\u578b\uff0c\u7136\u540e\u4f7f\u7528\u8be5\u6a21\u578b\u6765\u521d\u59cb\u5316\u7f51\u7edc\u7684\u6743\u91cd\u53c2\u6570\u6216\u4f5c\u4e3a\u56fa\u5b9a\u7279\u5f81\u63d0\u53d6\u5668\u5e94\u7528\u4e8e\u7279\u5b9a\u7684\u4efb\u52a1\u4e2d\u3002</p> <p>\u6b64\u6559\u7a0b\u5c06\u4ee5\u4f7f\u7528ImageNet\u4e0a\u9884\u8bad\u7ec3\u7684DenseNet\u6a21\u578b\u4e3a\u4f8b\uff0c\u4ecb\u7ecd\u4e24\u79cd\u4e0d\u540c\u7684\u5fae\u8c03\u7b56\u7565\uff0c\u89e3\u51b3\u5c0f\u6837\u672c\u60c5\u51b5\u4e0b\u72fc\u548c\u72d7\u7684\u56fe\u50cf\u5206\u7c7b\u95ee\u9898:</p> <ol> <li>\u6574\u4f53\u6a21\u578b\u5fae\u8c03\u3002</li> <li>\u51bb\u7ed3\u7279\u5f81\u7f51\u7edc(freeze backbone)\uff0c\u53ea\u5fae\u8c03\u5206\u7c7b\u5668\u3002</li> </ol> <p>\u8fc1\u79fb\u5b66\u4e60\u8be6\u7ec6\u5185\u5bb9\u89c1Stanford University CS231n</p>"},{"location":"zh/tutorials/finetune/#_2","title":"\u6570\u636e\u51c6\u5907","text":""},{"location":"zh/tutorials/finetune/#_3","title":"\u4e0b\u8f7d\u6570\u636e\u96c6","text":"<p>\u4e0b\u8f7d\u6848\u4f8b\u6240\u7528\u5230\u7684\u72d7\u4e0e\u72fc\u5206\u7c7b\u6570\u636e\u96c6\uff0c \u6bcf\u4e2a\u7c7b\u522b\u5404\u6709120\u5f20\u8bad\u7ec3\u56fe\u50cf\u4e0e30\u5f20\u9a8c\u8bc1\u56fe\u50cf\u3002\u4f7f\u7528<code>mindcv.utils.download</code>\u63a5\u53e3\u4e0b\u8f7d\u6570\u636e\u96c6\uff0c\u5e76\u5c06\u4e0b\u8f7d\u540e\u7684\u6570\u636e\u96c6\u81ea\u52a8\u89e3\u538b\u5230\u5f53\u524d\u76ee\u5f55\u4e0b\u3002</p> <pre><code>import os\nfrom mindcv.utils.download import DownLoad\n\ndataset_url = \"https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/datasets/intermediate/Canidae_data.zip\"\nroot_dir = \"./\"\n\nif not os.path.exists(os.path.join(root_dir, 'data/Canidae')):\n    DownLoad().download_and_extract_archive(dataset_url, root_dir)\n</code></pre> <p>\u6570\u636e\u96c6\u7684\u76ee\u5f55\u7ed3\u6784\u5982\u4e0b\uff1a</p> <pre><code>data/\n\u2514\u2500\u2500 Canidae\n    \u251c\u2500\u2500 train\n    \u2502   \u251c\u2500\u2500 dogs\n    \u2502   \u2514\u2500\u2500 wolves\n    \u2514\u2500\u2500 val\n        \u251c\u2500\u2500 dogs\n        \u2514\u2500\u2500 wolves\n</code></pre>"},{"location":"zh/tutorials/finetune/#_4","title":"\u6570\u636e\u96c6\u52a0\u8f7d\u53ca\u5904\u7406","text":""},{"location":"zh/tutorials/finetune/#_5","title":"\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u7684\u52a0\u8f7d","text":"<p>\u901a\u8fc7\u8c03\u7528<code>mindcv.data</code>\u4e2d\u7684<code>create_dataset</code>\u51fd\u6570\uff0c\u6211\u4eec\u53ef\u8f7b\u677e\u5730\u52a0\u8f7d\u9884\u8bbe\u7684\u6570\u636e\u96c6\u4ee5\u53ca\u81ea\u5b9a\u4e49\u7684\u6570\u636e\u96c6\u3002</p> <ul> <li>\u5f53\u53c2\u6570<code>name</code>\u8bbe\u4e3a\u7a7a\u65f6\uff0c\u6307\u5b9a\u4e3a\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u3002(\u9ed8\u8ba4\u503c)</li> <li>\u5f53\u53c2\u6570<code>name</code>\u8bbe\u4e3a<code>MNIST</code>, <code>CIFAR10</code>\u7b49\u6807\u51c6\u6570\u636e\u96c6\u540d\u79f0\u65f6\uff0c\u6307\u5b9a\u4e3a\u9884\u8bbe\u6570\u636e\u96c6\u3002</li> </ul> <p>\u540c\u65f6\uff0c\u6211\u4eec\u9700\u8981\u8bbe\u5b9a\u6570\u636e\u96c6\u7684\u8def\u5f84<code>data_dir</code>\u548c\u6570\u636e\u5207\u5206\u7684\u540d\u79f0<code>split</code> (\u5982train, val)\uff0c\u4ee5\u52a0\u8f7d\u5bf9\u5e94\u7684\u8bad\u7ec3\u96c6\u6216\u8005\u9a8c\u8bc1\u96c6\u3002</p> <pre><code>from mindcv.data import create_dataset, create_transforms, create_loader\n\nnum_workers = 8\n\n# \u6570\u636e\u96c6\u76ee\u5f55\u8def\u5f84\ndata_dir = \"./data/Canidae/\"\n\n# \u52a0\u8f7d\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\ndataset_train = create_dataset(root=data_dir, split='train', num_parallel_workers=num_workers)\ndataset_val = create_dataset(root=data_dir, split='val', num_parallel_workers=num_workers)\n</code></pre> <p>\u6ce8\u610f: \u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u7684\u76ee\u5f55\u7ed3\u6784\u5e94\u4e0eImageNet\u4e00\u6837\uff0c\u5373root -&gt; split -&gt; class -&gt; image \u7684\u5c42\u6b21\u7ed3\u6784</p> <pre><code>DATASET_NAME\n    \u251c\u2500\u2500 split1(e.g. train)/\n    \u2502  \u251c\u2500\u2500 class1/\n    \u2502  \u2502   \u251c\u2500\u2500 000001.jpg\n    \u2502  \u2502   \u251c\u2500\u2500 000002.jpg\n    \u2502  \u2502   \u2514\u2500\u2500 ....\n    \u2502  \u2514\u2500\u2500 class2/\n    \u2502      \u251c\u2500\u2500 000001.jpg\n    \u2502      \u251c\u2500\u2500 000002.jpg\n    \u2502      \u2514\u2500\u2500 ....\n    \u2514\u2500\u2500 split2/\n       \u251c\u2500\u2500 class1/\n       \u2502   \u251c\u2500\u2500 000001.jpg\n       \u2502   \u251c\u2500\u2500 000002.jpg\n       \u2502   \u2514\u2500\u2500 ....\n       \u2514\u2500\u2500 class2/\n           \u251c\u2500\u2500 000001.jpg\n           \u251c\u2500\u2500 000002.jpg\n           \u2514\u2500\u2500 ....\n</code></pre>"},{"location":"zh/tutorials/finetune/#_6","title":"\u6570\u636e\u5904\u7406\u53ca\u589e\u5f3a","text":"<p>\u9996\u5148\u6211\u4eec\u901a\u8fc7\u8c03\u7528<code>create_transforms</code>\u51fd\u6570, \u83b7\u5f97\u9884\u8bbe\u7684\u6570\u636e\u5904\u7406\u548c\u589e\u5f3a\u7b56\u7565(transform list)\uff0c\u6b64\u4efb\u52a1\u4e2d\uff0c\u56e0\u72fc\u72d7\u56fe\u50cf\u548cImageNet\u6570\u636e\u4e00\u81f4\uff08\u5373domain\u4e00\u81f4\uff09\uff0c\u6211\u4eec\u6307\u5b9a\u53c2\u6570<code>dataset_name</code>\u4e3aImageNet\uff0c\u76f4\u63a5\u7528\u9884\u8bbe\u597d\u7684ImageNet\u7684\u6570\u636e\u5904\u7406\u548c\u56fe\u50cf\u589e\u5f3a\u7b56\u7565\u3002<code>create_transforms</code> \u540c\u6837\u652f\u6301\u591a\u79cd\u81ea\u5b9a\u4e49\u7684\u5904\u7406\u548c\u589e\u5f3a\u64cd\u4f5c\uff0c\u4ee5\u53ca\u81ea\u52a8\u589e\u5f3a\u7b56\u7565(AutoAug)\u3002\u8be6\u89c1API\u8bf4\u660e\u3002</p> <p>\u6211\u4eec\u5c06\u5f97\u5230\u7684transform list\u4f20\u5165<code>create_loader()</code>\uff0c\u5e76\u6307\u5b9a<code>batch_size</code>\u548c\u5176\u4ed6\u53c2\u6570\uff0c\u5373\u53ef\u5b8c\u6210\u8bad\u7ec3\u548c\u9a8c\u8bc1\u6570\u636e\u7684\u51c6\u5907\uff0c\u8fd4\u56de<code>Dataset</code> Object\uff0c\u4f5c\u4e3a\u6a21\u578b\u7684\u8f93\u5165\u3002</p> <pre><code># \u5b9a\u4e49\u548c\u83b7\u53d6\u6570\u636e\u5904\u7406\u53ca\u589e\u5f3a\u64cd\u4f5c\ntrans_train = create_transforms(dataset_name='ImageNet', is_training=True)\ntrans_val = create_transforms(dataset_name='ImageNet',is_training=False)\n\nloader_train = create_loader(\n    dataset=dataset_train,\n    batch_size=16,\n    is_training=True,\n    num_classes=2,\n    transform=trans_train,\n    num_parallel_workers=num_workers,\n)\nloader_val = create_loader(\n    dataset=dataset_val,\n    batch_size=5,\n    is_training=True,\n    num_classes=2,\n    transform=trans_val,\n    num_parallel_workers=num_workers,\n)\n</code></pre>"},{"location":"zh/tutorials/finetune/#_7","title":"\u6570\u636e\u96c6\u53ef\u89c6\u5316","text":"<p>\u5bf9\u4e8e<code>create_loader</code>\u63a5\u53e3\u8fd4\u56de\u7684\u5b8c\u6210\u6570\u636e\u52a0\u8f7d\u7684Dataset object\uff0c\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7 <code>create_tuple_iterator</code> \u63a5\u53e3\u521b\u5efa\u6570\u636e\u8fed\u4ee3\u5668\uff0c\u4f7f\u7528 <code>next</code> \u8fed\u4ee3\u8bbf\u95ee\u6570\u636e\u96c6\uff0c\u8bfb\u53d6\u5230\u4e00\u4e2abatch\u7684\u6570\u636e\u3002</p> <pre><code>images, labels = next(loader_train.create_tuple_iterator())\nprint(\"Tensor of image\", images.shape)\nprint(\"Labels:\", labels)\n</code></pre> <pre><code>Tensor of image (16, 3, 224, 224)\nLabels: [0 1 1 1 1 0 0 0 0 0 1 0 1 0 1 1]\n</code></pre> <p>\u5bf9\u83b7\u53d6\u5230\u7684\u56fe\u50cf\u53ca\u6807\u7b7e\u6570\u636e\u8fdb\u884c\u53ef\u89c6\u5316\uff0c\u6807\u9898\u4e3a\u56fe\u50cf\u5bf9\u5e94\u7684label\u540d\u79f0\u3002</p> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\n# class_name\u5bf9\u5e94label\uff0c\u6309\u6587\u4ef6\u5939\u5b57\u7b26\u4e32\u4ece\u5c0f\u5230\u5927\u7684\u987a\u5e8f\u6807\u8bb0label\nclass_name = {0: \"dogs\", 1: \"wolves\"}\n\nplt.figure(figsize=(15, 7))\nfor i in range(len(labels)):\n    # \u83b7\u53d6\u56fe\u50cf\u53ca\u5176\u5bf9\u5e94\u7684label\n    data_image = images[i].asnumpy()\n    data_label = labels[i]\n    # \u5904\u7406\u56fe\u50cf\u4f9b\u5c55\u793a\u4f7f\u7528\n    data_image = np.transpose(data_image, (1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    data_image = std * data_image + mean\n    data_image = np.clip(data_image, 0, 1)\n    # \u663e\u793a\u56fe\u50cf\n    plt.subplot(3, 6, i + 1)\n    plt.imshow(data_image)\n    plt.title(class_name[int(labels[i].asnumpy())])\n    plt.axis(\"off\")\n\nplt.show()\n</code></pre> <p></p>"},{"location":"zh/tutorials/finetune/#_8","title":"\u6a21\u578b\u5fae\u8c03","text":""},{"location":"zh/tutorials/finetune/#1","title":"1. \u6574\u4f53\u6a21\u578b\u5fae\u8c03","text":""},{"location":"zh/tutorials/finetune/#_9","title":"\u9884\u8bad\u7ec3\u6a21\u578b\u52a0\u8f7d","text":"<p>\u6211\u4eec\u4f7f\u7528<code>mindcv.models.densenet</code>\u4e2d\u5b9a\u4e49DenseNet121\u7f51\u7edc\uff0c\u5f53\u63a5\u53e3\u4e2d\u7684<code>pretrained</code>\u53c2\u6570\u8bbe\u7f6e\u4e3aTrue\u65f6\uff0c\u53ef\u4ee5\u81ea\u52a8\u4e0b\u8f7d\u7f51\u7edc\u6743\u91cd\u3002 \u7531\u4e8e\u8be5\u9884\u8bad\u7ec3\u6a21\u578b\u662f\u9488\u5bf9ImageNet\u6570\u636e\u96c6\u4e2d\u76841000\u4e2a\u7c7b\u522b\u8fdb\u884c\u5206\u7c7b\u7684\uff0c\u8fd9\u91cc\u6211\u4eec\u8bbe\u5b9a<code>num_classes=2</code>, DenseNet\u7684classifier(\u5373\u6700\u540e\u7684FC\u5c42)\u8f93\u51fa\u8c03\u6574\u4e3a\u4e24\u7ef4\uff0c\u6b64\u65f6\u53ea\u52a0\u8f7dbackbone\u7684\u9884\u8bad\u7ec3\u6743\u91cd\uff0c\u800cclassifier\u5219\u4f7f\u7528\u521d\u59cb\u503c\u3002</p> <pre><code>from mindcv.models import create_model\n\nnetwork = create_model(model_name='densenet121', num_classes=2, pretrained=True)\n</code></pre> <p>DenseNet\u7684\u5177\u4f53\u7ed3\u6784\u53ef\u53c2\u89c1DenseNet\u8bba\u6587\u3002</p>"},{"location":"zh/tutorials/finetune/#_10","title":"\u6a21\u578b\u8bad\u7ec3","text":"<p>\u4f7f\u7528\u5df2\u52a0\u8f7d\u5904\u7406\u597d\u7684\u5e26\u6807\u7b7e\u7684\u72fc\u548c\u72d7\u56fe\u50cf\uff0c\u5bf9DenseNet\u8fdb\u884c\u5fae\u8c03\u7f51\u7edc\u3002\u6ce8\u610f\uff0c\u5bf9\u6574\u4f53\u6a21\u578b\u505a\u5fae\u8c03\u65f6\uff0c\u5e94\u4f7f\u7528\u8f83\u5c0f\u7684learning rate\u3002</p> <pre><code>from mindcv.loss import create_loss\nfrom mindcv.optim import create_optimizer\nfrom mindcv.scheduler import create_scheduler\nfrom mindspore import Model, LossMonitor, TimeMonitor\n\n# \u5b9a\u4e49\u4f18\u5316\u5668\u548c\u635f\u5931\u51fd\u6570\nopt = create_optimizer(network.trainable_params(), opt='adam', lr=1e-4)\nloss = create_loss(name='CE')\n\n# \u5b9e\u4f8b\u5316\u6a21\u578b\nmodel = Model(network, loss_fn=loss, optimizer=opt, metrics={'accuracy'})\nmodel.train(10, loader_train, callbacks=[LossMonitor(5), TimeMonitor(5)], dataset_sink_mode=False)\n</code></pre> <pre><code>epoch: 1 step: 5, loss is 0.5195528864860535\nepoch: 1 step: 10, loss is 0.2654373049736023\nepoch: 1 step: 15, loss is 0.28758567571640015\nTrain epoch time: 17270.144 ms, per step time: 1151.343 ms\nepoch: 2 step: 5, loss is 0.1807008981704712\nepoch: 2 step: 10, loss is 0.1700802594423294\nepoch: 2 step: 15, loss is 0.09752683341503143\nTrain epoch time: 1372.549 ms, per step time: 91.503 ms\nepoch: 3 step: 5, loss is 0.13594701886177063\nepoch: 3 step: 10, loss is 0.03628234937787056\nepoch: 3 step: 15, loss is 0.039737217128276825\nTrain epoch time: 1453.237 ms, per step time: 96.882 ms\nepoch: 4 step: 5, loss is 0.014213413000106812\nepoch: 4 step: 10, loss is 0.030747078359127045\nepoch: 4 step: 15, loss is 0.0798817127943039\nTrain epoch time: 1331.237 ms, per step time: 88.749 ms\nepoch: 5 step: 5, loss is 0.009510636329650879\nepoch: 5 step: 10, loss is 0.02603740245103836\nepoch: 5 step: 15, loss is 0.051846928894519806\nTrain epoch time: 1312.737 ms, per step time: 87.516 ms\nepoch: 6 step: 5, loss is 0.1163717582821846\nepoch: 6 step: 10, loss is 0.02439398318529129\nepoch: 6 step: 15, loss is 0.02564268559217453\nTrain epoch time: 1434.704 ms, per step time: 95.647 ms\nepoch: 7 step: 5, loss is 0.013310655951499939\nepoch: 7 step: 10, loss is 0.02289542555809021\nepoch: 7 step: 15, loss is 0.1992517113685608\nTrain epoch time: 1275.935 ms, per step time: 85.062 ms\nepoch: 8 step: 5, loss is 0.015928998589515686\nepoch: 8 step: 10, loss is 0.011409260332584381\nepoch: 8 step: 15, loss is 0.008141174912452698\nTrain epoch time: 1323.102 ms, per step time: 88.207 ms\nepoch: 9 step: 5, loss is 0.10395607352256775\nepoch: 9 step: 10, loss is 0.23055407404899597\nepoch: 9 step: 15, loss is 0.04896317049860954\nTrain epoch time: 1261.067 ms, per step time: 84.071 ms\nepoch: 10 step: 5, loss is 0.03162381425499916\nepoch: 10 step: 10, loss is 0.13094250857830048\nepoch: 10 step: 15, loss is 0.020028553903102875\nTrain epoch time: 1217.958 ms, per step time: 81.197 ms\n</code></pre>"},{"location":"zh/tutorials/finetune/#_11","title":"\u6a21\u578b\u8bc4\u4f30","text":"<p>\u5728\u8bad\u7ec3\u5b8c\u6210\u540e\uff0c\u6211\u4eec\u5728\u9a8c\u8bc1\u96c6\u4e0a\u8bc4\u4f30\u6a21\u578b\u7684\u7cbe\u5ea6\u3002</p> <pre><code>res = model.eval(loader_val)\nprint(res)\n</code></pre> <pre><code>{'accuracy': 1.0}\n</code></pre>"},{"location":"zh/tutorials/finetune/#_12","title":"\u53ef\u89c6\u5316\u6a21\u578b\u63a8\u7406\u7ed3\u679c","text":"<p>\u5b9a\u4e49 <code>visualize_mode</code> \u51fd\u6570\uff0c\u53ef\u89c6\u5316\u6a21\u578b\u9884\u6d4b\u3002</p> <pre><code>import matplotlib.pyplot as plt\nimport mindspore as ms\n\ndef visualize_model(model, val_dl, num_classes=2):\n    # \u52a0\u8f7d\u9a8c\u8bc1\u96c6\u7684\u6570\u636e\u8fdb\u884c\u9a8c\u8bc1\n    images, labels= next(val_dl.create_tuple_iterator())\n    # \u9884\u6d4b\u56fe\u50cf\u7c7b\u522b\n    output = model.predict(images)\n    pred = np.argmax(output.asnumpy(), axis=1)\n    # \u663e\u793a\u56fe\u50cf\u53ca\u56fe\u50cf\u7684\u9884\u6d4b\u503c\n    images = images.asnumpy()\n    labels = labels.asnumpy()\n    class_name = {0: \"dogs\", 1: \"wolves\"}\n    plt.figure(figsize=(15, 7))\n    for i in range(len(labels)):\n        plt.subplot(3, 6, i + 1)\n        # \u82e5\u9884\u6d4b\u6b63\u786e\uff0c\u663e\u793a\u4e3a\u84dd\u8272\uff1b\u82e5\u9884\u6d4b\u9519\u8bef\uff0c\u663e\u793a\u4e3a\u7ea2\u8272\n        color = 'blue' if pred[i] == labels[i] else 'red'\n        plt.title('predict:{}'.format(class_name[pred[i]]), color=color)\n        picture_show = np.transpose(images[i], (1, 2, 0))\n        mean = np.array([0.485, 0.456, 0.406])\n        std = np.array([0.229, 0.224, 0.225])\n        picture_show = std * picture_show + mean\n        picture_show = np.clip(picture_show, 0, 1)\n        plt.imshow(picture_show)\n        plt.axis('off')\n\n    plt.show()\n</code></pre> <p>\u4f7f\u7528\u5fae\u8c03\u8fc7\u540e\u7684\u6a21\u578b\u5bf9\u9a8c\u8bc1\u96c6\u7684\u72fc\u548c\u72d7\u56fe\u50cf\u6570\u636e\u8fdb\u884c\u9884\u6d4b\u3002\u82e5\u9884\u6d4b\u5b57\u4f53\u4e3a\u84dd\u8272\u8868\u793a\u9884\u6d4b\u6b63\u786e\uff0c\u82e5\u9884\u6d4b\u5b57\u4f53\u4e3a\u7ea2\u8272\u8868\u793a\u9884\u6d4b\u9519\u8bef\u3002</p> <pre><code>visualize_model(model, loader_val)\n</code></pre> <p></p>"},{"location":"zh/tutorials/finetune/#2","title":"2. \u51bb\u7ed3\u7279\u5f81\u7f51\u7edc, \u5fae\u8c03\u5206\u7c7b\u5668","text":""},{"location":"zh/tutorials/finetune/#_13","title":"\u51bb\u7ed3\u7279\u5f81\u7f51\u7edc\u7684\u53c2\u6570","text":"<p>\u9996\u5148\uff0c\u6211\u4eec\u8981\u51bb\u7ed3\u9664\u6700\u540e\u4e00\u5c42\u5206\u7c7b\u5668\u4e4b\u5916\u7684\u6240\u6709\u7f51\u7edc\u5c42\uff0c\u5373\u5c06\u76f8\u5e94\u7684\u5c42\u53c2\u6570\u7684<code>requires_grad</code>\u5c5e\u6027\u8bbe\u7f6e\u4e3a<code>False</code>\uff0c\u4f7f\u5176\u4e0d\u5728\u53cd\u5411\u4f20\u64ad\u4e2d\u8ba1\u7b97\u68af\u5ea6\u53ca\u66f4\u65b0\u53c2\u6570\u3002</p> <p>\u56e0\u4e3a<code>mindcv.models</code> \u4e2d\u6240\u6709\u7684\u6a21\u578b\u5747\u4ee5<code>classifier</code> \u6765\u6807\u8bc6\u548c\u547d\u540d\u6a21\u578b\u7684\u5206\u7c7b\u5668(\u5373Dense\u5c42)\uff0c\u6240\u4ee5\u901a\u8fc7 <code>classifier.weight</code> \u548c <code>classifier.bias</code> \u5373\u53ef\u7b5b\u9009\u51fa\u5206\u7c7b\u5668\u5916\u7684\u5404\u5c42\u53c2\u6570\uff0c\u5c06\u5176<code>requires_grad</code>\u5c5e\u6027\u8bbe\u7f6e\u4e3a<code>False</code>.</p> <pre><code># freeze backbone\nfor param in network.get_parameters():\n    if param.name not in [\"classifier.weight\", \"classifier.bias\"]:\n        param.requires_grad = False\n</code></pre>"},{"location":"zh/tutorials/finetune/#_14","title":"\u5fae\u8c03\u5206\u7c7b\u5668","text":"<p>\u56e0\u4e3a\u7279\u5f81\u7f51\u7edc\u5df2\u7ecf\u56fa\u5b9a\uff0c\u6211\u4eec\u4e0d\u5fc5\u62c5\u5fc3\u8bad\u7ec3\u8fc7\u7a0b\u4f1adistort pratrained features\uff0c\u56e0\u6b64\uff0c\u76f8\u6bd4\u4e8e\u7b2c\u4e00\u79cd\u65b9\u6cd5\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06learning rate\u8c03\u5927\u4e00\u4e9b\u3002</p> <p>\u4e0e\u6ca1\u6709\u9884\u8bad\u7ec3\u6a21\u578b\u76f8\u6bd4\uff0c\u5c06\u8282\u7ea6\u4e00\u5927\u534a\u65f6\u95f4\uff0c\u56e0\u4e3a\u6b64\u65f6\u53ef\u4ee5\u4e0d\u7528\u8ba1\u7b97\u90e8\u5206\u68af\u5ea6\u3002</p> <pre><code># \u52a0\u8f7d\u6570\u636e\u96c6\ndataset_train = create_dataset(root=data_dir, split='train', num_parallel_workers=num_workers)\nloader_train = create_loader(\n    dataset=dataset_train,\n    batch_size=16,\n    is_training=True,\n    num_classes=2,\n    transform=trans_train,\n    num_parallel_workers=num_workers,\n)\n\n# \u5b9a\u4e49\u4f18\u5316\u5668\u548c\u635f\u5931\u51fd\u6570\nopt = create_optimizer(network.trainable_params(), opt='adam', lr=1e-3)\nloss = create_loss(name='CE')\n\n# \u5b9e\u4f8b\u5316\u6a21\u578b\nmodel = Model(network, loss_fn=loss, optimizer=opt, metrics={'accuracy'})\n\nmodel.train(10, loader_train, callbacks=[LossMonitor(5), TimeMonitor(5)], dataset_sink_mode=False)\n</code></pre> <pre><code>epoch: 1 step: 5, loss is 0.051333948969841\nepoch: 1 step: 10, loss is 0.02043312042951584\nepoch: 1 step: 15, loss is 0.16161368787288666\nTrain epoch time: 10228.601 ms, per step time: 681.907 ms\nepoch: 2 step: 5, loss is 0.002121545374393463\nepoch: 2 step: 10, loss is 0.0009798109531402588\nepoch: 2 step: 15, loss is 0.015776708722114563\nTrain epoch time: 562.543 ms, per step time: 37.503 ms\nepoch: 3 step: 5, loss is 0.008056879043579102\nepoch: 3 step: 10, loss is 0.0009347647428512573\nepoch: 3 step: 15, loss is 0.028648357838392258\nTrain epoch time: 523.249 ms, per step time: 34.883 ms\nepoch: 4 step: 5, loss is 0.001014217734336853\nepoch: 4 step: 10, loss is 0.0003159046173095703\nepoch: 4 step: 15, loss is 0.0007699579000473022\nTrain epoch time: 508.886 ms, per step time: 33.926 ms\nepoch: 5 step: 5, loss is 0.0015687644481658936\nepoch: 5 step: 10, loss is 0.012090332806110382\nepoch: 5 step: 15, loss is 0.004598274827003479\nTrain epoch time: 507.243 ms, per step time: 33.816 ms\nepoch: 6 step: 5, loss is 0.010022152215242386\nepoch: 6 step: 10, loss is 0.0066385045647621155\nepoch: 6 step: 15, loss is 0.0036080628633499146\nTrain epoch time: 517.646 ms, per step time: 34.510 ms\nepoch: 7 step: 5, loss is 0.01344013586640358\nepoch: 7 step: 10, loss is 0.0008538365364074707\nepoch: 7 step: 15, loss is 0.14135593175888062\nTrain epoch time: 511.513 ms, per step time: 34.101 ms\nepoch: 8 step: 5, loss is 0.01626245677471161\nepoch: 8 step: 10, loss is 0.02871556021273136\nepoch: 8 step: 15, loss is 0.010110966861248016\nTrain epoch time: 545.678 ms, per step time: 36.379 ms\nepoch: 9 step: 5, loss is 0.008498094975948334\nepoch: 9 step: 10, loss is 0.2588501274585724\nepoch: 9 step: 15, loss is 0.0014278888702392578\nTrain epoch time: 499.243 ms, per step time: 33.283 ms\nepoch: 10 step: 5, loss is 0.021337147802114487\nepoch: 10 step: 10, loss is 0.00829876959323883\nepoch: 10 step: 15, loss is 0.008352771401405334\nTrain epoch time: 465.600 ms, per step time: 31.040 ms\n</code></pre>"},{"location":"zh/tutorials/finetune/#_15","title":"\u6a21\u578b\u8bc4\u4f30","text":"<p>\u8bad\u7ec3\u5b8c\u6210\u4e4b\u540e\uff0c\u6211\u4eec\u5728\u9a8c\u8bc1\u96c6\u4e0a\u8bc4\u4f30\u6a21\u578b\u7684\u51c6\u786e\u7387\u3002</p> <pre><code>dataset_val = create_dataset(root=data_dir, split='val', num_parallel_workers=num_workers)\nloader_val = create_loader(\n    dataset=dataset_val,\n    batch_size=5,\n    is_training=True,\n    num_classes=2,\n    transform=trans_val,\n    num_parallel_workers=num_workers,\n)\n\nres = model.eval(loader_val)\nprint(res)\n</code></pre> <pre><code>{'accuracy': 1.0}\n</code></pre>"},{"location":"zh/tutorials/finetune/#_16","title":"\u53ef\u89c6\u5316\u6a21\u578b\u9884\u6d4b","text":"<p>\u4f7f\u7528\u5fae\u8c03\u8fc7\u540e\u7684\u6a21\u578b\u4ef6\u5bf9\u9a8c\u8bc1\u96c6\u7684\u72fc\u548c\u72d7\u56fe\u50cf\u6570\u636e\u8fdb\u884c\u9884\u6d4b\u3002\u82e5\u9884\u6d4b\u5b57\u4f53\u4e3a\u84dd\u8272\u8868\u793a\u9884\u6d4b\u6b63\u786e\uff0c\u82e5\u9884\u6d4b\u5b57\u4f53\u4e3a\u7ea2\u8272\u8868\u793a\u9884\u6d4b\u9519\u8bef\u3002</p> <pre><code>visualize_model(model, loader_val)\n</code></pre> <p></p> <p>\u5fae\u8c03\u540e\u7684\u72fc\u72d7\u9884\u6d4b\u7ed3\u679c\u5747\u6b63\u786e</p>"},{"location":"zh/tutorials/inference/","title":"\u56fe\u50cf\u5206\u7c7b\u9884\u6d4b","text":"<p>\u672c\u6559\u7a0b\u4ecb\u7ecd\u5982\u4f55\u5728MindCV\u4e2d\u8c03\u7528\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5728\u6d4b\u8bd5\u56fe\u50cf\u4e0a\u8fdb\u884c\u5206\u7c7b\u9884\u6d4b\u3002</p>"},{"location":"zh/tutorials/inference/#_2","title":"\u6a21\u578b\u52a0\u8f7d","text":""},{"location":"zh/tutorials/inference/#_3","title":"\u67e5\u770b\u5168\u90e8\u53ef\u7528\u7684\u7f51\u7edc\u6a21\u578b","text":"<p>\u901a\u8fc7\u8c03\u7528<code>mindcv.models</code>\u4e2d\u7684<code>registry.list_models</code>\u51fd\u6570\uff0c\u53ef\u4ee5\u6253\u5370\u51fa\u5168\u90e8\u7f51\u7edc\u6a21\u578b\u7684\u540d\u5b57\uff0c\u4e00\u4e2a\u7f51\u7edc\u5728\u4e0d\u540c\u53c2\u6570\u914d\u7f6e\u4e0b\u7684\u6a21\u578b\u4e5f\u4f1a\u5206\u522b\u6253\u5370\u51fa\u6765\uff0c\u4f8b\u5982resnet18 / resnet34 / resnet50 / resnet101 / resnet152\u3002</p> <pre><code>import sys\nsys.path.append(\"..\")\nfrom mindcv.models import registry\nregistry.list_models()\n</code></pre> <pre><code>['BiT_resnet50',\n 'repmlp_b224',\n 'repmlp_b256',\n 'repmlp_d256',\n 'repmlp_l256',\n 'repmlp_t224',\n 'repmlp_t256',\n 'convit_base',\n 'convit_base_plus',\n 'convit_small',\n ...\n 'visformer_small',\n 'visformer_small_v2',\n 'visformer_tiny',\n 'visformer_tiny_v2',\n 'vit_b_16_224',\n 'vit_b_16_384',\n 'vit_b_32_224',\n 'vit_b_32_384',\n 'vit_l_16_224',\n 'vit_l_16_384',\n 'vit_l_32_224',\n 'xception']\n</code></pre>"},{"location":"zh/tutorials/inference/#_4","title":"\u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b","text":"<p>\u6211\u4eec\u4ee5resnet50\u6a21\u578b\u4e3a\u4f8b\uff0c\u4ecb\u7ecd\u4e24\u79cd\u4f7f\u7528<code>mindcv.models</code>\u4e2d<code>create_model</code>\u51fd\u6570\u8fdb\u884c\u6a21\u578bcheckpoint\u52a0\u8f7d\u7684\u65b9\u6cd5\u3002</p> <p>1). \u5f53\u63a5\u53e3\u4e2d\u7684<code>pretrained</code>\u53c2\u6570\u8bbe\u7f6e\u4e3aTrue\u65f6\uff0c\u53ef\u4ee5\u81ea\u52a8\u4e0b\u8f7d\u7f51\u7edc\u6743\u91cd\u3002</p> <pre><code>from mindcv.models import create_model\nmodel = create_model(model_name='resnet50', num_classes=1000, pretrained=True)\n# \u5207\u6362\u7f51\u7edc\u7684\u6267\u884c\u903b\u8f91\u4e3a\u63a8\u7406\u573a\u666f\nmodel.set_train(False)\n</code></pre> <pre><code>102453248B [00:16, 6092186.31B/s]\n\nResNet&lt;\n  (conv1): Conv2d&lt;input_channels=3, output_channels=64, kernel_size=(7, 7), stride=(2, 2), pad_mode=pad, padding=3, dilation=(1, 1), group=1, has_bias=False, weight_init=normal, bias_init=zeros, format=NCHW&gt;\n  (bn1): BatchNorm2d&lt;num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)&gt;\n  (relu): ReLU&lt;&gt;\n  (max_pool): MaxPool2d&lt;kernel_size=3, stride=2, pad_mode=SAME&gt;\n  ...\n  (pool): GlobalAvgPooling&lt;&gt;\n  (classifier): Dense&lt;input_channels=2048, output_channels=1000, has_bias=True&gt;\n  &gt;\n</code></pre> <p>2). \u5f53\u63a5\u53e3\u4e2d\u7684<code>checkpoint_path</code>\u53c2\u6570\u8bbe\u7f6e\u4e3a\u6587\u4ef6\u8def\u5f84\u65f6\uff0c\u53ef\u4ee5\u4ece\u672c\u5730\u52a0\u8f7d\u540e\u7f00\u4e3a<code>.ckpt</code>\u7684\u6a21\u578b\u53c2\u6570\u6587\u4ef6\u3002</p> <pre><code>from mindcv.models import create_model\nmodel = create_model(model_name='resnet50', num_classes=1000, checkpoint_path='./resnet50_224.ckpt')\n# \u5207\u6362\u7f51\u7edc\u7684\u6267\u884c\u903b\u8f91\u4e3a\u63a8\u7406\u573a\u666f\nmodel.set_train(False)\n</code></pre>"},{"location":"zh/tutorials/inference/#_5","title":"\u6570\u636e\u51c6\u5907","text":""},{"location":"zh/tutorials/inference/#_6","title":"\u6784\u9020\u6570\u636e\u96c6","text":"<p>\u8fd9\u91cc\uff0c\u6211\u4eec\u4e0b\u8f7d\u4e00\u5f20Wikipedia\u7684\u56fe\u7247\u4f5c\u4e3a\u6d4b\u8bd5\u56fe\u7247\uff0c\u4f7f\u7528<code>mindcv.data</code>\u4e2d\u7684<code>create_dataset</code>\u51fd\u6570\uff0c\u4e3a\u5355\u5f20\u56fe\u7247\u6784\u9020\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u3002</p> <pre><code>from mindcv.data import create_dataset\nnum_workers = 1\n# \u6570\u636e\u96c6\u76ee\u5f55\u8def\u5f84\ndata_dir = \"./data/\"\ndataset = create_dataset(root=data_dir, split='test', num_parallel_workers=num_workers)\n# \u56fe\u50cf\u53ef\u89c6\nfrom PIL import Image\nImage.open(\"./data/test/dog/dog.jpg\")\n</code></pre> <p></p>"},{"location":"zh/tutorials/inference/#_7","title":"\u6570\u636e\u9884\u5904\u7406","text":"<p>\u901a\u8fc7\u8c03\u7528<code>create_transforms</code>\u51fd\u6570\uff0c\u83b7\u5f97\u9884\u8bad\u7ec3\u6a21\u578b\u4f7f\u7528\u7684ImageNet\u6570\u636e\u96c6\u7684\u6570\u636e\u5904\u7406\u7b56\u7565(transform list)\u3002</p> <p>\u6211\u4eec\u5c06\u5f97\u5230\u7684transform list\u4f20\u5165<code>create_loader</code>\u51fd\u6570\uff0c\u6307\u5b9a<code>batch_size=1</code>\u548c\u5176\u4ed6\u53c2\u6570\uff0c\u5373\u53ef\u5b8c\u6210\u6d4b\u8bd5\u6570\u636e\u7684\u51c6\u5907\uff0c\u8fd4\u56de<code>Dataset</code> Object\uff0c\u4f5c\u4e3a\u6a21\u578b\u7684\u8f93\u5165\u3002</p> <pre><code>from mindcv.data import create_transforms, create_loader\ntransforms_list = create_transforms(dataset_name='imagenet', is_training=False)\ndata_loader = create_loader(\n    dataset=dataset,\n    batch_size=1,\n    is_training=False,\n    num_classes=1000,\n    transform=transforms_list,\n    num_parallel_workers=num_workers\n)\n</code></pre>"},{"location":"zh/tutorials/inference/#_8","title":"\u6a21\u578b\u63a8\u7406","text":"<p>\u5c06\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u7684\u56fe\u7247\u4f20\u5165\u6a21\u578b\uff0c\u83b7\u5f97\u63a8\u7406\u7684\u7ed3\u679c\u3002\u8fd9\u91cc\u4f7f\u7528<code>mindspore.ops</code>\u7684<code>Squeeze</code>\u51fd\u6570\u53bb\u9664batch\u7ef4\u5ea6\u3002</p> <pre><code>import mindspore.ops as P\nimport numpy as np\nimages, _ = next(data_loader.create_tuple_iterator())\noutput = P.Squeeze()(model(images))\npred = np.argmax(output.asnumpy())\n</code></pre> <pre><code>with open(\"imagenet1000_clsidx_to_labels.txt\") as f:\n    idx2label = eval(f.read())\nprint('predict: {}'.format(idx2label[pred]))\n</code></pre> <pre><code>predict: Labrador retriever\n</code></pre>"},{"location":"zh/tutorials/quick_start/","title":"\u5feb\u901f\u5165\u95e8","text":"<p>MindCV\u662f\u4e00\u4e2a\u57fa\u4e8eMindSpore\u5f00\u53d1\u7684\uff0c\u81f4\u529b\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u76f8\u5173\u6280\u672f\u7814\u53d1\u7684\u5f00\u6e90\u5de5\u5177\u7bb1\u3002 \u5b83\u63d0\u4f9b\u5927\u91cf\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u7684\u7ecf\u5178\u6a21\u578b\u548cSoTA\u6a21\u578b\u4ee5\u53ca\u5b83\u4eec\u7684\u9884\u8bad\u7ec3\u6743\u91cd\u3002\u540c\u65f6\uff0c\u8fd8\u63d0\u4f9b\u4e86AutoAugment\u7b49SoTA\u7b97\u6cd5\u6765\u63d0\u9ad8\u6027\u80fd\u3002 \u901a\u8fc7\u89e3\u8026\u7684\u6a21\u5757\u8bbe\u8ba1\uff0c\u60a8\u53ef\u4ee5\u8f7b\u677e\u5730\u5c06MindCV\u5e94\u7528\u5230\u60a8\u81ea\u5df1\u7684CV\u4efb\u52a1\u4e2d\u3002\u672c\u6559\u7a0b\u4e2d\u6211\u4eec\u5c06\u63d0\u4f9b\u4e00\u4e2a\u5feb\u901f\u4e0a\u624bMindCV\u7684\u6307\u5357\u3002</p> <p>\u672c\u6559\u7a0b\u5c06\u4ee5DenseNet\u5206\u7c7b\u6a21\u578b\u4e3a\u4f8b\uff0c\u5b9e\u73b0\u5bf9CIFAR-10\u6570\u636e\u96c6\u7684\u8fc1\u79fb\u5b66\u4e60\uff0c\u5e76\u5728\u6b64\u6d41\u7a0b\u4e2d\u5bf9MindCV\u5404\u6a21\u5757\u7684\u7528\u6cd5\u4f5c\u8bb2\u89e3\u3002</p>"},{"location":"zh/tutorials/quick_start/#_2","title":"\u73af\u5883\u51c6\u5907","text":"<p>\u8be6\u89c1\u5b89\u88c5\u3002</p>"},{"location":"zh/tutorials/quick_start/#_3","title":"\u6570\u636e","text":""},{"location":"zh/tutorials/quick_start/#_4","title":"\u6570\u636e\u96c6","text":"<p>\u901a\u8fc7mindcv.data\u4e2d\u7684create_dataset\u6a21\u5757\uff0c\u6211\u4eec\u53ef\u4ee5\u5feb\u901f\u5730\u8bfb\u53d6\u6807\u51c6\u6570\u636e\u96c6\u6216\u81ea\u5b9a\u4e49\u7684\u6570\u636e\u96c6\u3002</p> <pre><code>import os\nfrom mindcv.data import create_dataset, create_transforms, create_loader\n\ncifar10_dir = './datasets/cifar/cifar-10-batches-bin'  # \u4f60\u7684\u6570\u636e\u5b58\u653e\u8def\u5f84\nnum_classes = 10  # \u7c7b\u522b\u6570\nnum_workers = 8  # \u6570\u636e\u8bfb\u53d6\u53ca\u52a0\u8f7d\u7684\u5de5\u4f5c\u7ebf\u7a0b\u6570\n\n# \u521b\u5efa\u6570\u636e\u96c6\ndataset_train = create_dataset(\n    name='cifar10', root=cifar10_dir, split='train', shuffle=True, num_parallel_workers=num_workers\n)\n</code></pre>"},{"location":"zh/tutorials/quick_start/#_5","title":"\u6570\u636e\u53d8\u6362","text":"<p>create_transforms\u51fd\u6570\u53ef\u76f4\u63a5\u751f\u6210\u9002\u914d\u6807\u51c6\u6570\u636e\u96c6\u7684\u6570\u636e\u5904\u7406\u589e\u5f3a\u7b56\u7565(transform list)\uff0c\u5305\u62ecCifar10, ImageNet\u4e0a\u5e38\u7528\u7684\u6570\u636e\u5904\u7406\u7b56\u7565\u3002</p> <pre><code># \u521b\u5efa\u6240\u9700\u7684\u6570\u636e\u589e\u5f3a\u64cd\u4f5c\u7684\u5217\u8868\ntrans = create_transforms(dataset_name='cifar10', image_resize=224)\n</code></pre>"},{"location":"zh/tutorials/quick_start/#_6","title":"\u6570\u636e\u52a0\u8f7d","text":"<p>\u901a\u8fc7mindcv.data.create_loader\u51fd\u6570\uff0c\u8fdb\u884c\u6570\u636e\u8f6c\u6362\u548cbatch\u5207\u5206\u52a0\u8f7d\uff0c\u6211\u4eec\u9700\u8981\u5c06create_transforms\u8fd4\u56de\u7684transform_list\u4f20\u5165\u3002</p> <pre><code># \u6267\u884c\u6570\u636e\u589e\u5f3a\u64cd\u4f5c\uff0c\u751f\u6210\u6240\u9700\u6570\u636e\u96c6\u3002\nloader_train = create_loader(dataset=dataset_train,\n                             batch_size=64,\n                             is_training=True,\n                             num_classes=num_classes,\n                             transform=trans,\n                             num_parallel_workers=num_workers)\n\nnum_batches = loader_train.get_dataset_size()\n</code></pre> <p>\u5728notebook\u4e2d\u907f\u514d\u91cd\u590d\u6267\u884ccreate_loader\u5355\u4e2aCell\uff0c\u6216\u5728\u6267\u884ccreate_dataset\u4e4b\u540e\u518d\u6b21\u6267\u884c\u3002</p>"},{"location":"zh/tutorials/quick_start/#_7","title":"\u6a21\u578b\u521b\u5efa\u548c\u52a0\u8f7d","text":"<p>\u4f7f\u7528create_model\u63a5\u53e3\u83b7\u5f97\u5b9e\u4f8b\u5316\u7684DenseNet\uff0c\u5e76\u52a0\u8f7d\u9884\u8bad\u7ec3\u6743\u91cddensenet_121_224.ckpt\uff08ImageNet\u6570\u636e\u96c6\u8bad\u7ec3\u5f97\u5230\uff09\u3002</p> <pre><code>from mindcv.models import create_model\n\n# \u5b9e\u4f8b\u5316 DenseNet-121 \u6a21\u578b\u5e76\u52a0\u8f7d\u9884\u8bad\u7ec3\u6743\u91cd\u3002\nnetwork = create_model(model_name='densenet121', num_classes=num_classes, pretrained=True)\n</code></pre> <p>\u7531\u4e8eCIFAR-10\u548cImageNet\u6570\u636e\u96c6\u6240\u9700\u7c7b\u522b\u6570\u91cf\u4e0d\u540c\uff0c\u5206\u7c7b\u5668\u53c2\u6570\u65e0\u6cd5\u5171\u4eab\uff0c\u51fa\u73b0\u5206\u7c7b\u5668\u53c2\u6570\u65e0\u6cd5\u52a0\u8f7d\u7684\u544a\u8b66\u4e0d\u5f71\u54cd\u5fae\u8c03\u3002</p>"},{"location":"zh/tutorials/quick_start/#_8","title":"\u635f\u5931\u51fd\u6570","text":"<p>\u901a\u8fc7create_loss\u63a5\u53e3\u83b7\u5f97\u635f\u5931\u51fd\u6570</p> <pre><code>from mindcv.loss import create_loss\n\nloss = create_loss(name='CE')\n</code></pre>"},{"location":"zh/tutorials/quick_start/#_9","title":"\u5b66\u4e60\u7387\u8c03\u5ea6\u5668","text":"<p>\u4f7f\u7528create_scheduler\u63a5\u53e3\u8bbe\u7f6e\u5b66\u4e60\u7387\u7b56\u7565\u3002</p> <pre><code>from mindcv.scheduler import create_scheduler\n\n# \u8bbe\u7f6e\u5b66\u4e60\u7387\u7b56\u7565\nlr_scheduler = create_scheduler(steps_per_epoch=num_batches,\n                                scheduler='constant',\n                                lr=0.0001)\n</code></pre>"},{"location":"zh/tutorials/quick_start/#_10","title":"\u4f18\u5316\u5668","text":"<p>\u4f7f\u7528create_optimizer\u63a5\u53e3\u521b\u5efa\u4f18\u5316\u5668\u3002</p> <pre><code>from mindcv.optim import create_optimizer\n\n# \u8bbe\u7f6e\u4f18\u5316\u5668\nopt = create_optimizer(network.trainable_params(), opt='adam', lr=lr_scheduler)\n</code></pre>"},{"location":"zh/tutorials/quick_start/#_11","title":"\u8bad\u7ec3","text":"<p>\u4f7f\u7528mindspore.Model\u63a5\u53e3\u6839\u636e\u7528\u6237\u4f20\u5165\u7684\u53c2\u6570\u5c01\u88c5\u53ef\u8bad\u7ec3\u7684\u5b9e\u4f8b\u3002</p> <pre><code>from mindspore import Model\n\n# \u5c01\u88c5\u53ef\u8bad\u7ec3\u6216\u63a8\u7406\u7684\u5b9e\u4f8b\nmodel = Model(network, loss_fn=loss, optimizer=opt, metrics={'accuracy'})\n</code></pre> <p>\u4f7f\u7528<code>mindspore.Model.train</code>\u63a5\u53e3\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u3002</p> <pre><code>from mindspore import LossMonitor, TimeMonitor, CheckpointConfig, ModelCheckpoint\n\n# \u8bbe\u7f6e\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4fdd\u5b58\u7f51\u7edc\u53c2\u6570\u7684\u56de\u8c03\u51fd\u6570\nckpt_save_dir = './ckpt'\nckpt_config = CheckpointConfig(save_checkpoint_steps=num_batches)\nckpt_cb = ModelCheckpoint(prefix='densenet121-cifar10',\n                          directory=ckpt_save_dir,\n                          config=ckpt_config)\n\nmodel.train(5, loader_train, callbacks=[LossMonitor(num_batches//5), TimeMonitor(num_batches//5), ckpt_cb], dataset_sink_mode=False)\n</code></pre> <pre><code>[WARNING] DEVICE(1769,ffff87c70ac0,python):2022-12-21-16:04:30.001.890 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/network-WithLossCell/_loss_fn-CrossEntropySmooth/GatherD-op5273] don't support int64, reduce precision from int64 to int32.\n\n\nepoch: 1 step: 156, loss is 2.0816354751586914\nepoch: 1 step: 312, loss is 1.4474115371704102\nepoch: 1 step: 468, loss is 0.8935483694076538\nepoch: 1 step: 624, loss is 0.5588696002960205\nepoch: 1 step: 780, loss is 0.3161369860172272\n\n\n[WARNING] DEVICE(1769,ffff87c70ac0,python):2022-12-21-16:09:20.261.851 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/network-WithLossCell/_loss_fn-CrossEntropySmooth/GatherD-op16720] don't support int64, reduce precision from int64 to int32.\n\n\nTrain epoch time: 416429.509 ms, per step time: 532.519 ms\nepoch: 2 step: 154, loss is 0.19752007722854614\nepoch: 2 step: 310, loss is 0.14635677635669708\nepoch: 2 step: 466, loss is 0.3511860966682434\nepoch: 2 step: 622, loss is 0.12542471289634705\nepoch: 2 step: 778, loss is 0.22351759672164917\nTrain epoch time: 156746.872 ms, per step time: 200.444 ms\nepoch: 3 step: 152, loss is 0.08965137600898743\nepoch: 3 step: 308, loss is 0.22765043377876282\nepoch: 3 step: 464, loss is 0.19035443663597107\nepoch: 3 step: 620, loss is 0.06591956317424774\nepoch: 3 step: 776, loss is 0.0934530645608902\nTrain epoch time: 156574.210 ms, per step time: 200.223 ms\nepoch: 4 step: 150, loss is 0.03782692924141884\nepoch: 4 step: 306, loss is 0.023876197636127472\nepoch: 4 step: 462, loss is 0.038690414279699326\nepoch: 4 step: 618, loss is 0.15388774871826172\nepoch: 4 step: 774, loss is 0.1581358164548874\nTrain epoch time: 158398.108 ms, per step time: 202.555 ms\nepoch: 5 step: 148, loss is 0.06556802988052368\nepoch: 5 step: 304, loss is 0.006707251071929932\nepoch: 5 step: 460, loss is 0.02353120595216751\nepoch: 5 step: 616, loss is 0.014183484017848969\nepoch: 5 step: 772, loss is 0.09367241710424423\nTrain epoch time: 154978.618 ms, per step time: 198.182 ms\n</code></pre>"},{"location":"zh/tutorials/quick_start/#_12","title":"\u8bc4\u4f30","text":"<p>\u73b0\u5728\u8ba9\u6211\u4eec\u5728CIFAR-10\u4e0a\u5bf9\u521a\u521a\u8bad\u7ec3\u7684\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\u3002</p> <pre><code># \u52a0\u8f7d\u9a8c\u8bc1\u6570\u636e\u96c6\ndataset_val = create_dataset(name='cifar10', root=cifar10_dir, split='test', shuffle=True, num_parallel_workers=num_workers, download=download)\n\n# \u6267\u884c\u6570\u636e\u589e\u5f3a\u64cd\u4f5c\uff0c\u751f\u6210\u6240\u9700\u6570\u636e\u96c6\u3002\nloader_val = create_loader(dataset=dataset_val,\n                           batch_size=64,\n                           is_training=False,\n                           num_classes=num_classes,\n                           transform=trans,\n                           num_parallel_workers=num_workers)\n</code></pre> <p>\u52a0\u8f7d\u5fae\u8c03\u540e\u7684\u53c2\u6570\u6587\u4ef6\uff08densenet121-cifar10-5_782.ckpt\uff09\u5230\u6a21\u578b\u3002</p> <p>\u6839\u636e\u7528\u6237\u4f20\u5165\u7684\u53c2\u6570\u5c01\u88c5\u53ef\u63a8\u7406\u7684\u5b9e\u4f8b\uff0c\u52a0\u8f7d\u9a8c\u8bc1\u6570\u636e\u96c6\uff0c\u9a8c\u8bc1\u5fae\u8c03\u7684 DenseNet121\u6a21\u578b\u7cbe\u5ea6\u3002</p> <pre><code># \u9a8c\u8bc1\u5fae\u8c03\u540e\u7684DenseNet121\u7684\u7cbe\u5ea6\nacc = model.eval(loader_val, dataset_sink_mode=False)\nprint(acc)\n</code></pre> <pre><code>[WARNING] DEVICE(1769,ffff87c70ac0,python):2022-12-21-16:24:11.927.472 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/network-WithLossCell/_loss_fn-CrossEntropySmooth/GatherD-op24314] don't support int64, reduce precision from int64 to int32.\n\n\n{'accuracy': 0.951}\n\n\n[WARNING] DEVICE(1769,ffff87c70ac0,python):2022-12-21-16:25:01.871.273 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/network-WithLossCell/_loss_fn-CrossEntropySmooth/GatherD-op27139] don't support int64, reduce precision from int64 to int32.\n</code></pre>"},{"location":"zh/tutorials/quick_start/#yaml","title":"\u4f7f\u7528YAML\u6587\u4ef6\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u548c\u9a8c\u8bc1","text":"<p>\u6211\u4eec\u8fd8\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u8bbe\u7f6e\u597d\u6a21\u578b\u53c2\u6570\u7684yaml\u6587\u4ef6\uff0c\u901a\u8fc7<code>train.py</code>\u548c<code>validate.py</code>\u811a\u672c\u6765\u5feb\u901f\u6765\u5bf9\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\u548c\u9a8c\u8bc1\u3002\u4ee5\u4e0b\u662f\u5728ImageNet\u4e0a\u8bad\u7ec3SqueezenetV1\u7684\u793a\u4f8b \uff08\u9700\u8981\u5c06ImageNet\u63d0\u524d\u4e0b\u8f7d\u5230\u76ee\u5f55\u4e0b\uff09</p> <p>\u8be6\u7ec6\u6559\u7a0b\u8bf7\u53c2\u8003 \u4f7f\u7528yaml\u6587\u4ef6\u7684\u6559\u7a0b</p> <pre><code>#  \u5355\u5361\u8bad\u7ec3\npython train.py -c configs/squeezenet/squeezenet_1.0_gpu.yaml --data_dir /path/to/dataset --distribute False\n</code></pre> <pre><code>python validate.py -c configs/squeezenet/squeezenet_1.0_gpu.yaml --data_dir /path/to/dataset --ckpt_path /path/to/ckpt\n</code></pre>"}]}